\section{Introduction}
Type systems are invaluable tools for developing robust and extensible modern software \citehere. Programming languages theorists have long understood the utility that type systems bring outside of the standard assurance that well-typed programs do not go wrong \citehere. Indeed, type systems can be designed to help programmers reason about myriad facets of their programs, including but certainly not limited to security and privacy \citehere, nondeterminism \citehere, computational effects \citehere, low-level representation details \red{[Kinds calling conventions]}, asynchronous communication \red{[Session types]}, staging \citehere, and program modularity \red{[Module systems]}. 
\\

Most relevant to this thesis, however, is the ability to create type systems which allow programmers to reason about their programs' resource usage. \red{more general background here about type systems for resource analysis}.


In this \red{chapter}, we will investigate and implement a variant of \lambdaamorminus (pronounced ``lambda amor minus"), a language with a type system for amortized cost analysis. Programs written in \lambdaamorminus have types which are annotated with costs and potential, such that every type-correct program in \lambdaamorminus is a valid amortized analysis. Moreover, the costs expressed in the types give a sound upper bound on the actual execution cost of the program. \lambdaamorminus is expressive enough to statically verify amortized cost bounds for a wide class of functional programs, from the traditional examples of amortized analysis, to fully general cost-polymorphic higher-order functions like \texttt{map} and \texttt{fold}, which aren't well handled by existing resource-analysis languages like Resource Aware ML \citehere. \red{Should I talk more about Lambda-Amor here?}

By and large, the original creators of \lambdaamorminus were interested in it as a unifying framework for amortized analysis type systems. There are many axes along which one may design a type system, and \lambdaamorminus does a good job of interpreting many different styles. In this work, however, we will primarily interest ourselves in \lambdaamorminus's usefulness as a programming language which provides strong type-based cost reasoning principles to the user. To this end, the primary goal of this work is to design a version of \lambdaamorminus called \dlambdaamor which is amenable to implementation, and subsequently implement it.

We will begin in Section~\ref{sec:lambdaamor-overview} by giving an overview of the concepts \lambdaamorminus's type system draws on. \lambdaamorminus includes two modalities-- unary operators on types for tracking cost and potential, respectively. To soundly manage this potential, \lambdaamorminus is based on an affine logic in which every variable may be used at most once so that values with potential cannot be duplicated. In order to make complex potential functions, \lambdaamor uses refinement types in the style of DML \citehere, which we review. Next, we discuss the main obstacle the original type system presents to implementation: constraint solving. Our solution to this problem is based on univariate polynomial potential functions in the style of Automated Amortized Resource Analysis (AARA) \citehere, which we briefly review.

Then, in Section~\ref{sec:dlambdaamor-syntax-and-types}, we will discuss the syntax and type system of \dlambdaamor in depth, providing intuition for the each of the judgments, and discussing selected rules from the type system. \dlambdaamor's type system is many-layered, with rules for type formation, term formation, and a smaller type system for the sub-language which governs the refinement types. We pay special attention to the rules which govern the cost analysis-specific language features, and describe them in detail.

In Section~\ref{sec:dlambdaamor-sound}, we sketch the soundness proof for \dlambdaamor, by showing that it may be embedded in \lambdaamorminus, and appealing to its soundness theorem featured in \citet{rajani-et-al:popl21}.



%\red{Discuss it here! This is the part where you do the specs dump-- has two modalities, RAML-style ideas, affine types, refinement types, polymorphism. Then talk about game plan for algorithmic: bidirectional, I/O, constraint output/solving.}

\section{Overview of \dlambdaamor} 
\label{sec:lambdaamor-overview}
In this section, we will present the type system and semantics of \dlambdaamor, the variant of \lambdaamor which we plan to implement.
\red{Last sentence is false, lol. fix that.}

\subsection{Cost and Potential Modalities}
One of the most basic insights that \lambdaamor takes advantage of in its design is that costly computation can be thought of an effect\footnote{
In fact, cost can also be thought of as a \textit{coeffect} \cite{girard-et-al:tcs92:bll}, and one of the major breakthroughs of \lambdaamor is the unification
of both styles of resource tracking in a single calculus.
}. When a program does work, it has an effect on the world, namely the effect of taking time. In this sense, nearly all ``pure" programming languages are impure, as they allow pervasive use of the effect of cost. Unlike most languages, \lambdaamor encapsulates this effect by forcing all costly computation to happen in a monad \citehere.

\subsubsection{Cost Monad}
 However, a simple monad is not enough. We care not only that a term may incur cost, but how much cost it can incur! For this purpose, \lambdaamor uses a \textit{graded} monad \citehere $\M \; I \; \tau$. A computation of this type is a computation which returns a value of type $\tau$, and may incur up to $I$ cost, where $I$ is drawn from the sort of positive real numbers. As a graded modality, this monad's operations interact with the grade in nontrivial ways: for instance, the ``pure" computation $\texttt{ret}(e)$ has type $\M \; 0 \; \tau$ when $e : \tau$. Of course, any pure term may be lifted to a monadic computation which incurs no cost (\red{How do I explain that things run...}). Most importantly, given a costly computation $e_1 : \M \; I_1 \; \tau_1$ and a continuation $x : \tau_2 \vdash e_2 : \M \; I_2 \; \tau_2$, they can be sequenced into a computation $\texttt{bind}\, x = e_1\, \texttt{in}\, e_2 : \M \; (I_1 + I_2) \; \tau_2$. Note that the costs add-- a computation which may take up to $I_1$ units of time followed by a computation which takes up to $I_2$ units of course takes at most $I_1 + I_2$ units. However, neither \texttt{ret} nor \texttt{bind} incurs any nontrivial cost: any program written using only \texttt{ret}s and \texttt{tick}s will have type $\M \, 0 \, \tau$. For this, \lambdaamor includes a term $\texttt{tick}[I]$ of type $M \, I \, \texttt{unit}$, which incurs cost $I$. This is the only construct in \lambdaamor which incurs any ``extra cost": the idea is that programmers insert \texttt{tick}s in front of the operations their specific cost model dictates are costly. This technique is widely used in the cost analysis literature \citehere, and so \lambdaamor also adopts it for simplicity.
 
But of course, this cost monad can only be half the story. In a language which seeks to provide types for amortized analysis, a mechanism for handling potential is required.
 
\subsubsection{Potential Modality and Affine Types}
In addition to the cost monad, \lambdaamor includes another graded modality for tracking potential. A term of type $[I] \; \tau$ can be thought of a term of type $\tau$ which stores $I$ potential\footnote{
In some senses, potential in \lambdaamor behaves more like the credits of the banker's method discussed in Section~\ref{sec:amortized-primer}-- it can be created and attached to specific values. To avoid confusion, we follow \citet{rajani-et-al:popl21} with the terminology of ``potential"
}, where $I$ is again drawn from a sort of positive real numbers.
The most important operation associated with the potential modality is the ability to use potential to offset the cost of a computation. Concretely, given a term $e_1 : [I] \; \tau_1$ and a monadic continuation $x : \tau_1 \vdash e_2 : \M \; (I + J) \; \tau_2$, we can form the computation $\texttt{release}\, x = e_1 \, \texttt{in}\, e_2 : \M \; J \; \tau_2$. The crucial aspect of this construction is the fact that the resulting computation requires at most $J$ units of time to run, while the initial computation $e_2$ required $I + J$. Intuitively, we think of this as the $I$ units of potential ``paying for" $I$ steps of computation. 

Potential may also be created, and attached to values. In \lambdaamor, these two functions are handled by the same construct. For terms $e : \tau$, we may form $\texttt{store}[I](e) : \M \; I \; ([I] \; \tau)$, which is a computation which runs for at most $I$ units of time, and returns a $\tau$ with $I$ potential attached. The fact that \texttt{store} incurs this cost is what justifies the term \texttt{release}-- the program has paid an ``extra" cost of $I$ to create $[I] \; \tau$, and thus can exercise this option to reduce the cost of a subsequent computation with \texttt{release}.

This dynamic between \texttt{store} and \texttt{release} forces a restriction on the type system-- variables can only be used at most once. Our argument for the soundness of \texttt{release} relies on an the assumption that the potential we are releasing has not already been released elsewhere, and so duplication of variables must be disallowed. Of course, this kind of restriction is very common-- we simply require that \lambdaamor be \textit{affine}: weaking of the context is allowed, but contraction is disallowed. 

\subsubsection{Refinement Types}
\label{sec:lambdaamor-overview-refty}
So far, the situation we've described would only allow types with \textit{constant} amounts of potential. For nontrivial analyses, this is wholly insufficient: the potential of a data structure must be able to depend on the size or other numerical parameters of that data structure. For this purpose, \lambdaamor includes \textit{refinement types} in the style of Dependent ML \citehere. Concretely, \lambdaamor includes length-refined lists: a value of type $L^n \tau$ is a list of length $n$, where $n$ is an \textit{index term}-- an term in a small language of arithmetic expressions over a set of variables. Further, these index terms which appear in refinements may also appear in potentials! For instance, $\left[n^2\right] \; (L^n \tau)$ is the type of lists of length $n$ with potential $n^2$.

\subsection{Potential Vectors and AARA}
The story we've just told about \lambdaamor's type system is loyal to the original presentation in \citep{rajani-et-al:popl21}, but somewhat inadequate for implementation purposes. As we will discuss in Section~\ref{sec:lambdaamor-impl}, efficient subtyping is necessary for implementation of \lambdaamor. However,
the inclusion of the potential and cost modalities presents a challenge. In order for $[I] \; \tau_1$ to be a subtype of $[J] \; \tau_2$, it must be that $\tau_1 \subty \tau_2$, and that $J \leq I$. But as discussed above, $I$ and $J$ are index terms, and may be polynomials in a set of index variables. Ideally, we would like to discharge these inequalities generated by subtyping by constraint solver, but even modern SMT solvers struggle to handle polynomial inequalities.

To solve this problem, we borrow a key idea from Automatic Amortized Resource Analysis (AARA) \citehere which will allow us to generate only linear constraints over index variables, while still allowing univariate polynomial potentials and cost. The main idea is to fix a clever ``basis" for the space of polynomials, and then represent polynomials as a vector of their coefficients with respect to that basis. The basis in question is chosen to satisfy one key property: if $f(n)$ is written in terms of the basis, then the coefficients of $f(n-1)$ may be efficiently determined from $f(n)$. This property gives rise to the ability to easily analyze list algorithms in \lambdaamor: when writing a function $([f(n)] \, (L^n \, \tau)) \loli \sigma$, it is simple to pattern match on the argument and determine the type of the tail $[f(n-1)] \, (L^{n-1} \, \tau)$ to pass to a recursive call.

In \dlambdaamor, we will mostly syntactically restrict potential functions to be of this form, with some exception. We show in Section \textbf{??} that this language may be trivially elaborated into the original \lambdaamor, and further in Section \textbf{??} we show that the restricted set of allowable potential functions are still expressive enough for practical purposes.
\red{transition...}

\textbf{How do I cite that literally all of this is from JanH}

\begin{definition}[Potential Vector]
For a fixed $k$, we call a vector of nonnegative reals $(a_0,\dots,a_k)$ a potential vector.
\end{definition}

\begin{definition}[$\phi$ Function]
For fixed $k$, we define $\phi : \N \times \R_{\geq 0}^k \loli \R$ to be
$$
\phi\left(n,(p_0,\dots,p_k)\right) = \sum_{i=0}^k p_i\binom{n}{i}
$$
where $\binom{n}{r}$ is the binomial coefficient. We refer to the first argument of $\phi$ as the ``base", and the second argument as the ``potential".
\end{definition}

With $\phi$ in hand, we redefine the cost and potential modalities. In \dlambdaamor, the cost modality is written as $M \, (I,\vec{p}) \, \tau$ and the potential modality is $[I|\vec{p}] \,  \tau$. These two types classify values of type $\tau$ which cost up to $\phi(I,\vec{p})$ units of time and posess $\phi(I,\vec{p})$ potential, respectively.

\begin{theorem}[Monotonicity and Additivity of $\Phi$]
Let $\vec{p}$ and $\vec{q}$ be potential vectors.
\begin{enumerate}
  \item If $\vec{p} \leq \vec{q}$ componentwise, then $\phi(n,\vec{p}) \leq \phi(n,\vec{q})$.
  \item $\phi(n,\vec{p} + \vec{q}) = \phi(n,\vec{p}) + \phi(n,\vec{q})$
\end{enumerate}
\end{theorem}

The fact that $\phi$ is monotone in its second argument allows us to reduce the problematic subtyping rule for potentials (and costs) to generating linear inequalities and equalities \red{this isn't really true in presence of the sum...}: $[I|\vec{p}] \, \tau_1$ is a subtype of $[J|\vec{q}]$ when $I = J$ and $\vec{q} \leq \vec{p}$ componentwise.

The additivity of $\phi$ also allows us to simplify the bind and release- given a computation $e_1 : \M \, (I,\vec{p}) \, \tau_1$ and a continuation $x : \tau_1 \vdash e_2 : \M \, (I,\vec{q}) \, \tau_2$,  we may perform the computations in sequence with $\texttt{bind}\, x = e_1 \, \texttt{in}\, e_2 : \M \, (I,\vec{p} + \vec{q}) \, \tau_2$.

The final ingredient of this new version of the cost and potential modalities is the ability to change base. To illustrate, consider the process of writing a function $L^n \tau \loli \M \, (n,\vec{p}) \, \sigma$. The recursive call on the tail of the input list will have type $\M \, (n-1,\vec{p}) \, \sigma$, but the function expects a return value of type $\M \, (n,\vec{p}) \, \sigma$. Since the \texttt{bind} requires that the argument and the continuation have the same base, the recursive call cannot be used in this context, rendering it useless. To fix this, we include a term \texttt{shift} in \dlambdaamor which ``promotes" a computation of type $\M \, (n-1,\vec{p}) \, \sigma$ to one of type $\M \, (n,\vec{q}) \, \sigma$, for a specific $\vec{q}$ determined by $\vec{p}$. This concept is likely familar to the reader familiar with AARA: in Resource Aware ML (an implementation of OCaml based on AARA) this construct is baked into the pattern match rule, while we make it explicit.

\begin{definition}[Additive Shift]
For $\vec{p} = (a_0,\dots,a_{k-1},a_k)$ a potential vector, we define $\lhd \vec{p} = (a_0 + a_1,\dots,a_{k-1} + a_k,a_k)$
\end{definition}

\begin{theorem}
For $n \geq 1$ and $\vec{p}$ a potential vector, $\phi(n,\vec{p}) = \phi(n-1,\lhd \vec{p})$
\end{theorem}
\label{thm:raml-shift}
\begin{proof}
It is straightforward to prove (either by combinatorial argument or direct computation) that $\binom{n-1}{i} + \binom{n-1}{i+1} = \binom{n}{i+1}$. Using this fact,
we may compute as follows:
\begin{align*}
  \phi(n-1,\lhd \vec{p}) &= \sum_{i=0}^k p_i\binom{n-1}{i} + \sum_{i=0}^{k-1} p_{i+1}\binom{n-1}{i}\\
                         &= p_0 + \sum_{i=1}^k p_i\binom{n-1}{i} + \sum_{i=0}^{k-1} p_{i+1}\binom{n-1}{i}\\
                         &= p_0 + \sum_{i=0}^{k-1} p_{i+1}\left(\binom{n-1}{i+1} + \binom{n-1}{i}\right)\\
                         &= p_0 + \sum_{i=0}^{k-1} p_{i+1}\binom{n}{i+1}\\
                         &= \sum_{i=0}^k p_i \binom{n}{i}\\
                         &= \phi(n,\vec{p})
\end{align*}
\end{proof}

\section{Syntax and Type System \dlambdaamor}
\label{sec:dlambdaamor-syntax-and-types}

\subsection{Syntax of \dlambdaamor}
In preparation to discuss \dlambdaamor's type system, we present its syntax in Figure~\ref{fig:dlambdaamor-syntax}.
\begin{figure}
\label{fig:dlambdaamor-syntax}
\caption{Syntax of \dlambdaamor}
\end{figure}

\subsubsection{Index Terms, Sorts, Kinds, and Constraints}
\dlambdaamor's refinement types are modeled in the style of DML \citehere, which takes the form of a two-level type system. As discussed in Section \textbf{??}, these refinements allow the user to assign types potential which depend on the sizes of data structures, such as the length of lists. These numerical values are denoted by \textit{index terms} ($I,J$) which decorate some of the types and surface syntax of \dlambdaamor. Index terms may be of three possible numerical \textit{base sorts}: natural numbers $\N$, positive real numbers $\R^+$, and potential vectors of some fixed length $k$, $\vec{\R^+}$. Additionally, \dlambdaamor also includes first-order sort-level functions.

The syntax of index terms themselves is generated by the standard arithmetic operations, along with constants, variables, and application/abstraction forms for the sort-level functions. Of special note are the \texttt{const} and $\Sigma$ constructs. For an index term $I$ of sort $\R^+$, the term $\texttt{const}(I)$ is of potential vector sort, and may be thought of as the ``constant" potential vector $(I,0,\dots,0)$, such that for all $n \N$, $\phi(n,\texttt{const}(I)) = I$.
The $\Sigma$ construct is as expected, although the upper bound is non-inclusive: the sum $\sum_{i=I_0}^{I_1} J$ sums from $J[I_0/i]$ to $J[(I_1-1)/i]$, as long as the range is nonempty, when the sum is of course zero.

\dlambdaamor also supports full System F-style impredicative polymorphism, as well as sort-indexed types. We denote the kind of types as $\star$. Note that sort-indexed types may have sort-level arrows in negative position, and so sort-function-indexed types are included also.

Finally, \dlambdaamor includes constraints over index terms, generated by conjunction, disjunction, implication, both kinds of quantification, as well as the trivially true and false propositions. Note that we will not provide a proof system for these constraints. Instead, we will only ever interact with constraints via an abstract satisfiability relation $\vDash$, and all the proofs of soundness and completeness in Section \textbf{??} will be relative to a decision procedure/oracle for $\vDash$.

\subsubsection{Types}
\dlambdaamor's types include all of the standard connectives from affine logic, namely positive and negative products ($\otimes$ and $\amp$), sums ($\oplus$), affine functions ($\loli$), and the exponential modality $! \tau$, whose values may be used more than once.
Of course, \dlambdaamor also supports a litany of more specialized types for amortized cost analysis.

Chief among these are the cost monad and potential types, A monadic type $M \, (I,\vec{p}) \, \tau$ classifies monadic computations of type $\tau$, which may incur up to $\phi(I,\vec{p})$ cost. The type formation rules (Figure \textbf{??}) ensure that $I$ is of sort $\N$, and $\vec{p}$ is of sort $\vec{\R^+}$. With the same restrictions on the sorts of its index terms, the potential type $[I|\vec{p}]\, \tau$ classifies values with at least $\phi(I,\vec{p})$ potential. In addition to the AARA-style potential, \dlambdaamor also has a ``constant potential" modality $[I] \, \tau$, whose values are those of type $\tau$, with $I = \phi(n,\texttt{const}(I))$ potential, for any $n$. While not strictly necessary for the theoretical development of \dlambdaamor, this modality is sometimes useful in practice.

Index variables may be quantified over in types with the $\forall i : S.\tau$ and $\exists i : S.\tau$ types, and polymorphic type variables are quantified over using the $\forall \alpha : K .\tau$ type constructor-- we do not support existential types, though there is no metatheoretical barrier to their inclusion.

As previously mentioned, the type of lists $L^I \, \tau$ is refined by length-- the values of this type all have length $I$. Next, \dlambdaamor also includes two ``constraint types", $\Phi \implies \tau$, and $\Phi \amp \tau$. Values of the first type are known to have type $\tau$ when $\Phi$ holds, and values of the second type are values of type $\tau$, along with an (irrelevant) proof of $\Phi$. As \lambdaamor has no error handling mechanism, this construct is helpful for statically preventing errors by encoding function pre and post-conditions in a type: for instance, the \texttt{head} function may be typed as $\forall n : \N. (n \geq 1) \implies (L^n \, \tau \loli \tau)$

Finally, \dlambdaamor's types include abstraction and application forms for indexed types. The abstraction form $\lambda i :S.\tau$ has kind $S \to K$ when $\tau$ has kind $K$, and so term variables will never have type $\lambda i : S.\tau$, as it is a higher-kinded type.

\subsubsection{Terms}
While the original presentation of \lambdaamor takes great care to include only a barebones term syntax, \dlambdaamor will have to expand this syntax somewhat to ensure that the textual representation of a program is unambiguous for programming purposes. Practically, this means that every logical connective has explicit syntactic introduction and elimination forms, whereas this is handled silently in \lambdaamor.

The term syntax for all of the standard connectives should be familiar. The two products are distinguished by double angled brackets for positive pairs, and parentheses for negative pairs. All binders are un-annotated to reduce the burden on the programmer: \bilambdaamor's bidirectional type inference means that type annotations will need to be written only when needed. Lists are constructed with nil and cons constructors, and the elimination form is a pattern match. The last standard inclusion is a fixpoint operator \texttt{fix}, which allows us to write recursive functions. 

The syntax associated to the amortized analysis constructs is likely less familiar. The monadic cost type $M \, (I,\vec{p}) \, \tau$ has three operations associated with it: $\textbf{ret}(e)$ and $\texttt{bind} \, x = e_1 \, \texttt{in}\, e_2$, the unit and bind of the monad, respectively, as well as $\texttt{tick}[I|\vec{p}]$, an atomic operation which incurs a cost of $\phi(I,\vec{p})$. The potential type has introduction form $\texttt{store}[I|\vec{p}](e)$ and elimination form $\texttt{release} \, x = e_1, \texttt{in} \, e_2$. Similarly, the \textit{constant} potential type has introduction form $\texttt{store}[I](e)$, and the same elimination syntax as the AARA-style potential type.

\subsection{Type System of \dlambdaamor}
\begin{figure}
\label{fig:dlambdaamor-typing-judgments}
\caption{Judgment Forms of the \dlambdaamor Type System}
\end{figure}

In Figure~\ref{fig:dlambdaamor-typing-judgments}, we provide a listing of the judgments which make up \dlambdaamor's type system. Selected rules are presented in Figure~\ref{fig:dlambdaamor-selected-typing-rules}, and a listing of all rules can be found in Appendix \textbf{??}.

\subsubsection{Contexts}
Judgments in \dlambdaamor have as many as five contexts.
 Contexts $\Psi$ map type variables to their kinds. $\Theta$ is an index variable context, which maps index variables to their sorts. $\Delta$ is a list of constraints, which are assumptions of the judgment-- constraints in $\Delta$ may mention variables in $\Theta$, and so there is a weak form of dependence between the two contexts. The final two contexts $\Omega$ and $\Gamma$ are term variable contexts, which map variables to their types. The context $\Omega$ is referred to as the exponential context, and it contains variables which may be used more than once: i.e. are not subject to the affine restriction.
\footnote{
One may think of all types in $\Omega$ implicitly beginning with $!$, and imagine the variable rule for exponential variables to be silently inserting the counit $!\tau \loli \tau$. This dual-context construction is standard in the study of modal types. \cite{kavvos:lmcs}
}. Finally, the context $\Gamma$ lists the rest of the variables, which may be used at most once.

In order to avoid questions of exchange, we consider all of the contexts except for $\Delta$ up to permutations. Indeed, we will frequently treat contexts like sets, testing membership. Further, it will be useful later on to take intersections, unions, and differences of contexts: these operations will only be defined when both operations involved are subsets of a common superset.

\subsubsection{Index Terms and their Sorts}
The rules that make up the sort system for index terms (prefixed I-) are mostly self-explanatory: we ensure that arguments to arithmetic operators have the same sorts.
Since all three base sorts ($\N$, $\R^+$, $\vec{\R^+}$) are nonnegative, the rule for subtraction $I - J$ must ensure that $I \geq J$. As discussed in Section~\textbf{??}, the rule I-ConstVec shows how \texttt{const} promotes index terms of sort $\R^+$ to sort $\vec{\R^+}$. Finally, the I-Lam and I-App rules give the introduction and elimination rules for the index-level functions.

\subsubsection{Types and their Kinds}
The type formation rules (prefixed K-) for \dlambdaamor are very straightforward. All types have kind $\star$, with the exception of the index type abstraction and elimination forms. The rule K-FamLam ensures that an indexed type $\lambda i : S. \tau$ has kind $S \to K$ when $\tau$ has kind $K$, and the indexed-type application $\tau \, I$ has kind $K$ when $\tau$ has kind $S \to K$ and $I$ is of sort $S$, as seen in K-FamApp.

\subsubsection{Subtyping}
The majority of the rules for subtyping in \dlambdaamor (prefixed by S-) are the standard congruences for logical connectives. The rules for the types involved in cost analysis for refinements, however, warrant some discussion.

The rule S-Monad gives the subtyping relation for the cost monad: $\M \, (I,\vec{q}) \, \tau_1 \subty \M (J,\vec{p}) \,\tau_2$ when $\tau_1 \subty \tau_2$, $I = J$, and $\vec{q} \leq \vec{p}$ componentwise. The soundness of this rule relies on the fact that $\phi(n,\vec{q}) \leq \phi(n,\vec{p})$ when $\vec{q} \leq \vec{p}$, and the fact that the cost annotations represent \textit{upper bounds}-- it is safe to use a computation which incurs less cost in a context which expects one that incurs more. Dually, it is always safe to throw away potential in the subtyping rules for the two potential modalities, S-Pot and S-ConstPot.

In addition to the subtyping rules at base kind, the rules S-FamLam and S-FamApp govern the subtyping of indexed types. The rule S-FamLam states that subtyping at kind $S \to K$ is simply generated by pointwise subtyping in the codomain $K$, while S-FamApp is a standard congruence rule. Note that S-FamApp requires that the two arguments be equal: since we do not require that indexed types be monotone, this is the strongest possible form of the rule.

Finally, the rules S-FamBeta-$1$ and S-FamBeta-$2$ serve to include $\beta$-equality of type families in the subtyping relation. The combination of these rules with S-FamApp and S-FamLam makes the subtyping relation not syntax directed at higher kind, which provides another barrier to simple implementation, as discussed in Section\textbf{??}.

\subsubsection{Well-Formedness Judgments and Context Subsumption}
The judgment $\Theta ; \Delta \vdash \Phi \; \texttt{wf}$ ensures that $\Phi$ is a well-formed context: all index terms mentioned in it are sort-correct, and relations are only judged between index terms of the same sort.

\dlambdaamor also requires two auxiliary context-wellformedness judgments: $\Theta \vdash \Delta \; \texttt{wf}$ and $\Psi ; \Theta ; \Delta \vdash \Gamma \; \texttt{wf}$. The former ensures that the constraints in the context $\Delta$ are well-typed with respect to the context $\Theta$, and the latter ensures that all of the types in $\Gamma$ have kind $\star$.

Finally, The judgment $\Psi ; \Theta ; \Delta \vdash \Gamma' \wknto \Gamma$ determines when we may relax a context $\Gamma'$ to a weaker one $\Gamma$ with the T-Weaken rule. Intuitively, this judgment encodes the permission to weaken a context as a kind of record subtyping. This intuition is made concrete in Theorem~\textbf{??}.

\subsubsection{Terms and their Types}
The typing rules for all of the logical connectives have the standard caveats for an affine type system: affine arrow introduction T-ArrI binds variable $x : A$ in the affine context $\Gamma$. Multi-premise rules like tensor introduction (T-TensorI) and sum elimination (T-Case) require splitting the affine context to type the premises. As usual, ``parallel" premises such as the two arms of a case may share affine resources, as only one branch will be taken at runtime. The exponential modality $!\tau$ also has the standard rules: T-ExpI ensures that one may only introduce a value of type $!\tau$ when the affine context is empty, and T-ExpE destructs a value of type $!\tau$ by binding an exponential variable of type $\tau$ for use in the continuation.

Of greater interest are the rules for the cost analysis and refinement type-related constructs. The return of the cost monad lifts a pure value $e : \tau$ to a monadic computation $\texttt{ret}(e)$ which incurs no cost. So, the rule T-Ret types $\texttt{ret}(e)$ at $\M(I,\vec{0}) \, \tau$ for any index term $I$ of sort $\N$, where $\vec{0}$ is the length $k$ vector of $0$s. Since $\phi(I,\vec{0}) = 0$ independent of the base $I$, this rule has the desired effect. Meanwhile, the bind of the cost monad sums the costs of the computation and the continuation. T-Bind operationalizes this by typing $\texttt{bind}\, x = e \, \texttt{in} \, e' \; : \M(I,\vec{p} + \vec{q}) \, \tau_2$ when $e : \M(I,\vec{p}) \, \tau_1$ and $x : \tau_1 \vdash e' : \M(I,\vec{q}) \, \tau_2$/ The soundness of this rule is justified by the linearity of the $\phi$ function, as outlined in Section~\textbf{??}.

The two operations for the potential modality are carefully constructed to work harmoniously with the cost monad. Firstly, given a term $e : \tau$, the rule T-Store allows us to store $\phi(I,\vec{p})$ potential on the term by incurring that amount of cost: this takes the form of assigning the type $\M(I,\vec{p}) \, \left([I|\vec{p}] \, \tau\right)$ to the term $\texttt{store}[I|\vec{p}](e)$. Note that in order to access the underlying potential, one must first $\texttt{bind}$ the computation, in effect incurring the requisite $\phi(I,\vec{p})$ cost to have access to the potential. Dually, the rule T-Release gives the typing for using potential. The potential on a term $e : [I|\vec{p}] \, \tau_1$ can be to pay for a monadic continuation $x : \tau_1 \vdash e' : \M(I,\vec{q} + \vec{p}) \, \tau_2$ to get
$\texttt{store}\, x = e \, \texttt{in} \, e' : \M(I,\vec{q}) \, \tau_2$. Of course, the rules for constant potentials follow a similar pattern: the constant store expression $\texttt{store}[J](e)$ has type $\M(I,\texttt{const}(J)) \, \left([J] \, \tau\right)$ by T-StoreConst.
We note that the type system enforces a discipline that all potential-related activities happen inside the cost monad, which greatly simplifies the type soundness proof found in \citet{rajani-et-al:popl21}.

The list type ($L^I \, \tau$) is length-indexed, and so its typing rules are somewhat more involved than the standard ones. To enforce the length refinement, the rules T-Nil and T-Cons specify that the empty list $\texttt{[]}$ has type $L^0 \tau$, while a cons list $e :: e'$ has type $L^{I+1} \tau$ for $e : \tau$ and $e' :: L^I \tau$. The list elimination rule T-Match is more or less standard, but the two branches are typed under extra constraints in the constraint context $\Delta$. If the scrutinee has type $L^I \tau$, then the nil case of the match is typed under the assumption that $I = 0$. Meanwhile the cons case is given the assumption $I \geq 1$, and the tail of the list is bound as having type $L^{I-1} \tau$.

In addition to the length-refined lists, the refinement type portion of \dlambdaamor's type system also includes index term quantifiers in types ($\forall,\exists$), as well as the two constraint types ($\Phi \amp \cdot$, $\Phi \implies \cdot$/).  The treatment of the quantifiers is standard: the rules T-ILam and T-ExistE bind index variables in the index context $\Theta$, while the rules T-IApp and T-ExistI substitute in index terms provided by the syntax. The rules for the constraint types operate in a similarly dual fashion.

Finally, \dlambdaamor includes two special ``structural" rules. The first is a subtyping rule T-Sub, which may be used to downcast the type of a term to a less precise one. This rule has no syntactic form, and thus may be inserted anywhere in a derivation. The second is T-Weaken, which allows for the weakening of the two term variable contexts, $\Omega$ and $\Gamma$. As previously mentioned, the weakening relation on which this rule depends includes subtyping, and so a weaker context may include less precise types, and not just fewer available variables.

\begin{figure}
\label{fig:dlambdaamor-selected-typing-rules}
\caption{Selected \dlambdaamor rules}
\end{figure}

\subsubsection{Other Well-Formedness Judgments and Presuppositions}
All of the judgments presented so far are ``raw" judgments-- one may mechanically derive a proof of one using the inference rules, without regard for whether or not the judgment makes any sense. Traditionally, the requisite assumptions for stating a judgment in a sensical manner are known as \textit{presuppositions}. For example, the sort-checking judgment $\Theta ; \Delta \vdash I : S$ requires that the constraint context $\Delta$ be well-formed with respect to $\Theta$. There are many ways of handling these, but in this work we choose to make them explicit. Each raw judgment form has an associated judgment form which packages together the requisite well-formedness presuppositions for that judgment. We denote this by a subscript $p$ on the turnstile, as shown in Figure~\ref{fig:dlambdaamor-presupps}

\begin{figure}
\label{fig:dlambdaamor-presupps}
\caption{Judgments of \dlambdaamor with Presuppositions}
\end{figure}

\section{Semantics and Soundness of \dlambdaamor}
For \dlambdaamor to be useful, its type system must be \textit{sound}. In this context, soundness means that the statically-predicted execution costs from the types given to programs are in fact actual upper bounds on the programs' real execution cost. In order to prove that \dlambdaamor's type system is sound in this way, we will appeal to a version of the soundness proof of \lambdaamorminus. As discussed in Section~\textbf{??}, \lambdaamorminus differs from \dlambdaamor mainly in its treatment of potentials and costs. In fact the two languages are sufficiently similar (by design, of course) that there is a straightforward embedding of \dlambdaamor into \lambdaamorminus. This embedding is cost-preserving, and so the soundness of \dlambdaamor follows immediately from the soundness of \lambdaamorminus. Formally, we do not present a true embedding into \lambdaamorminus, as it does not have a sort of potential vectors! However, potential vectors can be trivially added to \lambdaamorminus: the kripke logical relation which forms the basis for its soundness proof never inspects index terms, and conflates index terms with the semantic objects they denote. For this reason, we freely consider \lambdaamorminus as having a sort of potential vectors in the style of \dlambdaamor.

In Section~\textbf{??}, we present the operational semantics for \dlambdaamor upon which the soundness theorem is based. This semantics is a big-step cost-indexed operational semantics: the cost indices are the concrete notion of cost that will be bounded by the statically-predicted costs in the soundness theorem.

Then, in Section~\textbf{??}, we sketch the embedding of \dlambdaamor into \lambdaamorminus, and further sketch proofs that the cost semantics of \dlambdaamor coincides with that of \lambdaamorminus under the embedding, as well as the overall soundness theorem of \dlambdaamor.


\subsection{Operational Semantics of \dlambdaamor}
In order to pin down the exact cost of programs written in \dlambdaamor, we provide a \textit{cost semantics} for the language: a big-step operational semantics which is indexed by the cost of evaluation.

Operationally, \dlambdaamor behaves like a call-by-name monadic version of PCF. The cost semantics, for which selected rules are presented in Figure~\ref{fig:dlambdaamor-selected-operational-rules}, consists of two separate judgments. First is a \textit{pure} evaluation relation: $e \Downarrow v$, which evaluates an expression of type $\tau$ to a value of the same type. Evaluations in this relation are not thought to incur any cost: in fact, the set of values includes all of the monadic computations, which must be \textit{forced}. This is accomplished with the \textit{forcing} evaluation relation $e \Downarrow^\kappa v'$, which relates monadic values of type $\M \, I \, \tau$ to values of type $\tau$. Selected rules from these two judgments can be found in Figure~\ref{fig:selected-sem-rules}.

The rules for the pure evaluation relation are straightforward- as all monadic terms are values, the pure relation simply behaves like a big-step evaluation relation for by-name PCF. The rules for the refinement syntax at term level behave as if the syntax for refinements has been erased at runtime- they contribute nothing meaningful to the operational semantics.

The rules for the forcing relation warrant some discussion. Since all monadic computations are values, the forcing relation depends on the pure relation to evaluate sub-expressions. For instance, the forcing relation evaluates $\texttt{ret}(e)$ to $v$ in $0$ steps when $e \Downarrow v$. Of course, the pure relation will take some steps of computation by performing $\beta$-redexes, but we will not consider these to be \textit{costly}, and thus do not need to be accounted for in the forcing relation.

Most importantly, the $\texttt{tick}[I|\vec{p}]$ term evaluates with cost $\phi(I,\vec{p})$ to the trivial value $()$. This rule encodes the heretofore intutitive cost behavior of the type $\M (I,\vec{p}) \, \tau$, by explicitly assigning the atomic costly operation the cost $\phi(I,\vec{p})$ in our cost semantics.  The final cost-monadic term, the \texttt{bind}, is assigned cost in a purely compositional way. The evaluation of \texttt{bind} proceeds like the evaluation of a let-binding, where the costs of forcing the argument and then the subsequent continuation are added, and given as the total cost.

Finally, the two potential-related operations incur no semantic cost. This may come as a surprise-- the statically predicted cost for the \texttt{store} operation (for example) is the amount of potential to be allocated. However, this cost is entirely for bookeeping purposes to ensure that potentially is used soundly: it is not truly incurred when the program runs. Similarly, the \texttt{release} operation runs identically to \texttt{bind}: it is simply a monadic sequencing. This ``ghost" nature of potential at runtime is congruent with the way we think about amortized analysis. Recalling the notation of Section~\textbf{??}, the operational semantics give the costs $C(f)$, while the static types encode the amortized cost $A(f) + \Delta\Phi$.

\begin{figure}
\label{fig:selected-sem-rules}
\caption{Selected Rules of \dlambdaamor's Cost Semantics}
\end{figure}

\subsection{Embedding of \dlambdaamor in \lambdaamorminus}
The translation of \dlambdaamor into \lambdaamorminus requires little insight: we simply compile the costs and potentials written abstractly as a base and potential vector to the $\phi$ function applied to a pair. Concretely, the meat of the translation on types consists of two rules: the \dlambdaamor cost type $\M(I,\vec{p}) \, \tau$ is translated to the \lambdaamorminus $\M \left(\phi(I,\vec{p})\right)\, \tau'$, and the potential type $[I|\vec{p}] \, \tau$ is translated to $\left[\phi(I,\vec{p})\right] \, \tau'$, where $\tau'$ is the translation of $\tau$. These translations respect rules of the two type systems: the monotonicity and additivity of the $\phi$ function from Theorem~\textbf{??} justify the translations of the \texttt{bind} and \texttt{release} operations, as well as the subtyping rule for costs and potentials. The rest of the translation is primarily an erasure. \lambdaamorminus's syntax includes no explicit index terms or types at the term level, and so these are all erased. In particular, the shift operation is erased, a move which is justified by Theorem~\textbf{thm:raml-shift}.


\subsubsection{Statement of Soundness of \dlambdaamor}

\red{Write this section}


\subsection{Examples of Programs in \dlambdaamor}
Having just spent a great many pages discussing the technical details of \dlambdaamor's syntax, type system, and semantics, we now arrive at the fun part: analyzing the cost of programs! In this section, we will present a number of examples of programs written in \dlambdaamor, each of which exemplifies a different component of its cost analysis capabilities. These examples will loosely follow the presentation of Section 3 of \citet{rajani-et-al:popl21}, where more in-depth discussion can be found.

\subsubsection{Add One}

We begin with a (very) simple example to demonstrate the utility of \dlambdaamor's AARA-style costs. Consider writing a function $\texttt{addOne}$, which adds one to each integer in a list. If we assume the cost model that natural number addition costs one unit of time, the function would have type $\forall n : \N. \, L^n(\texttt{nat}) \loli \M \, (n,\langle 0,1 \rangle) \, \left(L^n(\texttt{nat})\right)$. Recalling the intended meaning of the AARA-style cost functions, this means that \texttt{addOne} costs $\phi(n,\langle 0,1 \rangle) = n$ in total, where $n$ is the length of the input list (and also the output). Of course, this makes sense, as each entry in the list incurs a single cost, to add one to it. The term for this type can be found in Figure~\ref{fig:example-dlambdaamor-addone}. The operational aspects of the program are exactly what one expects from an instance of map. More interesting are the cost-related aspects of the code. In the cons branch, we immediately \texttt{shift}. This allows us to provide a term of type  $\M \, (n-1,\langle 1,1 \rangle) \, \left(L^n(\texttt{nat})\right)$ in place of the expected type $\M \, (n,\langle 0,1 \rangle) \, \left(L^n(\texttt{nat})\right)$. This shift is required to perform the recursive call on the tail: \textbf{??} has type $\M \, (n-1,\langle 0,1 \rangle) \, \left(L^{n-1} (\texttt{nat})\right)$, which can only be bound into a continuation which results in something of type $\M \, (n-1, \_) \, \_$. Further, the shift ``exposes" the one constant cost, which is incurred by the tick (which we attribute to the addition). This raises a crucial point: a ``hole" in a program expecting $\M \, (n,\langle 0,1 \rangle) \, \tau$ cannot accept a term of type $\M \, (n, \langle 1,0 \rangle) \, \tau$, despite this being semantically sound.

\begin{figure}
\label{fig:example-dlambdaamor-addone}
\caption{\texttt{addOne} function in \dlambdaamor}
\end{figure}


This example can also be performed using potentials, rather than costs. Instead of a function which incurs $n$ cost, we can instead think of \texttt{addOne} as a free-to-execute function which expects $n$ potential. One possible choice for this function's type is:
$$\forall n : \N .\, [n|\langle 0,1 \rangle] \, 1 \loli L^n(\texttt{nat}) \loli \M \, (n,\langle 0,0 \rangle) \, \left(L^n(\texttt{nat})\right)$$
This style is indicative of ``gas-cost" analyses, as we expect $n$ gas up front to run, and spend it all towards performing the additions. For technical reasons relating to expressivity\footnote{
In short, coeffect-style analyses require the use of potentials.
}, we often use this style (preferring the type $[I] \, \tau \loli \M \, 0 \, \sigma$ over the type $\tau \loli \M \, I \, \sigma$) even in cost analyses which are not amortized. \red{Should this be said elsewhere?}

Another option is a type which attaches a single potential to each element of the input list, in a style indicative of the Banker's method:
$$
\forall n : \N .\, L^n\left([1] \, \texttt{nat}\right) \loli \M \, (n,\langle 0,0 \rangle) \, \left(L^n(\texttt{nat})\right)
$$
The terms corresponding to both of these types can be found in Appendix~\textbf{??}. Of course, this cost analysis is tight and fairly uninteresting: it requires no ``real" amortized analysis.

To illustrate the utility of \dlambdaamor as a platform for actual amortized analysis, we show how a few classic examples of amortized analysis can be performed by writing the programs in \dlambdaamor.

\subsubsection{Insertion Sort}
\red{Talk about how you can EZ PZ do quadratic analyses}

\subsubsection{Functional Queue}
The first example of amortized analysis is the traditional functional queue \citehere. Here, a queue is represented as a pair of lists, $l_f$ and $l_r$, which we refer to as the front and rear lists, respectively. To enqueue an element, we cons it to the head of the front list, and to dequeue, an element is removed from the head of the rear list. If the rear list is empty when a dequeue operation is issued, the front list is reversed into the rear.

If we assume that cons operations are the only costly operation, and that they each incur one cost, this dequeue operation has worst-case complexity $O(n)$ where $n$ is the size of the queue (the sum of the sizes of $l_f$ and $l_r$), since it sometimes needs to reverse the entire front list. However, by employing the banker's method, we may enforce the invariant that each element of the front list carries two credits to be used to pay for its eventual reversal. Under this scheme, both enqueue and dequeue are constant time.

This entire informal analysis is captured formally by the types\footnote{
We will sometimes write $\M \, \vec{p} \, \tau$ to mean $\forall j : \N. \M \, (j,\vec{p}) \, \tau$.
} of the enqueue and dequeue operations in \dlambdaamor. In order to encode this analysis, we define a queue to be of type $ L^n([2] \, \tau) \otimes L^m \, \tau$: a pair of $\tau$-lists, where the front has $2$ potential on each of its $n$ elements.

The enqueue function has the following fairly obvious type.

$$
\texttt{enq} \; : \; \forall n,m : \N. \, [3] \, 1 \loli \tau \loli L^n([2] \, \tau) \otimes L^m \, \tau \loli \M \, \langle 0 \rangle \, \left(L^{n+1}([2] \, \tau) \otimes L^m \, \tau\right)
$$

From a queue and three extra potential, we may enqueue a single element, resulting in queue with one more element on its front list, for no cost. The term implementing \texttt{enc} can be found in Figure~\ref{fig:example-dlambdaamor-enc}. The type of dequeue is somewhat more involved, since the sizes of the output lists are not a simple function of this inputs. In addition, the function has a precondition: the queue cannot be empty. These two numerical restrictions provide a nice illustration of \dlambdaamor's refinement types.

$$
\texttt{deq} \; : \; \forall m,n : \N. (m + n > 0) \implies L^n([2] \, \tau) \otimes L^m \, \tau \loli \M \langle 0 \rangle \left(\exists n',m' : \N. (n' + m' + 1 = n + m) \amp \left(L^n([2] \, \tau) \otimes L^m \, \tau\right)\right)
$$

\texttt{deq} takes a nonempty queue, and produces another queue which has one element removed. The implementation of \texttt{deq} relies on a function \texttt{move}, which reverses the rear list into the front. The terms for \texttt{move} and \texttt{dec} can be found in Appendix~\textbf{??}.
\red{Do i want to do the binary counter here?}


\begin{figure}
\label{fig:example-dlambdaamor-enc}
\caption{\texttt{enc} function in \dlambdaamor}
\end{figure}

\subsubsection{Cost-Parametric Map and Fold}
While many existing languages and type systems for (amortized) resource analysis also support higher-order functions, the allowable analyses with higher-order functions are limited. One such limitation is that function arguments to higher-order functions are usually assumed to be constant-cost: for instance, in the cost analysis of a map, each application of the mapping function is assumed to incur the same amount of cost.

In order to improve on this, we employ a cost family $C : \N \to \R^+$ to encode the costs of each application of the function: the $i$-th call to the function is thought to incur $C(i)$ cost. Then in total, the map function incurs $\sum_{0 \leq i < n} C(i)$ cost. This analysis is reified in the type of map:
$$
\texttt{map} \; : \; \forall \alpha,\beta : \star. \forall C : \N \to \R^+. \forall n : \N. \, !\left(\forall i : \N. [C \, i] \, 1 \loli \texttt{Nat}(i) \loli \alpha \loli \M \, \langle 0 \rangle\,  \beta\right)
\loli !\texttt{Nat}(n)
\loli L^n \,\alpha \loli \M \, \langle \texttt{const}\left(\sum_{0 \leq i < n} C(i)\right) \rangle\, \left(L^n \, \beta\right)
$$

Most importantly, the mapping function has type $!\left(\forall i : \N. [C \, i] \, 1 \loli \texttt{Nat}(i) \loli \alpha \loli \M \, \langle 0 \rangle\,  \beta\right)$. Since it must be applied to each element of the list, its type is $!$-ed to ensure it may be duplicated. The function is parameterized by the index $i$ on which it operates. In order to ensure that the mapping function at $i$ is actually only ever used at index $i$, the mapping function takes an additional argument of type $\texttt{Nat}(i)$, which is the singleton type of natural numbers equal to $i$\footnote{
This is simply an alias for $L^i \, 1$.
}. Finally, the mapping function requires $C \, i$ potential to run, and incurs no amortized cost, which ensures that its actual cost is bounded by $C \, i$.

Given the mapping function, the function \texttt{map} then transforms an $L^n \, \alpha$ into a monadic computation of an $L^n \, \beta$, incurring $\sum_{0 \leq i < n} C(i)$ amortized (\red{You should go back and make sure you're calling the static costs ``amortized cost" everywhere, that's nice.}) cost.


%\red{UGH, I should have done shifts by subtyping!!}


\section{\bilambdaamor}
\label{sec:bilambdaamor}
In order for \dlambdaamor to be useful as a programming language, it must be implementable! While a declarative type system on paper is useful for modeling and proving purposes, it has limited utility from a language design standpoint. While \dlambdaamor calculus described in Section~\ref{sec:dlambdaamor} is far more implementation-ready than its predecessor \lambdaamorminus, the rules of the type system do not provide us with an obvious implementation method. Traditionally, one hopes to implement a type system in a manner similar to implementing a definitional interpreter \red{cite reynolds here}. For each judgment of the type system, the programmer writes a function which essentially runs a proof search for that judgment.

For some of the judgments of \dlambdaamor, such as the sort-assignment judgment for index terms, a proof search procedure seems straightforward to define. For others, such as subtyping or the type-assignment judgment for terms, a few features of the type system present five immediate challenges.

\begin{enumerate}
  \item The main typing judgment is ambiguous. It is not at all clear which rule to apply at any given step of building a derivation, since the subtyping and weakening rules can always be tried at each stage. Indeed, one could always implement proof search for \dlambdaamor using backtracking, but it is preferable to avoid this if possible. Instead, we would like our implementation-ready calculus \bilambdaamor \red{mention the name earlier} to be \textit{syntax-directed} in the sense that the outermost syntax of the current term informs us which typing rule must be applied next in order to build a successful derivation. 
  
  \item \dlambdaamor includes full System F impredicative polymorphism, but a well-known result of \red{(figure out who)} states that type inference for System F is undecidable. Hence, we will not be able to design a type inference algorithm for \dlambdaamor. A natural second option is to shoot for implementing a type checker. Unfortunately, this too has its limitations. In order to implement proper type checking, the syntax of \dlambdaamor would have to be changed such that every variable binder includes a type annotation. This is a heavy burden on the programmer: annotating binders with types is tedious, error prone, and generally uninteresting\footnote{
As \citet{pierce:lics03} notes: ``The more interesting your types get, the less fun it is to write them down!"
%https://www.cis.upenn.edu/~bcpierce/papers/tng-lics2003-slides.pdf
  }. Instead, \bilambdaamor adopts \textit{bidirectional type checking}, a technique pioneered by \citehere which trades off some of the generality of full type inference for added ergonomics over standard type checking. The mechanics of this technique are discussed in Section~\textbf{??}
  
  \item \dlambdaamor's subtyping relation provides a challenge which should be familiar to the reader who is versed in the implementation of dependent type theories. The inclusion of the two subtyping rules S-Fam-Beta1 and S-FamBeta2 (found in Figure~\ref{fig:dlambdaamor-selected-typing-rules} or Figure~\textbf{??}) mean that the deciding the subtyping relation includes deciding $\beta$ equality at the type level. Luckily, the equational theory of types is simpler than that of a simply-typed lambda calculus, since the type-level lambda in \dlambdaamor $\lambda i : S. \tau$ ranges over index terms, not types. This allows for a very simple single-pass normalization procedure which decides the subtyping relation: this is discussed in Section~\textbf{??}.
  
  \item Many of the crucial rules of the \dlambdaamor subtyping relation include constraint satisfiability premises of the form $\Theta ; \Delta \vDash \Phi$. These premises will need to be discharged by an SMT solver. However, repeatedly pausing the subtyping algorithm to send constraints to a solver is inefficient. Instead, we would prefer to do one pass of typechecking, followed by a single call to the solver. To achieve this, the judgments of \bilambdaamor ``output" constraints. The intended meaning of this is that when the constraints are valid, the declarative version of the same judgment is derivable.
  
  \item The final barrier to implementation comes not from the refinement type or cost analysis features of \dlambdaamor, but simply from the fact that it is an affine type system. As an illustration, consider the typing rule T-TensorI: the ``input" context to the typechecker must be split into two disjoint parts which can be used in the two premises. This choice is nondeterministic: there is no way to know a priori what allocation of resources to give to each premise until later. To solve this, we employ a classical technique for implementing substructural type systems, known as \red{(is it though?)} the IO method.

\end{enumerate}


\subsection{Overview of Solutions (change this name)}

Below, we present the solutions to these five problems that we choose to adopt. All five solutions are well-known techniques, but to our knowledge \red{try a bit harder to make sure this is true before you say it...} \bilambdaamor is the first type system to show that they may all be simultaneously integrated into a single system. Moreover, since the five techniques are orthogonal, we present each feature of \bilambdaamor in isolation for a significantly more simple language. In Section~\textbf{??}, we show how all of the techniques are applied to \dlambdaamor to form the algorithmic type system \bilambdaamor.

\subsubsection{Bidirectional Type Systems}
Bidirectional type checking, also known as ``local type checking" is a type system algorithmization technique pioneered by \red{Pierce and Turner}. The technique works by separating the type checking judgment $\Gamma \vdash e : \tau$ of a declarative type system into two algorithmic judgments: $\Gamma \vdash e \checks \tau$ and $\Gamma \vdash e \infers  \tau$, which are read ``$e$ checks against $\tau$" and ``$e$ infers $\tau$" (sometimes ``synthesizes"), respectively. These two judgments are mutually-recursively defined in a specific manner. The process of turning a declarative type system into a bidirectional algorithmic one is straightforward to the point of mechanical: Dunfield and Pfenning \citehere provide a simple-to-follow recipe for this conversion, which extends from the simple type system they consider all the way to \dlambdaamor. 

Syntax-directed algorithmic type systems presented in a bidirectional style are trivially implementable: the implementation strategy is built into the structure of the rules. To implement a bidirectional type system, one writes two mutually-recursive functions \texttt{check:ctx->tm->typ->bool} and \texttt{infer:ctx->tm->typ} by recursion on the term input: the recursive calls are guided by the premises of each rule. Note that the types of these functions indicate the intended \textit{modes} of the three positions of the judgment, in the sense of logic programming. In the checking judgment, all positions are imagined to be \textit{inputs}, while the inference judgment indicates that the type position is an \textit{output} of the judgment.

As alluded to earlier, the ``inference" of the judgment $\Gamma \vdash e \infers \tau$ is not full inference, but merely ``local" inference: this judgment is derivable when enough information is present the form of $e$ to determine its type. This is in contrast to full type inference, where the type of a term may not be fully known until its type constraints are put in the context of those from the larger term in which it sits. For this reason, every syntactic form in the language has either an inference or checking rule: if requiring one of the premises to be inference gathers enough information to determine the type of the conclusion, then that conclusion will be an inference judgment. Otherwise, the judgment will be checking.

%\subsubsection{Subsumption and Annotation}

In order to mediate between the two judgments, bidirectional type systems include two special rules. First, is the rule which is traditionally referred to as ``subsumption": in order to show that $e \checks \tau$, it suffices to show that $e \infers \tau$. In other words, if $e$ can infer a type, then it checks against that type. This rule is usually strengthened by subtyping:
$$
\infer{\Gamma \vdash e \checks \tau}{\Gamma \vdash e \infers \tau' & \tau' \subty \tau}
$$ For $e$ to check against $\tau$, it suffices for $e$ to synthesize a more precise type $\tau'$.

Going in the other direction from a checking premise to an infering conclusion is somewhat more involved. In general, the desired converse rule is not true: there will always be terms such that $e \checks \tau$ but it is not the case that $e \infers \tau$. In order to remedy this, bidirectional type systems introduce a new piece of syntax to the declarative language on which they're based: annotations. When $e$ checks against $\tau$, the annotated term $(e : \tau)$ infers the type $\tau$:
$$
\infer{\Gamma \vdash (e : \tau) \infers \tau}{\Gamma \vdash e \checks \tau}
$$

These annotations must be manually added to terms by the programmer as they write the program. However, the only place where annotations are truly required are at the sites of \textit{bare $\beta$-redexs}. For example, to check the term $(\lambda x. e)\, e'$,, it must be annotated as $(\lambda x.e : \tau \to \sigma) \, e'$. Since most programs only contain bare $\beta$-redexes in the form of let-bindings, this requirement is both predictable and fairly ergonomic.

% \subsubsection{Soundness and Completeness}
As of yet, the relationship between a declarative calculus and its bidirectional algorithmic counterpart has been left unstated. However, the point of the bidirectional calculus is to be able to algorithmically generate declarative derivations! To this end, one always requires that the bidirectional type system be \textit{sound} for the declarative one.
\begin{theorem}[Bidirectional Soundness]
If $\Gamma \vdash e \checks \tau$, then $\Gamma \vdash e : \tau$
\end{theorem}
In other words, running $\texttt{check}(\Gamma,e,\tau)$ and getting \texttt{true} is sufficient to show that $e$ in fact has type $\tau$.

Conversely, completeness is also desirable, but not strictly necessary for bidirectional type systems. However, the most obvious statement of completeness ($\Gamma \vdash e : \tau$ implies $\Gamma \vdash e \checks \tau$) does not hold! This is because of the annotation requirement: the term $e$ may contain un-annotated bare $\beta$-redexes. For this reason, the following slightly weaker theorem is used as the completeness result for bidirectional type systems.
\begin{theorem}[Bidirectional Completeness]
If $\Gamma \vdash e : \tau$, then there exists $e'$ such that $\Gamma \vdash e' \checks \tau$, and $|e'| = e$, where $|e'|$ is the annotation-erasure of $e'$.
\end{theorem}
\label{thm:bidir-compl-example}
When proven constructively, this completeness result encodes an algorithm which inserts annotations into the term $e$ so that the resulting term checks against $\tau$. When Theorem~\ref{thm:bidir-compl-example} is proven directly by induction, the algorithm it encodes introduces far more annotations than is often strictly necessary: we improve on this with our completeness proof of \bilambdaamor in Section~\textbf{??} by proving an equivalent statement whose constructive proof inserts fewer annotations than the standard theorem.

\subsubsection{Algorithmic Subtyping and Normalization}
In order to implement the subsumption rule mentioned above, a decision procedure for the subtyping relation $\tau \subty \tau'$ is required. However, \dlambdaamor's subtyping is not immediately implementable for two important reasons.

Firstly, like \dlambdaamor's typing relation, it is not syntax directed: the transitivity rule S-Trans can be used at any step of a derivation. Similarly, the reflexivity rule T-Refl conflicts with all of the congruence rules. In order to avoid a backtracking implementation, it will be necessary to design an algorithmic subtyping relation for \bilambdaamor which includes neither of these rules. Of course, the algorithmic subtyping will need to be sound and complete for the declarative one. This requirement means that the algorithmic subtyping relation will need to have reflexivity and transitivity as admissible rules: in effect, we will need to prove identity and cut elimination.

The second (and more pernicious) problem is the inclusion of indexed types. While many refinement type systems (including DML \citehere, on which \lambdaamor's refinement types are based) include indexed types \citehere, they are usually implemented only as types of the form $\forall i : S. \tau$ of kind $\star$. While useful, these indexed types are not fully general, as their abstraction and application is controlled by term-level introduction and elimination rules. Instead, \dlambdaamor includes indexed types of the form $\lambda i : S. \tau$, which allow the programmer to use a richer set of types. But, the inclusion of type-level abstractions and applications requires the subtyping relation to include $\beta$ equalities for these indexed type families (S-Fam-Beta{1,2}): without them, the subtyping relation would not be able to judge relations like $(\lambda i : \N. L^i\, \tau) \, 3 \subty L^3 \, \tau$, where the subtying relation holds up to $\beta$ equality.

The inclusion of the two $\beta$-inequalities makes a simple algorithmic subtyping relation unlikely, since any way of deciding the subtyping relation must also decide $\beta$ equality of this small lambda calculus at the type level. However, the situation is sufficiently simple that we can get away with a fairly low-powered solution. To this end, \bilambdaamor's subtying relation is split into two phases. First, both types are evaluated (or \textit{normalized}) to normal forms, and then judged for subtyping by a relation which only contains the congruence rules. Since the abstractions $\lambda i : S.\tau$ range over \textit{index terms} and not types, a $\beta$ reduct has strictly fewer type connectives than its redex. For this reason, the normalization can be implemented in a single pass: substituting an index term for a free variable in a type in normal form yields another type in normal form. This two-phase algorithmic subtyping relation, as well as the normalization proof, are discussed in detail in Section~\textbf{??}

\subsubsection{Constraint Generation}
As motivated in Section~\textbf{?? intro}, most of the changes to \lambdaamorminus that result in \dlambdaamor are there for the purpose of simplifying the constraint-solving process that arises as a part of subtyping. Efficiently handling these constraints is crucial to an efficient implementation. For this reason, it is useful to defer the discharging of these constraint satisfiability premises of rules until \textit{after} the typechecking pass has finished.

We operationalize this in \bilambdaamor by designing each judgment to ``output" a constraint: we replace declarative judgments $\mathcal{J}$ with algorithmic ones $\mathcal{J} \gens \Phi$. For instance, the declarative sort-checking judgment $\Theta ; \Delta \vdash I : S$ of \dlambdaamor corresponds to the algorithmic judgment $\Theta ; \Delta \vdash I : S \gens \Phi$ from \bilambdaamor. The intended meaning of this (and the shape of the soundness theorem for an algorithmic judgment with a constraint output) is that if we can derive $\mathcal{J} \gens \Phi$ and $\Phi$ holds, then $\mathcal{J}$ is derivable.

This scheme is pervasive. Since every judgment in \dlambdaamor either has a rule with a constraint satisfaction premise or depends on one that does, every judgment in \bilambdaamor must emit constraints. The pattern in transforming a declarative judgment to an algorithmic one which emits constraints is fairly uniform: the output constraint of a rule is essentially the conjunction of the constraints output by its premises. One must also ensure that implications and quantifiers are inserted for constraints and index variables bound in premises: the logical structure of the output constraint mirrors the structure of the premises.

\red{Talk a bit here about where these arise in other places}

\subsubsection{I/O Method}
In order to maintain the soundness of potentials, \dlambdaamor has an affine type system. On top of the implementation challenges created by the fancier aspects of \dlambdaamor's type system, its affine-ness presents a well-understood barrier to implementation. To illustrate, consider writing the following case of the \texttt{check:ctx->tm->typ->bool} function from earlier.\footnote{
In order to simplify some of the presentation of this section, we will specialize to the non-bidirectional setting, and work in a simply-typed language where binders are fully annotated. \red{Should I say this in the main copy?}
}

$$
\texttt{check gamma Pair(e1,e2) Tensor(t1,t2) = } ??
$$

This case corresponds to the introduction rule for tensor,

%As outlined in Section~\textbf{??}, the implementation of bidirectional type systems usually takes the form of two mutually recursive functions \texttt{check:ctx->tm->typ->bool} and \texttt{infer:ctx->tm->typ}. These functions are implemented recursively on the second argument, and the recursive calls for a specific case are dictated by the premises of the corresponding typing rule. However, in the presence of an affine type system, this clean story is somewhat complicated. To illustrate, consider this simplified version of a first cut at the algorithmic tensor introduction rule.
$$
\infer{\Gamma_1,\Gamma_2 \vdash (e_1,e_2) : \tau_1 \otimes \tau_2}{\Gamma_1 \vdash e_1 : \tau_1 & \Gamma_2 \vdash e_2 : \tau_2}
$$


It is not at all clear how to proceed in this case. The tensor introduction rule prescribes that we make two recursive calls \texttt{check gamma1 e1 t1} and \texttt{check gamma2 e2 t2}, but provides no direction how to obtain \texttt{gamma1} and \texttt{gamma2} from \texttt{gamma}: the rule is presented in the standard way so that the two halves of the context are given at the outset.

This problem has two naive solutions. Firstly, one could analyze the structure of \texttt{e1} and \texttt{e2} in order to determine the variables they each use, and partition the context accordingly. Of course, this is very inefficient: even if done with a pre-processing step, this adds at least one pass through the term. Secondly, one could split the context \textit{symbolically}, and generate yet more constraints to unify at the end of the typechecking process.

Instead of either of these, we adopt a more principled approach based on the work of \citet{cervesato:tcs00}. In short, we extend the main typing judgment with yet another output-- this time a second context, which contains the variables which were unused in typing the term. A simplified version of the typing judgment takes the form $\Gamma \vdash e : \tau \gens \Gamma'$, where $\Gamma$ is the \textit{input context}, and $\Gamma'$ is the \textit{output context}. The key idea of this setup (known sometimes as the I/O method) is that we may thread the contexts through the premises of a rule as follows:

$$
\infer{\Gamma \vdash (e_1,e_2) : \tau_1 \otimes \tau_2 \gens \Gamma_2}{\Gamma \vdash e_1 : \tau_1 \gens \Gamma_1 & \Gamma_1 \vdash e_2 : \tau_2 \gens \Gamma_2}
$$

The first premise (the first component of the pair) has access to the entire input context, and it outputs $\Gamma_1$, the variables in $\Gamma$ which were unused in typing $e_1$. This context is then used as the \textit{input} context for checking $e_2$: since affine variables may be used at most once, the only variables which $e_2$ may access are those unused by $\Gamma_2$. This property is enforced ``in parallel" by splitting the context up front in the declarative rules, but it may similarly be enforced ``sequentially" by lazily deciding which premises may use which variables in this algorithmic styles.

The key rule in designing an algorithmic type system which uses the I/O method is of course the affine variable rule. When a variable is used, it must be removed from the output context:

$$
\infer{\Gamma \vdash x : \tau \gens \Gamma \setminus \{x\}}{x : \tau \in \Gamma}
$$

Moreover, this I/O method will be trivial to implement. We simply change the type of \texttt{check} and \texttt{infer} to output a context as well. Since these functions both receive and output a context, one can think of typechecking with the I/O method as happening inside a state monad of contexts, as opposed to the usual reader monad.

While this solution is clearly preferable to the naive ones efficiency-wise, it is not at all clear that this way of algorithmizing an affine type system is sound, much less complete, for the standard presentation of the rules. The proof of soundness is fairly straightforward, and relies on a simple intuition about the output context: if we can derive that $\Gamma \vdash e : \tau \gens \Gamma'$, then the variables used by $e$ are precisely $\Gamma \setminus \Gamma'$. Writing $\Gamma \vdash e : \tau$ (with no output context) as the declarative typing relation, we can prove

\begin{theorem}[Soundness of the I/O Method]
If $\Gamma \vdash e : \tau \gens \Gamma'$, then $\Gamma \setminus \Gamma' \vdash e : \tau$
\end{theorem}

Note that $\Gamma \setminus \Gamma'$ is well-defined because $\Gamma' \subseteq \Gamma$, a fact which must be proven by induction over the algorithmic rules.

The completeness theorem is simpler to state, but harder to prove.

\begin{theorem}[Completeness of the I/O Method]
If $\Gamma \vdash e : \tau$, then there is some $\Gamma'$ such that $\Gamma \vdash e : \tau \gens \Gamma'$
\end{theorem}

The proof of this theorem relies on the fact while weakening is not admissible for the (affine) declarative type system, it \textit{is} admissible for an algorithmic type system using the I/O method: if new variables are added to the input context, then they simply ``flow through" the judgment to the output context, and are left unused.

\begin{theorem}[Admissibility of Weakening for the I/O Method]
If $\Gamma \vdash e : \tau \gens \Gamma'$, then for all $\Gamma''$, we have that $\Gamma,\Gamma'' \vdash e : \tau \gens \Gamma',\Gamma''$
\end{theorem}

\subsection{Normalization of Types}
In order to circumvent the issue of deciding $\beta$-equality of types as a part of \bilambdaamor's subtyping routine, we employ a normalization (or evaluation) procedure to eliminate all $\beta$-redexes from a type. Once these $\beta$-redexes have been eliminated, subtyping only requires congruence rules. The normalization proof that we describe in this section is a normalization relative to the equational theory induced by \dlambdaamor's subtyping relation, that is to say: we will eventually prove that a type and its normal form are mutual subtypes of each other \textit{with the subtyping relation of \dlambdaamor}. This may seem strange-- after all, the normalization is required for \bilambdaamor's subtyping relation. However, we will see in Section~\textbf{??} that to prove the completeness of \bilambdaamor's algorithmic subtyping (Theorem~\textbf{??}), a normalization proof for \dlambdaamor's subtyping is exactly what's required. Moreover, the eventual soundness and completeness theorems for algorithmic subtyping will allow us to transport this normalization result to \bilambdaamor's type system when required.

This normalization procedure computes \textit{normal forms} for types, which should be thought of as canonical representatives of the $\beta$-equivalence classes of types. These normal forms can characterized syntactically: we present a pair of relations $\tau \, \texttt{ne}$ and $\tau \, \texttt{nf}$, which judge a type to be neutral or normal, respectively. Neutral types are those which can be of arrow kind, but will not induce any $\beta$-redexes when applied to, while normal types are types which include no $\beta$-redexes. The former are required to define the latter: the type $\tau \, I$ is only in normal form when $\tau$ is not of the form $\lambda i : S.\tau'$. The rules generating these two relations can be found in Appendix~\textbf{??}.

Before we present the normalization function, let us briefly take a moment to discuss why the solution we are about to present is incredibly simple. Proofs of normalization for most calculi require fairly high-powered proof techniques such as logical relations or categorical arguments. The inherent complexity of normalization proofs stems from the fact that straightforward induction on terms rarely works, since one would need to induct on substitution instances of lambda terms which are not subterms of the original term. However, \dlambdaamor's type-level lambdas do not range over types, they range over terms. Because of this, a substitution instance $\tau[I/i] : K$ of an open type $i : S \vdash \tau : K$ has the exact same number of type connectives as the open term. Further, substituting an index term into a type cannot introduce any new redexes, and so any substitution instance of an open type in normal form is also normal. These observations are codified in the following theorems.

\begin{theorem}
$\#\texttt{eval}(\tau) \leq \#\tau$, where $\#(\cdot)$ denotes the number of connectives in a type.
\end{theorem}
\begin{proof}
By induction on $\#\tau$
\end{proof}

\red{Very strange spacing here... fix this}

\begin{theorem}
~\begin{enumerate}
  \item If $\tau \; \texttt{ne}$ then $\tau[I/i] \; \texttt{ne}$
  \item If $\tau \; \texttt{nf}$ then $\tau[I/i] \; \texttt{nf}$
\end{enumerate}
\end{theorem}
\label{thm:idx-subst-nf}
\begin{proof}
We prove the two claims simultaneously by induction on the derivations of $\tau \; \texttt{ne}$ and $\tau \; \texttt{nf}$.
\end{proof}

Because of these simplifying factors, we can define an evaluation function \texttt{eval} defined inductively on the structure of types which computes normal forms.
The most important clauses of the definition can be found in Figure~\ref{fig:selected-eval-rules}. For all of the logical connectives, the definition proceeds compositionally-- the remaining rules can be found in Figure~\textbf{??}

\red{Factor out figures into sep. files}
\begin{figure}
\begin{mathpar}
  \texttt{eval}(\alpha) = \alpha
  
  \texttt{eval}(\lambda i : S. \tau) = \lambda i : S. \texttt{eval}(\tau)
  
  \\  
  
  \texttt{eval}(\tau \; I) = \begin{cases}
   \tau'[I/i] & \texttt{eval}(\tau) = \lambda i : S. \tau' \\
   \texttt{eval}(\tau) \; I & \text{otherwise}
                              \end{cases}
\end{mathpar}
\label{fig:selected-eval-rules}
\caption{Selected Clauses of the \texttt{eval} Function}
\end{figure}

The most important (and only nontrivial) clause of the definition is the application case. To evaluate $\tau \; I$, we begin by evaluating $\tau$. If its normal form
is a lambda, we simply perform the $\beta$-reduction. Note that we do not need to evaluate this substitution instance, as it must already be in normal form by Theorem~\ref{thm:idx-subst-nf}, assuming the correctness of the \texttt{eval} function. Otherwise, we simply re-apply the index term $I$.

Of course, it is not immediately clear that this function in fact computes what we want! In order for \texttt{eval} function to be a normalization procedure, its image must consist only of types in normal form, and every type must be equivalent to its evaluation. Note that we do not prove the stronger property that equivalence is completely characterized by syntactic equality of normal forms (up to satisfied equality of index terms). While almost certainly true, this property requires a bit more work to prove and is not required for the discussion in Section~\textbf{??}, and so we omit it. Finally, we must also prove that the \texttt{eval} function preserves kinds-- this proof follows the same inductive structure as the proof of normalization, and so we bundle them together. We present the case for evaluation below, and the remainder of the cases can be found in Appendix~\textbf{??}. The Normalization Theorem does depend on a small canonical forms lemma: types of arrow kind in normal form must either be lambdas or neutral.

\begin{theorem}[Canonical Forms for $S \to K$]
If $\Psi ; \Theta ; \Delta \vdash \tau : S \to K$ and $\tau \; \texttt{nf}$, then either:
\begin{enumerate}
  \item $\tau = \lambda i : S.\tau'$ with $\tau' \; \texttt{nf}$
  \item $\tau \; \texttt{ne}$
\end{enumerate}
\end{theorem}


\begin{theorem}[Normalization Theorem]
If $\Psi ; \Theta ; \Delta \pvdash \tau : K$, then:
\begin{enumerate}
  \item $\Psi ; \Theta ; \Delta \pvdash \texttt{eval}(\tau) : K$
  \item $\Psi ; \Theta ; \Delta \pvdash \tau \equiv \texttt{eval}(\tau) : K$
  \item $\texttt{eval}(\tau) \; \texttt{nf}$
\end{enumerate}
\end{theorem}

\red{Include the cut case here.}

\subsection{Algorithmic Type System of \bilambdaamor}
The syntax of \bilambdaamor is nearly identical to that of \dlambdaamor: this is of course by design, as \bilambdaamor is intended to be an implementable version of \dlambdaamor. The only difference is the addition of the type annotation syntax $(e : \tau)$ described in Section~\textbf{??}. The main change between the two type systems is in the forms of the judgments. Some judgments change in only minor ways: the sort-checking, kind-checking, and constraint well-formedness judgments are all the same as in \dlambdaamor, with the exception of the added constraint outputs as described in Section~\textbf{??}. The subtyping judgment also sports a constraint output, but is also is split into two, with first a ``normal form subtyping" relation which judges one type to be a subtype of another when both are in normal form, and then the general algorithmic subtyping relation which relates two types by normalizing them and then relating them via the normal form subtyping relation. Finally, the typing judgment changes the most: it splits into a checking ($\downarrow$) and inferring/synthesis ($\uparrow$) judgment to support bidirectional type inference, with added constraint outputs for solving and unused variable context output for the I/O method. These judgment forms are all shown in Figure~\ref{fig:bilambdaamor-typing-judgments}.

\begin{figure}
\label{fig:bilambdaamor-typing-judgments}
\caption{Judgment Forms of the \bilambdaamor Type System}
\end{figure}

\subsubsection{Sorts, Kinds, and Well-Formed Constraints}

\subsubsection{Algorithmic Subtyping}
%Talk about why this works! Why can we normalize first and then just do congruences?

\subsubsection{Bidirectional Typing Rules}

\section{Implementation of \lambdaamor}
\label{sec:lambdaamor-impl}