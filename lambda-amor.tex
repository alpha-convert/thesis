\section{Introduction}
Type systems are invaluable tools for developing robust and extensible modern software \citehere. Programming languages theorists have long understood the utility that type systems bring outside of the standard assurance that well-typed programs do not go wrong \citehere. Indeed, type systems can be designed to help programmers reason about myriad facets of their programs, including but certainly not limited to security and privacy \citehere, nondeterminism \citehere, computational effects \citehere, low-level representation details \red{[Kinds calling conventions]}, asynchronous communication \red{[Session types]}, staging \citehere, and program modularity \red{[Module systems]}. 
\\

Most relevant to this thesis, however, is the ability to create type systems which allow programmers to reason about their programs' resource usage. \red{more general background here about type systems for resource analysis}.


In this \red{chapter}, we will investigate and implement a variant of \lambdaamor, a language with a type system for amortized cost analysis. Programs written in \lambdaamor have types which are annotated with costs and potential, such that every type-correct program in \lambdaamor is a valid amortized analysis. Moreover, the costs expressed in the types give a sound upper bound on the actual execution cost of the program. \lambdaamor is expressive enough to statically verify amortized cost bounds for a wide class of functional programs, from the traditional examples of amortized analysis, to fully general cost-polymorphic higher-order functions like \texttt{map} and \texttt{fold}, which aren't well handled by existing resource-analysis languages like Resource Aware ML \citehere. \red{Should I talk more about Lambda-Amor here?}

By and large, the original creators of \lambdaamor were interested in it as a unifying framework for amortized analysis type systems. There are many axes along which one may design a type system, and \lambdaamor does a good job of interpreting many different styles. In this work, however, we will primarily interest ourselves in \lambdaamor's usefulness as a programming language which provides strong type-based cost reasoning principles to the user. To this end, the primary goal of this work is to design a version of \lambdaamor called \dlambdaamor which is amenable to implementation, and subsequently implement it.

We will begin in Section~\ref{sec:lambdaamor-overview} by giving an overview of the concepts \lambdaamor's type system draws on. \lambdaamor includes two modalities-- unary operators on types for tracking cost and potential, respectively. To soundly manage this potential, \lambdaamor is based on an affine logic in which every variable may be used at most once so that values with potential cannot be duplicated. In order to make complex potential functions, \lambdaamor uses refinement types in the style of DML \citehere, which we review. Next, we discuss the main obstacle the original type system presents to implementation: constraint solving. Our solution to this problem is based on univariate polynomial potential functions in the style of Automated Amortized Resource Analysis (AARA) \citehere, which we briefly review.
\red{Explain the contributions of other sections.}
%\red{Discuss it here! This is the part where you do the specs dump-- has two modalities, RAML-style ideas, affine types, refinement types, polymorphism. Then talk about game plan for algorithmic: bidirectional, I/O, constraint output/solving.}

\section{\dlambdaamor} 
\label{sec:lambdaamor-decl}
In this section, we will present the type system and semantics of \dlambdaamor, the variant of \lambdaamor which we plan to implement.
One of the most basic insights that \lambdaamor takes advantage of in its design is that costly computation can be thought of an effect\footnote{
In fact, cost can also be thought of as a \textit{coeffect} \cite{girard-et-al:tcs92:bll}, and one of the major breakthroughs of \lambdaamor is the unification
of both styles of resource tracking in a single calculus.
}. When a program does work, it has an effect on the world, namely the effect of taking time. In this sense, nearly all ``pure" programming languages are impure, as they allow pervasive use of the effect of cost. Unlike most languages, \lambdaamor encapsulates this effect by forcing all costly computation to happen in a monad \citehere.

\subsubsection{Cost Monad}
 However, a simple monad is not enough. We care not only that a term may incur cost, but how much cost it can incur! For this purpose, \lambdaamor uses a \textit{graded} monad \citehere $\M \; I \; \tau$. A computation of this type is a computation which returns a value of type $\tau$, and may incur up to $I$ cost, where $I$ is drawn from the sort of positive real numbers. As a graded modality, this monad's operations interact with the grade in nontrivial ways: for instance, the ``pure" computation $\texttt{ret}(e)$ has type $\M \; 0 \; \tau$ when $e : \tau$. Of course, any pure term may be lifted to a monadic computation which incurs no cost (\red{How do I explain that things run...}). Most importantly, given a costly computation $e_1 : \M \; I_1 \; \tau_1$ and a continuation $x : \tau_2 \vdash e_2 : \M \; I_2 \; \tau_2$, they can be sequenced into a computation $\texttt{bind}\, x = e_1\, \texttt{in}\, e_2 : \M \; (I_1 + I_2) \; \tau_2$. Note that the costs add-- a computation which may take up to $I_1$ units of time followed by a computation which takes up to $I_2$ units of course takes at most $I_1 + I_2$ units. However, neither \texttt{ret} nor \texttt{bind} incurs any nontrivial cost: any program written using only \texttt{ret}s and \texttt{tick}s will have type $\M \, 0 \, \tau$. For this, \lambdaamor includes a term $\texttt{tick}[I]$ of type $M \, I \, \texttt{unit}$, which incurs cost $I$. This is the only construct in \lambdaamor which incurs any ``extra cost": the idea is that programmers insert \texttt{tick}s in front of the operations their specific cost model dictates are costly. This technique is widely used in the cost analysis literature \citehere, and so \lambdaamor also adopts it for simplicity.
 
But of course, this cost monad can only be half the story. In a language which seeks to provide types for amortized analysis, a mechanism for handling potential is required.
 
\subsubsection{Potential Monad and Affine Types}
In addition to the cost monad, \lambdaamor includes another graded modality for tracking potential. A term of type $[I] \; \tau$ can be thought of a term of type $\tau$ which stores $I$ potential\footnote{
In some senses, potential in \lambdaamor behaves more like the credits of the banker's method discussed in Section~\ref{sec:amortized-primer}-- it can be created and attached to specific values. To avoid confusion, we follow \citet{rajani-et-al:popl21} with the terminology of ``potential"
}, where $I$ is again drawn from a sort of positive real numbers.
The most important operation associated with the potential modality is the ability to use potential to offset the cost of a computation. Concretely, given a term $e_1 : [I] \; \tau_1$ and a monadic continuation $x : \tau_1 \vdash e_2 : \M \; (I + J) \; \tau_2$, we can form the computation $\texttt{release}\, x = e_1 \, \texttt{in}\, e_2 : \M \; J \; \tau_2$. The crucial aspect of this construction is the fact that the resulting computation requires at most $J$ units of time to run, while the initial computation $e_2$ required $I + J$. Intuitively, we think of this as the $I$ units of potential ``paying for" $I$ steps of computation. 

Potential may also be created, and attached to values. In \lambdaamor, these two functions are handled by the same construct. For terms $e : \tau$, we may form $\texttt{store}[I](e) : \M \; I \; ([I] \; \tau)$, which is a computation which runs for at most $I$ units of time, and returns a $\tau$ with $I$ potential attached. The fact that \texttt{store} incurs this cost is what justifies the term \texttt{release}-- the program has paid an ``extra" cost of $I$ to create $[I] \; \tau$, and thus can exercise this option to reduce the cost of a subsequent computation with \texttt{release}.

This dynamic between \texttt{store} and \texttt{release} forces a restriction on the type system-- variables can only be used at most once. Our argument for the soundness of \texttt{release} relies on an the assumption that the potential we are releasing has not already been released elsewhere, and so duplication of variables must be disallowed. Of course, this kind of restriction is very common-- we simply require that \lambdaamor be \textit{affine}: weaking of the context is allowed, but contraction is disallowed. 

\subsubsection{Refinement Types}
\label{sec:lambdaamor-overview-refty}
So far, the situation we've described would only allow types with \textit{constant} amounts of potential. For nontrivial analyses, this is wholly insufficient: the potential of a data structure must be able to depend on the size or other numerical parameters of that data structure. For this purpose, \lambdaamor includes \textit{refinement types} in the style of Dependent ML \citehere. Concretely, \lambdaamor includes length-refined lists: a value of type $L^n \tau$ is a list of length $n$, where $n$ is an \textit{index term}-- an term in a small language of arithmetic expressions over a set of variables. Further, these index terms which appear in refinements may also appear in potentials! For instance, $\left[n^2\right] \; (L^n \tau)$ is the type of lists of length $n$ with potential $n^2$.

\subsection{Potential Vectors and AARA}
The story we've just told about \lambdaamor's type system is loyal to the original presentation in \citep{rajani-et-al:popl21}, but somewhat inadequate for implementation purposes. As we will discuss in Section~\ref{sec:lambdaamor-impl}, efficient subtyping is necessary for implementation of \lambdaamor. However,
the inclusion of the potential and cost modalities presents a challenge. In order for $[I] \; \tau_1$ to be a subtype of $[J] \; \tau_2$, it must be that $\tau_1 \subty \tau_2$, and that $J \leq I$. But as discussed above, $I$ and $J$ are index terms, and may be polynomials in a set of index variables. Ideally, we would like to discharge these inequalities generated by subtyping by constraint solver, but even modern SMT solvers struggle to handle polynomial inequalities.

To solve this problem, we borrow a key idea from Automatic Amortized Resource Analysis (AARA) \citehere which will allow us to generate only linear constraints over index variables, while still allowing univariate polynomial potentials and cost. The main idea is to fix a clever ``basis" for the space of polynomials, and then represent polynomials as a vector of their coefficients with respect to that basis. The basis in question is chosen to satisfy one key property: if $f(n)$ is written in terms of the basis, then the coefficients of $f(n-1)$ may be efficiently determined from $f(n)$. This property gives rise to the ability to easily analyze list algorithms in \lambdaamor: when writing a function $([f(n)] \, (L^n \, \tau)) \loli \sigma$, it is simple to pattern match on the argument and determine the type of the tail $[f(n-1)] \, (L^{n-1} \, \tau)$ to pass to a recursive call.

In \dlambdaamor, we will mostly syntactically restrict potential functions to be of this form, with some exception. We show in Section \textbf{??} that this language may be trivially elaborated into the original \lambdaamor, and further in Section \textbf{??} we show that the restricted set of allowable potential functions are still expressive enough for practical purposes.
\red{transition...}

\textbf{How do I cite that literally all of this is from JanH}

\begin{definition}[Potential Vector]
For a fixed $k$, we call a vector of nonnegative reals $(a_0,\dots,a_k)$ a potential vector.
\end{definition}

\begin{definition}[$\phi$ Function]
For fixed $k$, we define $\phi : \N \times \R_{\geq 0}^k \loli \R$ to be
$$
\phi\left(n,(p_0,\dots,p_k)\right) = \sum_{i=0}^k p_i\binom{n}{i}
$$
where $\binom{n}{r}$ is the binomial coefficient. We refer to the first argument of $\phi$ as the ``base", and the second argument as the ``potential".
\end{definition}

With $\phi$ in hand, we redefine the cost and potential modalities. In \dlambdaamor, the cost modality is written as $M \, (I,\vec{p}) \, \tau$ and the potential modality is $[I|\vec{p}] \,  \tau$. These two types classify values of type $\tau$ which cost up to $\phi(I,\vec{p})$ units of time and posess $\phi(I,\vec{p})$ potential, respectively.

\begin{theorem}[Monotonicity and Additivity of $\Phi$]
Let $\vec{p}$ and $\vec{q}$ be potential vectors.
\begin{enumerate}
  \item If $\vec{p} \leq \vec{q}$ componentwise, then $\phi(n,\vec{p}) \leq \phi(n,\vec{q})$.
  \item $\phi(n,\vec{p} + \vec{q}) = \phi(n,\vec{p}) + \phi(n,\vec{q})$
\end{enumerate}
\end{theorem}

The fact that $\phi$ is monotone in its second argument allows us to reduce the problematic subtyping rule for potentials (and costs) to generating linear inequalities and equalities \red{this isn't really true in presence of the sum...}: $[I|\vec{p}] \, \tau_1$ is a subtype of $[J|\vec{q}]$ when $I = J$ and $\vec{q} \leq \vec{p}$ componentwise.

The additivity of $\phi$ also allows us to simplify the bind and release- given a computation $e_1 : \M \, (I,\vec{p}) \, \tau_1$ and a continuation $x : \tau_1 \vdash e_2 : \M \, (I,\vec{q}) \, \tau_2$,  we may perform the computations in sequence with $\texttt{bind}\, x = e_1 \, \texttt{in}\, e_2 : \M \, (I,\vec{p} + \vec{q}) \, \tau_2$.

The final ingredient of this new version of the cost and potential modalities is the ability to change base. To illustrate, consider the process of writing a function $L^n \tau \loli \M \, (n,\vec{p}) \, \sigma$. The recursive call on the tail of the input list will have type $\M \, (n-1,\vec{p}) \, \sigma$, but the function expects a return value of type $\M \, (n,\vec{p}) \, \sigma$. Since the \texttt{bind} requires that the argument and the continuation have the same base, the recursive call cannot be used in this context, rendering it useless. To fix this, we include a term \texttt{shift} in \dlambdaamor which ``promotes" a computation of type $\M \, (n-1,\vec{p}) \, \sigma$ to one of type $\M \, (n,\vec{q}) \, \sigma$, for a specific $\vec{q}$ determined by $\vec{p}$. This concept is likely familar to the reader familiar with AARA: in Resource Aware ML (an implementation of OCaml based on AARA) this construct is baked into the pattern match rule, while we make it explicit.

\begin{definition}[Additive Shift]
For $\vec{p} = (a_0,\dots,a_{k-1},a_k)$ a potential vector, we define $\lhd \vec{p} = (a_0 + a_1,\dots,a_{k-1} + a_k,a_k)$
\end{definition}

\begin{theorem}
For $n \geq 1$ and $\vec{p}$ a potential vector, $\phi(n,\vec{p}) = \phi(n-1,\lhd \vec{p})$
\end{theorem}
\begin{proof}
It is straightforward to prove (either by combinatorial argument or direct computation) that $\binom{n-1}{i} + \binom{n-1}{i+1} = \binom{n}{i+1}$. Using this fact,
we may compute as follows:
\begin{align*}
  \phi(n-1,\lhd \vec{p}) &= \sum_{i=0}^k p_i\binom{n-1}{i} + \sum_{i=0}^{k-1} p_{i+1}\binom{n-1}{i}\\
                         &= p_0 + \sum_{i=1}^k p_i\binom{n-1}{i} + \sum_{i=0}^{k-1} p_{i+1}\binom{n-1}{i}\\
                         &= p_0 + \sum_{i=0}^{k-1} p_{i+1}\left(\binom{n-1}{i+1} + \binom{n-1}{i}\right)\\
                         &= p_0 + \sum_{i=0}^{k-1} p_{i+1}\binom{n}{i+1}\\
                         &= \sum_{i=0}^k p_i \binom{n}{i}\\
                         &= \phi(n,\vec{p})
\end{align*}
\end{proof}

\subsection{Syntax of \dlambdaamor}
In preparation to discuss \dlambdaamor's type system, we present its syntax in Figure~\ref{fig:dlambdaamor-syntax}.
\begin{figure}
\label{fig:dlambdaamor-syntax}
\caption{Syntax of \dlambdaamor}
\end{figure}

\subsubsection{Index Terms, Sorts, Kinds, and Constraints}
\dlambdaamor's refinement types are modeled in the style of DML \citehere, which takes the form of a two-level type system. As discussed in Section \textbf{??}, these refinements allow the user to assign types potential which depend on the sizes of data structures, such as the length of lists. These numerical values are denoted by \textit{index terms} ($I,J$) which decorate some of the types and surface syntax of \dlambdaamor. Index terms may be of three possible numerical \textit{base sorts}: natural numbers $\N$, positive real numbers $\R^+$, and potential vectors of some fixed length $k$, $\vec{\R^+}$. Additionally, \dlambdaamor also includes first-order sort-level functions.

The syntax of index terms themselves is generated by the standard arithmetic operations, along with constants, variables, and application/abstraction forms for the sort-level functions. Of special note are the \texttt{const} and $\Sigma$ constructs. For an index term $I$ of sort $\R^+$, the term $\texttt{const}(I)$ is of potential vector sort, and may be thought of as the ``constant" potential vector $(I,0,\dots,0)$, such that for all $n \N$, $\phi(n,\texttt{const}(I)) = I$.
The $\Sigma$ construct is as expected, although the upper bound is non-inclusive: the sum $\sum_{i=I_0}^{I_1} J$ sums from $J[I_0/i]$ to $J[(I_1-1)/i]$, as long as the range is nonempty, when the sum is of course zero.

\dlambdaamor also supports full System F-style impredicative polymorphism, as well as sort-indexed types. We denote the kind of types as $\star$. Note that sort-indexed types may have sort-level arrows in negative position, and so sort-function-indexed types are included also.

Finally, \dlambdaamor includes constraints over index terms, generated by conjunction, disjunction, implication, both kinds of quantification, as well as the trivially true and false propositions. Note that we will not provide a proof system for these constraints. Instead, we will only ever interact with constraints via an abstract satisfiability relation $\vDash$, and all the proofs of soundness and completeness in Section \textbf{??} will be relative to a decision procedure/oracle for $\vDash$.

\subsubsection{Types}
\dlambdaamor's types include all of the standard connectives from affine logic, namely positive and negative products ($\otimes$ and $\amp$), sums ($\oplus$), affine functions ($\loli$), and the exponential modality $! \tau$, whose values may be used more than once.
Of course, \dlambdaamor also supports a litany of more specialized types for amortized cost analysis.

Chief among these are the cost monad and potential types, A monadic type $M \, (I,\vec{p}) \, \tau$ classifies monadic computations of type $\tau$, which may incur up to $\phi(I,\vec{p})$ cost. The type formation rules (Figure \textbf{??}) ensure that $I$ is of sort $\N$, and $\vec{p}$ is of sort $\vec{\R^+}$. With the same restrictions on the sorts of its index terms, the potential type $[I|\vec{p}]\, \tau$ classifies values with at least $\phi(I,\vec{p})$ potential. In addition to the AARA-style potential, \dlambdaamor also has a ``constant potential" modality $[I] \, \tau$, whose values are those of type $\tau$, with $I = \phi(n,\texttt{const}(I))$ potential, for any $n$. While not strictly necessary for the theoretical development of \dlambdaamor, this modality is sometimes useful in practice.

Index variables may be quantified over in types with the $\forall i : S.\tau$ and $\exists i : S.\tau$ types, and polymorphic type variables are quantified over using the $\forall \alpha : K .\tau$ type constructor-- we do not support existential types, though there is no metatheoretical barrier to their inclusion.

As previously mentioned, the type of lists $L^I \, \tau$ is refined by length-- the values of this type all have length $I$. Next, \dlambdaamor also includes two ``constraint types", $\Phi \implies \tau$, and $\Phi \amp \tau$. Values of the first type are known to have type $\tau$ when $\Phi$ holds, and values of the second type are values of type $\tau$, along with an (irrelevant) proof of $\Phi$. As \lambdaamor has no error handling mechanism, this construct is helpful for statically preventing errors by encoding function pre and post-conditions in a type: for instance, the \texttt{head} function may be typed as $\forall n : \N. (n \geq 1) \implies (L^n \, \tau \loli \tau)$

Finally, \dlambdaamor's types include abstraction and application forms for indexed types. The abstraction form $\lambda i :S.\tau$ has kind $S \to K$ when $\tau$ has kind $K$, and so term variables will never have type $\lambda i : S.\tau$, as it is a higher-kinded type.

\subsubsection{Terms}
While the original presentation of \lambdaamor takes great care to include only a barebones term syntax, \dlambdaamor will have to expand this syntax somewhat to ensure that the textual representation of a program is unambiguous for programming purposes. Practically, this means that every logical connective has explicit syntactic introduction and elimination forms, whereas this is handled silently in \lambdaamor.

The term syntax for all of the standard connectives should be familiar. The two products are distinguished by double angled brackets for positive pairs, and parentheses for negative pairs. All binders are un-annotated to reduce the burden on the programmer: \bilambdaamor's bidirectional type inference means that type annotations will need to be written only when needed. Lists are constructed with nil and cons constructors, and the elimination form is a pattern match. The last standard inclusion is a fixpoint operator \texttt{fix}, which allows us to write recursive functions. 

The rest of the syntax is likely less familiar. The monadic type $M \, (I,\vec{p}) \, \tau$ has three operations associated with it: $\textbf{ret}(e)$ and $\texttt{bind} \, x = e_1 \, \texttt{in}\, e_2$, the unit and bind of the monad, respectively, as well as $\texttt{tick}[I|\vec{p}]$, which incurs a cost of $\phi(I,\vec{p})$.
%Type by type, give the constructors.
%Make sur to give constructors

\section{Algorithmic \lambdaamor}
\label{sec:lambdaamor-algo}
\red{Make sure you say that the syntax is inherited, plus annotations}
\section{Implementation of \lambdaamor}
\label{sec:lambdaamor-impl}
