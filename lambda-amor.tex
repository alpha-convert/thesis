\section{Introduction}
As anyone who's ever tried it knows, writing correct software is hard. Fortunately, decades of work in verification and interactive theorem proving for program correctness have brought forth a world of possibilities for future programmers to harness in their quest to build robust, correct, and extensible modern software \cite{ringer-et-al:qed}. The vast majority of work in this area is about \textit{functional} or \textit{extensional} correctness: proving that a program's input/output behavior matches the programmer's intended specification. Much less well studied is the correctness of programs with respect to \textit{intensional} properties: those which refer to \textit{how} a program runs, rather than simply what it computes. Of particular interest to this thesis is the intensional property of resource usage. While some intensional properties such as information flow can be rephrased in an extensional manner\footnote{For instance, noninterference \cite{spsm} is an extensional (hyper-)property which soundly underapproximates information flow control policies}, resource usage is inherently intensional. More specifically, as \autoref{ch:intro} suggests, we will restrict our view to a particular resource: that of \textit{cost}.

One particularly promising verification method based in interactive theorem proving is \textit{intrinsic} verification \cite{korkut-et-al:regex}, wherein the program being verified and the proof of its correctness are packaged together. In this approach, expressive (usually dependent) type systems are used to encode invariants in the types of the program, in such a way that a certificate that the program is type-correct is also a certificate of its functional correctness.

In this work, we apply the approach of intrinsic verification to proving cost bounds of functional programs. Unfortunately, the settings in which intrinsic verification of extensional properties is traditionally performed--- dependently-typed proof assistants such as Coq \cite{coq}, Agda \cite{norell:afp08}, or F* \cite{swamy-et-al:pldi13}, to name a few--- are not optimal settings for verifying cost bounds of programs. While some work \cite{mccarthy-et-al:flops16, danielsson:popl08, handley-et-al:popl20} has attempted to forge ahead and perform intrinsic cost analyses in dependently-typed languages, it is all limited in various ways by the fact that cost and potential is not a first-class notion in any of their logics\footnote{
Some dependent type theories such as Cost-Aware Type Theory (CATT) \cite{niu-harper:catt}, \textit{do} have first-class notions of cost, but there are no proof assistants built on top of them.
}.

Instead, the primary goal of this work is the development of \lambdaamorimpl, a domain-specific functional language for cost verification which combines a first class notion of cost with a rich refinement type system to statically verify time bounds of the programs written in it. By analogy to the behavioral invariants in the types of of traditional intrinsic analyses, programs written in \lambdaamor have types which enforce cost invariants. For instance, a function in \lambdaamorimpl could have a type like ``function from \texttt{nat} to \texttt{nat} which runs in no more than five steps". To expand the class of possible cost analyses, \lambdaamorimpl supports (as the name implies) amortized analysis. This is enabled by adding types which classify values carrying certain amounts of potential. These cost types and potential types can be combined in nontrivial ways to express nontrivial amortized analyses, which is often required when deriving tight bounds involves breaking data structure abstraction boundaries. Crucially, these cost and potential invariants are statically enforced by the type system: a certificate that a program in \lambdaamorimpl is type-correct is also a certificate that it is cost-correct. In this sense, programs written in \lambdaamorimpl \textit{are} cost analyses of themselves. This justifies an occasional reference to \lambdaamorimpl (or its core calculus) as allowing programmers to ``perform a cost analysis" of a program, ostensibly written elsewhere--- this amounts to simply re-writing the program in \lambdaamorimpl, with types that mirror the corresponding ``on-paper" analysis that would have been performed in \lambdaamorimpl's absence.


While other functional languages with amortized cost analysis capabilities (and resource analysis more generally) do exist, \lambdaamorimpl sits at a minimally-explored point in the design space. Some languages like Resource Aware ML \cite{hoffmann-et-al:cav12} aim for full automation, at the cost of struggling to handle some common language features. Other languages like TiML \cite{wang-et-al:oopsla17:timl} or LRT \cite{koth-et-al:icfp20} attempt to strike a balance by giving up on a degree of automation and requiring annotations from the programmer in order to make gains in expressiveness. In contrast to these languages\footnote{
A further comparison between \lambdaamorimpl and these languages along with others can be found in \autoref{sec:lambdaamor-related-work}
}, \lambdaamorimpl emphasizes expressiveness above all else. While this comes at the cost of some automation, \lambdaamorimpl is still backed by SMT, and so all of the \textit{quantative} proof obligations are handled automatically. 

The core of \lambdaamorimpl derives from a type system called \lambdaamor \cite{rajani-et-al:popl21}. By and large, the creators of \lambdaamor were interested in it as a unifying foundational framework in which one could embed \textit{other} cost analysis languages: there are many axes along which one may design a type system for resource analysis, and \lambdaamor serves as a calculus in which all sorts can be be emulated. In this work, however, we primarily interest ourselves in \lambdaamor's usefulness as a core calculus of a programming language which allows its users to prove cost bounds on the programs they write in it. \lambdaamor on paper is expressive enough to assign amortized cost bounds for a wide class of functional programs, from the traditional examples of amortized analysis such as functional queues and binary counters, to fully general cost-polymorphic higher-order functions like \texttt{map} and \texttt{fold}, which aren't well handled by existing resource-analysis languages like Resource Aware ML \cite{hoffmann-et-al:cav12}. This power and flexibility makes \lambdaamor a perfect starting point to develop the core calculus of \lambdaamorimpl.

%\red{FIXME}
%Because of the unique unifying nature of \lambdaamor, \lambdaamorimpl sits at a minimally-explored point in the design space of full-fledged resource-aware languages by trading off some automation for a degree of control and expressiveness not found elsewhere in the literature. This allows \lambdaamorimpl to fufil its goal of being used in place of traditional pencil-and-paper resource analysis techniques. 

The main contribution of this work is the design and theory of a version of \lambdaamor called \dlambdaamor which is amenable to implementation. This is accomplished by cutting out a fragment of \lambdaamor, and restricting its syntax somewhat. In the end, our changes will have been minor. \dlambdaamor bears all of the same major features as \lambdaamor: a pair of modalities for cost and potential, an affine substructural type system for soundly tracking the potential (values with potential must not be duplicated), and refinement types for encoding potential functions which vary in the sizes of data structures. The major change is the introduction of a construct to selectively restrict the forms of potential functions, borrowed from Automated Amortized Resource Analysis (AARA) \cite{hoffmann-et-al:esop10}. However, despite this work in cutting out an implementable fragment, \dlambdaamor does not admit a direct implementation, as its typing rules (just like \lambdaamor's) are written in declarative style, from which we cannot immediately construct an algorithm for type-checking or type inference. 

The traditional solution to this is to create yet another type system-- an \textit{algorithmic} one, from which a type-checker can be easily implemented. For this purpose, we will introduce \bilambdaamor, a type system which encodes the same typing relations as \dlambdaamor, but is presented in a manner that is trivial to implement. \bilambdaamor leverages several techniques from the type systems and type theory literature to algorithmize the \dlambdaamor type system. Most notably, \bilambdaamor makes use of bidirectional type inference \cite{pierce-and-turner:lti}, a technique which allows for the implementation of highly expressive type systems, while minimizing the amount of annotation required of the programmer. Additionally, \bilambdaamor harnesses normalization, syntax-directedness through admissible rules, and constraint generation to pave the way for an implementation.

Designing \bilambdaamor is a nontrivial task, and proving it correct even moreso. To show that \dlambdaamor and \bilambdaamor are the same type system presented in different ways (the latter being easily implementable), we must prove a bevy of theorems relating the two. This proof effort makes up the bulk of the technical contribution of this chapter. Finally, once the type system design and proof work is complete, we implement \bilambdaamor. Thanks to all of the work done in algorithmization, our implementation is at its core a trivial translation of the rules of \bilambdaamor into code.

The outline of the rest of the chapter is as follows.

\begin{figure}
\input{figs/lambdaamor-relationships}
\caption{Relationship Between Calculi}
\end{figure}

\begin{itemize}
 \item We will begin in \autoref{sec:dlambdaamor-overview} by giving an overview of the concepts \lambdaamor draws on. As mentioned previously, \lambdaamor includes two modalities for tracking cost and potential. To soundly manage this potential, \lambdaamor is based on an affine logic in which every variable may be used at most once so that values with potential cannot be duplicated. To make complex potential functions, \lambdaamor uses refinement types in the style of Dependent ML (DML) \cite{xi:jfp07}, which we review. Next, we discuss the main obstacle the original type system presents to implementation: constraint solving. Our solution to this problem is based on univariate polynomial potential functions in the style of AARA, which we introduce. This motivates the primary restriction of \dlambdaamor compared to \lambdaamor: costs and potentials are (with some exception) AARA-style univariate polynomials. Finally, we provide a more foundational account of \dlambdaamor's cost and potential modalities, based in linear logic.
 
 \item Next, in \autoref{sec:dlambdaamor-examples}, we explore some programs written ``on-paper" in the core calculus \dlambdaamor. These examples serve to illustrate the kind of cost-correctness proofs enabled by \dlambdaamor, and provide a first glimpse of how programs in \lambdaamorimpl will be packaged together with their cost-correctness proofs.  We present a wide variety of programs, each of which shows off a different facet of of \dlambdaamor's type system. These examples provide the beginnings of a comprehensive test suite against which we can evaluate our eventual implementation.
 
 \item Then, in \autoref{sec:bilambdaamor-overview}, we give an overview of \bilambdaamor, the algorithmic version of \dlambdaamor. To help motivate \bilambdaamor's creation, we begin by describing the pitfalls which make it impossible to directly implement \dlambdaamor. We then move to presenting a high-level overview of the techniques we use to avoid these implementation obstacles.
 
 
 \item In \autoref{sec:dlambdaamor-syntax-and-types}, we discuss the syntax and type system of \dlambdaamor in depth, providing intuition for the each of the judgments, and discussing selected rules from the type system. \dlambdaamor's type system is many-layered, with judgments for type formation, type assignment, and a smaller type system for the sub-language which governs the refinement types. We pay special attention to the rules which govern the cost-analysis-specific language features, and describe them in detail.
 
  \item In \autoref{sec:dlambdaamor-sound}, we sketch the soundness proof for \dlambdaamor, by showing that it may be embedded in \lambdaamor, and appealing to its soundness theorem featured in \citet{rajani-et-al:popl21}.
 
 \item In \autoref{sec:bilambdaamor-syntax-and-types}, we introduce the formalism for \bilambdaamor. While the majority of the syntax is carried over from \dlambdaamor, this formalism differs drastically from that of \dlambdaamor, and so we take time to explore the ways that the algorithmization features discussed previously in \autoref{sec:bilambdaamor-overview} are actually applied.
 
 \item In \autoref{sec:metatheory}, we prove that \bilambdaamor and \dlambdaamor are in fact (essentially) the same type system. This fact is a requirement for a good implementation, as it guarantees that our typechecker accurately and soundly types terms. The proof is broken into two parts. A proof of soundness tells us that when a typechecker derived from the algorithmic rules of \bilambdaamor confirms that an expression has a given type, our ``ground truth" declarative \dlambdaamor agrees. Dually, the proof of completeness ensures that every declaratively-derivable typing relationship in \dlambdaamor will be found by a typechecker which implements \bilambdaamor's algorithm.
 
 \item Finally, in \autoref{sec:lambdaamor-impl}, we discuss \lambdaamorimpl, our OCaml implementation of the \dlambdaamor. In order to support nontrivial programs, \lambdaamorimpl sports a top-level environment with multiple declaration types, on top of the simple typechecking prescribed by \bilambdaamor. We discuss these additions to the language, as well as the specific design choices made while building the artifact. We finish the section by writing the examples from \autoref{sec:dlambdaamor-examples} in \lambdaamorimpl, and benchmarking our implementation.
\end{itemize}


\section{Overview of \dlambdaamor} 
\label{sec:dlambdaamor-overview}
In this section, we will begin by presenting the overarching ideas which make \lambdaamor useful as a core calculus for our resource-aware language. Subsequently, we move to discussing its variant, \dlambdaamor, which we will focus on for the rest of the chapter.

One of the most basic insights that \lambdaamor takes advantage of in its design is that costly computation can be thought of an effect\footnote{
In fact, cost can also be thought of as a \textit{coeffect} \cite{girard-et-al:tcs92:bll}, and one of the major breakthroughs of \lambdaamor is the unification
of both styles of resource tracking in a single calculus.
}. When a program does work, it has an effect on the world, namely the effect of taking time. In this sense, nearly all ``pure" programming languages are impure, as they allow pervasive use of the effect of cost. In contrast to most languages, \lambdaamor enforces strict requirements on the use of this effect in particular. While many solutions to controlling effects have been explored in the literature \cite{lucassen1988polymorphic} \cite{plotkin2002computational}, \lambdaamor takes the approach of enforcing a monadic \cite{moggi91} discipline on the effect of cost.

\subsubsection{Cost Monad}
However, a simple monad is not enough. We care not only that a term may incur cost, but how much cost it can incur! For this purpose, \lambdaamor employs a \textit{graded} monad $\M \; I \; \tau$ to encapsulate the effect of cost \cite{gaboardi-et-al:icfp16}. A computation of this type returns a value of type $\tau$, and may incur up to $I$ cost, where $I$ is drawn from the sort of positive real numbers. As a graded modality, this monad's operations interact with the grade in nontrivial ways: for instance, the ``pure" computation $\texttt{ret}(e)$ has type $\M \; 0 \; \tau$ when $e : \tau$. This allows any pure term to be lifted to a monadic computation which incurs no cost. Most importantly, given a costly computation $e_1 : \M \; I_1 \; \tau_1$ and a continuation $x : \tau_2 \vdash e_2 : \M \; I_2 \; \tau_2$, the two can be sequenced into a computation $\texttt{bind}\, x = e_1\, \texttt{in}\, e_2 : \M \; (I_1 + I_2) \; \tau_2$. Note that the costs add: a computation which may take up to $I_1$ units of time followed by a computation which takes up to $I_2$ units takes at most $I_1 + I_2$ units. However, neither \texttt{ret} nor \texttt{bind} incurs any nontrivial cost: any program written using only \texttt{ret}s and \texttt{bind}s will have type $\M \, 0 \, \tau$. For this, \lambdaamor includes a term $\texttt{tick}[I]$ of type $M \, I \, 1$, which incurs cost $I$ (and $1$ is the unit type). This is the only construct in \lambdaamor which incurs any ``extra cost", the idea being that programmers insert \texttt{tick}s in front of the operations their specific cost model dictates are costly. This technique is widely used in the cost analysis literature \cite{danielsson:popl08}, and so \lambdaamor also adopts it for simplicity.
 
But of course, this cost monad can only be half the story. In a language which seeks to provide types for amortized analysis, a mechanism for handling potential is required.
 
\subsubsection{Potential Modality and Affine Types}
In addition to the cost monad, \lambdaamor includes another graded modality for tracking potential. A term of type $[I] \; \tau$ can be thought of a term of type $\tau$ which stores $I$ potential\footnote{
In some senses, potential in \lambdaamor behaves more like the credits of the banker's method discussed in \autoref{ch:intro}-- it can be created and attached to specific values. To avoid confusion, we follow \citet{rajani-et-al:popl21} with the terminology of ``potential"
}, where $I$ is again drawn from a sort of positive real numbers.
The most important operation associated with the potential modality is the ability to use potential to offset the cost of a computation. Concretely, given a term $e_1 : [I] \; \tau_1$ and a monadic continuation $x : \tau_1 \vdash e_2 : \M \; (I + J) \; \tau_2$, we can form the computation $\texttt{release}\, x = e_1 \, \texttt{in}\, e_2 : \M \; J \; \tau_2$. The crucial aspect of this construction is the fact that the resulting computation requires at most $J$ units of time to run, while the initial computation $e_2$ required $I + J$. Intuitively, we think of this as the $I$ units of potential ``paying for" $I$ steps of computation. 

Potential may also be created and attached to values. In \lambdaamor, these two operations are handled by the same construct. For terms $e : \tau$, we may form $\texttt{store}[I](e) : \M \; I \; ([I] \; \tau)$, which is a computation which runs for at most $I$ units of time, and returns a $\tau$ with $I$ potential attached. The fact that \texttt{store} incurs this cost is what justifies the term \texttt{release}-- the program has paid an ``extra" cost of $I$ to create $[I] \; \tau$, and thus can exercise this option to reduce the cost of a subsequent computation with \texttt{release}.

This dynamic between \texttt{store} and \texttt{release} forces a restriction on the type system: variables can only be used at most once. Our argument for the soundness of \texttt{release} relies on an the assumption that the potential we are releasing has not already been released elsewhere, and so duplication of variables must be disallowed. This kind of restriction is very common, as discussed in \autoref{sec:modal-and-substructural}: \lambdaamor is an affine type system.


\subsubsection{Refinement Types and Index Terms}
\label{sec:lambdaamor-overview-refty}
The situation we've described so far would only allow types with \textit{constant} amounts of potential. For nontrivial analyses, this is wholly insufficient, as the potential of a data structure must be able to depend on the size or other numerical parameters of that data structure. For this purpose, \lambdaamor includes \textit{refinement types} in the style of Dependent ML \cite{xi:jfp07}. Concretely, \lambdaamor supports length-refined lists. A value of type $L^I \tau$ is a list of length $I$, where $I$ is a term in a small language of arithmetic expressions over a set of variables, which we call an \textit{index term}. These index terms may also appear in potentials. For example, $\left[I^2\right] \; (L^I \tau)$ is the type of lists of length $I$ with potential $I^2$. 

\subsubsection{Index Term Quantifiers, Indexed Types, and Constraint Types}
To make good use of these refinements, \lambdaamor supports more refinement-related types. While not strictly part of the resource-analysis ``core" of \lambdaamor, these are required for practical use. First and foremost is the inclusion of universal and existential quantifiers over index terms, which allow for types like $(\tau \loli \sigma) \loli \forall n : \N. \left(L^n \,\tau \loli L^{n} \, \sigma\right)$, a possible type for a map function which can operate on lists of any length. \lambdaamor also includes a syntax for constraints over index terms, which take the form $I = J$, $I \leq J$, $I < J$ along with conjunctions, disjunctions, and implications thereof. These constraints are used in \textit{constraint types}. The conjunction constraint type $(n \geq 1) \amp \tau$ classifies values of type $\tau$ with an attached (irrelevant) proof of $n \geq 1$, while terms of the implication constraint type $(m + n = 1) \implies \sigma$ have type $\sigma$ when $m + n = 1$ is true. Finally, \lambdaamor sports indexed types, which can be thought of as type-level functions from sorts to types: $\lambda i : \N. L^i \tau$ is a function which, given a natural number $i$, produces the type $L^i \tau$. Note that this is not the same as $\forall i: \N. L^i \tau$, as they have different ``kinds": the first has kind $\N \to \star$ (a function which returns types), while the second has kind $\star$, the kind of types of terms.

\subsection{Potential Vectors and AARA}
The story we've just told about \lambdaamor is loyal to the original presentation in \citep{rajani-et-al:popl21}, but somewhat inadequate for implementation purposes. As we will discuss in \autoref{sec:bilambdaamor-overview}, efficient subtyping is necessary for implementation of \lambdaamor. However,
the inclusion of the potential and cost modalities presents a challenge. For $[I] \; \tau_1$ to be a subtype of $[J] \; \tau_2$, it must be that $\tau_1 \subty \tau_2$, and that $J \leq I$. But as discussed above, $I$ and $J$ are index terms, and may be polynomials in a set of index variables. Ideally, we would like to discharge these inequalities generated by subtyping by constraint solver, but even the most advanced SMT solvers struggle to handle polynomial inequalities.

To solve this problem, we borrow a key idea from AARA \cite{hoffmann-et-al:esop10} which will allow us to generate only linear constraints over index variables, while still allowing univariate polynomial potentials and cost. The main idea is to fix a clever ``basis" for the space of polynomials, and then represent polynomials as a vector of their coefficients with respect to that basis. The basis in question is chosen to satisfy one key property: if a real polynomial $f(n)$ is written in terms of the basis, then the coefficients of $f(n-1)$ may be efficiently determined from those of $f(n)$. This property gives rise to the ability to easily analyze list algorithms in \lambdaamor: when writing a function $([f(n)] \, (L^n \, \tau)) \loli \sigma$, it is simple to pattern match on the argument and determine the type of the tail $[f(n-1)] \, (L^{n-1} \, \tau)$ to pass to a recursive call.

In \dlambdaamor, we will syntactically restrict potential functions to be of this form, with some exception. It is intuitively clear that all AARA-style potential functions are expressible in the index term language of the original \lambdaamor\footnote{
While we do not prove this fact, a version of the requisite translation can be found in the code of the constraint elaboration pass described in \autoref{sec:lambdaamor-impl}.
}. This restricted potential form is also sufficiently expressive for practical purposes, as the examples we present in \autoref{sec:dlambdaamor-examples} show.

The reader accustomed to the literature surrounding AARA or Resource-Aware ML is likely to be familiar with the following presentation of AARA-style polynomial potential. The less familiar reader is encouraged to consult Hoffmann's Thesis \cite{hoffmann:thesis} for a more in-depth exposition of the technique.

\begin{definition}[Potential Vector]
For a fixed $k$, we call a vector of nonnegative reals $(a_0,\dots,a_k)$ a potential vector.
\end{definition}

\begin{definition}[$\phi$ Function]
For fixed $k$, we define $\phi : \N \times \R_{\geq 0}^k \to \R_{\geq 0}$ to be
$$
\phi\left(n,(p_0,\dots,p_k)\right) = \sum_{i=0}^k p_i\binom{n}{i}
$$
where $\binom{n}{r}$ is the binomial coefficient. We refer to the first argument of $\phi$ as the ``base", and the second argument as the ``potential".
\end{definition}

With $\phi$ in hand, we redefine the cost and potential modalities. In \dlambdaamor, the cost modality is written as $M \, (I,\vec{p}) \, \tau$ and the potential modality is $[I|\vec{p}] \,  \tau$. These two types classify values of type $\tau$ which cost up to $\phi(I,\vec{p})$ units of time and posess $\phi(I,\vec{p})$ potential, respectively.

\begin{theorem}[Monotonicity and Additivity of $\Phi$]
\label{thm:phi-linear}
Let $\vec{p}$ and $\vec{q}$ be potential vectors.
\begin{enumerate}
  \item If $\vec{p} \leq \vec{q}$ componentwise, then $\phi(n,\vec{p}) \leq \phi(n,\vec{q})$.
  \item $\phi(n,\vec{p} + \vec{q}) = \phi(n,\vec{p}) + \phi(n,\vec{q})$
\end{enumerate}
\end{theorem}

This theorem has two main consequences for the new cost and potential modalities. First, the fact that $\phi$ is monotone in its second argument allows us to reduce the problematic subtyping rule for potentials (and costs) to generating linear inequalities and equalities\footnote{
This is somewhat inaccurate: the presence of a sum construct in the language of index terms breaks linearity, but this is rarely a problem in practice.
} $[I|\vec{p}] \, \tau_1$ is a subtype of $[J|\vec{q}]$ when $I = J$ and $\vec{q} \leq \vec{p}$ componentwise. Second, the additivity of $\phi$ also allows us to pass from addition of polynomial index terms in the \texttt{bind} and \texttt{release} rules to componentwise (linear) addition on potential vectors. Given a computation $e_1 : \M \, (I,\vec{p}) \, \tau_1$ and a continuation $x : \tau_1 \vdash e_2 : \M \, (I,\vec{q}) \, \tau_2$,  we perform the computations in sequence with $\texttt{bind}\, x = e_1 \, \texttt{in}\, e_2 : \M \, (I,\vec{p} + \vec{q}) \, \tau_2$.

For convenience, it is sometimes useful to consider a restricted version of the orignal \lambdaamor cost monad in \dlambdaamor: for this we will sometimes write $\M \, \vec{p} \, \tau$ to mean $\forall j : \N. \M \, (j,\vec{p}) \, \tau$. Intuitively, this ought to be considered the same as $\M \, (0,\vec{p}) \, \tau$, since a computation that costs at most $\phi(j,\vec{p})$ for any $j$ must be bounded above by $\phi(0,\vec{p})$ by monotonicity of $\phi$.\footnote{
The $\forall j : \N...$ being \textit{irrelevant} is crucial here: since one cannot pattern match on the value of $j$, the cost of the computation must be uniform in $j$.} Rather than writing this type as such, we instead use the universally-quantified type $\forall j : \N. \M \, (j,\vec{p}) \, \tau$ to ensure that it can be composed with any other computation: the \texttt{bind} rule requires that the base of the term and the base of the continuation be equal. It is easy to compute that $\phi(0,\langle p_0,\dots,p_k \rangle) = p_0$, and so it is reasonable to think of a term of type $\M \, \langle p_0,\dots,p_k\rangle \, \tau \equiv \forall j : \N. \M \, (j,\langle p_0,\dots p_k\rangle)\, \tau$ as being a computation of a $\tau$ which takes $p_0$ time: throwing away the higher-order terms of the cost allows us to emulate the original \lambdaamor's ``constant" cost monad. Practical concerns necessitate the addition of one more construct into the mix: the constant potential vector $\texttt{const}(I)$, where $I$ is of sort $\R^+$. Intuitively, we think of this as being the potential vector $\langle I,0,\dots,0 \rangle$, such that $\phi(n,\texttt{const}(I)) = I$, for any $n$.

The final ingredient of this new version of the cost and potential modalities is the ability to change base. To illustrate, consider the process of writing a function $L^n \tau \loli \M \, (n,\vec{p}) \, \sigma$. The recursive call on the tail of the input list will have type $\M \, (n-1,\vec{p}) \, \sigma$, but the function expects a return value of type $\M \, (n,\vec{p}) \, \sigma$. Since the \texttt{bind} requires that the argument and the continuation have the same base, the recursive call cannot be used in this context, rendering it useless. To fix this, we include a term \texttt{shift} in \dlambdaamor which ``promotes" a computation of type $\M \, (n-1,\vec{p}) \, \sigma$ to one of type $\M \, (n,\vec{q}) \, \sigma$, for a specific $\vec{q}$ determined by $\vec{p}$. This concept is likely familar to the reader familiar with AARA: in Resource Aware ML (an implementation of OCaml based on AARA) this construct is baked into the pattern match rule, while we make it explicit.

\begin{definition}[Additive Shift]
\label{defn:shift}
For $\vec{p} = (a_0,\dots,a_{k-1},a_k)$ a potential vector, we define $\lhd \vec{p} = (a_0 + a_1,\dots,a_{k-1} + a_k,a_k)$
\end{definition}

\begin{theorem}
\label{thm:raml-shift}
For $n \geq 1$ and $\vec{p}$ a potential vector, $\phi(n,\vec{p}) = \phi(n-1,\lhd \vec{p})$
\label{thm:shift}
\end{theorem}
\begin{proof}
%It is straightforward to prove (either by combinatorial argument or direct computation) that $\binom{n-1}{i} + \binom{n-1}{i+1} = \binom{n}{i+1}$. Using this fact,
%we may compute as follows:
Follows from the fact that $\binom{n-1}{i} + \binom{n-1}{i+1} = \binom{n}{i+1}$, and unfolding definitions.
\iffalse
\begin{align*}
  \phi(n-1,\lhd \vec{p}) &= \sum_{i=0}^k p_i\binom{n-1}{i} + \sum_{i=0}^{k-1} p_{i+1}\binom{n-1}{i}\\
                         &= p_0 + \sum_{i=1}^k p_i\binom{n-1}{i} + \sum_{i=0}^{k-1} p_{i+1}\binom{n-1}{i}\\
                         &= p_0 + \sum_{i=0}^{k-1} p_{i+1}\left(\binom{n-1}{i+1} + \binom{n-1}{i}\right)\\
                         &= p_0 + \sum_{i=0}^{k-1} p_{i+1}\binom{n}{i+1}\\
                         &= \sum_{i=0}^k p_i \binom{n}{i}\\
                         &= \phi(n,\vec{p})
\end{align*}
\fi
\end{proof}

The shift operator allows us to define the proper type of the \texttt{shift} operator: $\texttt{shift}(e)$ has type $\M (n,\vec{p}) \, \tau$ when $e$ has type $\M (n-1,\lhd\vec{p})$. This shift in perspective will be of critical importance when performing AARA-style analyses: when required to provide a term of type $\M (n,\vec{p}) \, \tau$, we will often find ourselves in posession of only costly computations with base $n-1$, and so it we will be required to \texttt{shift} our perspective.

\subsection{Cost and Potential Foundations}
The more logically-inclined reader is likely to be unsatisfied with the presentation of the potential and cost modalities thusfar. Luckily, the two modalities are far from ad-hoc. In fact, they can easily be explained as being user-optimized instances of a more basic phenomenon. Consider affine types with an additional atomic type $P$ which encodes a single unit of potential. The potential type $[I|\vec{p}] \, \tau$ can be encoded as $P^{\phi(I,\vec{p})} \otimes \tau$, where $P^k$ is the $k$-fold tensor of $P$ with itself\footnote{
This analogy breaks down slightly when $\phi(I,\vec{p})$ is not an integer, but the principle stands.
}. This is in line with our intuitive understand of the potential type as carrying $I$ potential along with a value of type $\tau$. On the other hand, the cost type $M \,(I,\vec{p}) \, \tau$ is encoded as $P^{\phi(I,\vec{p})} \loli \tau$. This presents a monadic computation which costs up to $\phi(I,\vec{p})$ as a function \textit{requiring} that much potential to be provided in order to produce the result.

%Compile the ops to basic ones
Under this framing, all of the operations on costs and potentials in \dlambdaamor can be thought of in this way by considering their denotations in this model. For example, the \texttt{store} operation, which has type $\tau \loli \M \, (I,\vec{p}) \, ([I|\vec{p}] \, \tau)$ can be thought of as having type $\tau \loli P^{\phi(I,\vec{p})} \loli P^{\phi(I,\vec{p})} \otimes \tau$, which is plainly the pairing function. Similarly, the \texttt{release} function of type $[I|\vec{p}] \, \tau \loli (\tau \loli \M \, (I,\vec{p} + \vec{q}) \, \sigma) \loli \M \, (I,\vec{q})\, \sigma$ can instead be thought of a function of type:
$$
P^{\phi(I,\vec{p})} \otimes \tau \loli (\tau \loli P^{\phi(I,\vec{p} + \vec{q})} \loli \sigma) \loli P^{\phi(I,\vec{q})} \loli \sigma
$$
which operates by using \autoref{thm:phi-linear} to equate the types $P^{\phi(I,\vec{p})} \otimes P^{\phi(I,\vec{q})} = P^{\phi(I,\vec{p}) + \phi(I,\vec{q})} = P^{\phi(I,\vec{p} + \vec{q})}$.

This ``model" of \dlambdaamor also serves to motivate two of the fundamental strictures of programming in it. Since we have described no introduction forms for the type $P$, it is impossible to construct a closed term of that type. This is in line with the intended mental model of potentials in \dlambdaamor and amortized analysis more generally: potentials are unobservable, and thus should never ``escape" a program. The first consequence of this fact is that there are no closed terms of type $[I] \, \tau$: potential can only occur under a monadic computation. Second, given a function $P \loli \tau$, one should not be able to recover a $\tau$ in a closed context. Correspondingly, a programmer in \dlambdaamor can never internally ``run" a monadic computation of type $M \, (I,\vec{p}) \, \tau$ to get at the underlying $\tau$. This stricture is similar in spirit to Haskell's IO monad, whose terms cannot be evaluated except at the interactive top level.


Next, considering the two modalities in this model makes plain their potentially confusing subtyping rules. While $P$ is not a part of \dlambdaamor, its subtyping is the primary source of confusion, as the the actual subtyping rules in \lambdaamor are inherited from this model. Intuitively, we think of $P$ as being a subtype of the unit type $1$, as one can always discard ``an atom of potential". From this, it follows that $P^k \subty P^\ell$ when $\ell \leq k$. More potential can always be used in place of less\footnote{
Alternatively, this can be thought of as being analogous to width subtyping for records.
}. Note the contravariance in this subtyping rule: the ordering in amounts of potential is the opposite of the ordering on numbers. From this intuitive understanding of the subtyping in this atomic potential model, we can derive the subtyping rules of the cost and potential modalities in \dlambdaamor. We begin by supposing that $\vec{q} \leq \vec{p}$. By \autoref{thm:phi-linear}, we have $\phi(I,\vec{q}) \leq \phi(I,\vec{p})$, which implies that  $P^{\phi(I,\vec{p})} \subty P^{\phi(I,\vec{q})}$ as discussed. For the subtyping rule for potentials, we use the $\otimes$ subtyping rule  to get $P^{\phi(I,\vec{p})} \otimes \tau \subty P^{\phi(I,\vec{q})} \otimes \sigma$, when $\tau \subty \sigma$. However, these types are $[I|\vec{p}] \, \tau$ and $[I|\vec{q}] \, \sigma$, respectively, which justifies our subtyping rule: to show that $[I|\vec{p}] \, \tau \subty [I|\vec{q}] \, \sigma$, it suffices to have $\vec{q} \leq \vec{p}$ and $\tau \subty \sigma$. For costs, we use the $\loli$ subtyping rule to get  $P^{\phi(I,\vec{q})} \loli \tau \subty P^{\phi(I,\vec{p})} \loli \sigma$, when $\tau \subty \sigma$. This time, the types are $\M \, (I,\vec{q}) \, \tau$ and $\M \, (I,\vec{p}) \, \sigma$, which again justifies our eventual rule: to show $\M \, (I,\vec{q}) \, \tau \subty \M \, (I,\vec{p}) \, \sigma$, it suffices to have $\vec{q} \leq \vec{p}$ and $\tau \subty \sigma$.

Finally, while this model of the cost and potential modalities is useful, it is by no means complete. In other words, there are types which are inhabited in the model whose counterparts are uninhabited in \dlambdaamor. A key example is the lack of a term corresponding to potential ``application". In the model, the type $P^k \otimes (P^k \loli \tau) \loli \tau$ is inhabited by the $\lambda$-term which performs the application. Meanwhile in \dlambdaamor, the corresponding type $[I|\vec{p}] \, (\M \, (I,\vec{p})\, \tau) \loli \tau$ is uninhabited. The only way to use a term with potential is to \texttt{release} the potential into a monadic computation, which the target of this function is not. Another example is the lack of a potential fusion law. Since potentials only occur under the cost monad, the potential modality is not itself a monad: there is no term inhabiting $[I|\vec{p}] ([I|\vec{q}] \, \tau) \loli [I|\vec{p} + \vec{q}] \, \tau$. However, the potential join can be written in an ambient monadic context. In other words, there is a term of type $[I|\vec{p}] ([I|\vec{q}] \, \tau) \loli \M \, (I,\vec{0}) \left([I|\vec{p} + \vec{q}] \, \tau\right)$. 

%Another example is the lack of $0$-graded units and counits for both modalities: since $\phi(I,\vec{0}) = 0$ and $P^0 = 1$, both $P^{\phi(I,\vec{0})} \loli \tau$ and $P^{\phi(I,\vec{0})} \otimes \tau$ are interprovable with $\tau$ via standard $\lambda$-terms. However, the corresponding types in \dlambdaamor ($ \tau \loli [I|\vec{0}] \, \tau$, and $\M \, (I,\vec{0})\, \tau \loli \tau$, etc) are uninhabited.


\section{Examples of Programs in \dlambdaamor}
\label{sec:dlambdaamor-examples}
In this section, we will present a number of examples of programs written in \dlambdaamor, each of which exemplifies a different component of its cost analysis features. While we have not formally introduced the syntax of \dlambdaamor yet, we provide a simple term for the first example in \autoref{fig:example-dlambdaamor-addone} to illustrate the way programs are intertwined with their proofs. These examples will loosely follow the presentation of Section 3 of \citet{rajani-et-al:popl21}, where more in-depth discussion can be found.

\subsubsection{Add One}
We begin with a (very) simple example to demonstrate the utility of \dlambdaamor's AARA-style costs. Consider writing a function $\texttt{addOne}$, which adds one to each integer in a list. If we assume the cost model that natural number addition costs one unit of time, the function would have type $\forall n : \N. \, L^n(\texttt{nat}) \loli \M \, (n,\langle 0,1 \rangle) \, \left(L^n(\texttt{nat})\right)$. Recalling the intended meaning of the AARA-style cost functions, this means that \texttt{addOne} costs $\phi(n,\langle 0,1 \rangle) = n$ in total, where $n$ is the length of the input list (and also the output). This makes sense, as each entry in the list incurs a single cost to add one to it. Pseudocode of the term for this type can be found in \autoref{fig:example-dlambdaamor-addone}. The operational aspects of the program are exactly what one expects from an instance of map. More interesting are the cost-related aspects of the code. In the cons branch, we immediately \texttt{shift}. This allows us to provide a term of type  $\M \, (n-1,\langle 1,1 \rangle) \, \left(L^n(\texttt{nat})\right)$ in place of the expected type $\M \, (n,\langle 0,1 \rangle) \, \left(L^n(\texttt{nat})\right)$. Although this is guaranteed to be by \autoref{thm:shift}, we can check that this is sound by computing that $\phi(n-1,\langle 1,1 \rangle) = (n-1) + 1 = n = \phi(n,\langle 0,1 \rangle)$. This shift is required to perform the recursive call on the tail: $\texttt{addOne} \, [n-1] \, ys$ has type $\M \, (n-1,\langle 0,1 \rangle) \, \left(L^{n-1} (\texttt{nat})\right)$, which can only be bound into a continuation which results in something of type $\M \, (n-1, \_) \, \_$. Further, the shift ``exposes" the one constant cost, which is incurred by the tick (which we attribute to the addition). This raises a crucial point: a ``hole" in a program expecting $\M \, (n,\langle 0,1 \rangle) \, \tau$ cannot accept a term of type $\M \, (n, \langle 1,0 \rangle) \, \tau$ for any $n$, despite this being semantically sound for $n \geq 1$.

\begin{figure}
\input{figs/example-dlambdaamor-addone}
\caption{\texttt{addOne} function in \dlambdaamor}
\label{fig:example-dlambdaamor-addone}
\end{figure}


This example can also be performed using potentials, rather than costs. Instead of a function which incurs $n$ cost, we can instead think of \texttt{addOne} as a free-to-execute function which expects $n$ potential. One possible choice for this function's type is:
$$\forall n : \N .\, [n|\langle 0,1 \rangle] \, 1 \loli L^n(\texttt{nat}) \loli \M \, (n,\langle 0,0 \rangle) \, \left(L^n(\texttt{nat})\right)$$
This style is reminiscent of the ``gas-cost" analyses from \autoref{ch:intro}, as we expect $n$ gas up front to run, and spend it all towards performing the additions. For technical reasons relating to expressivity\footnote{
In short, coeffect-style analyses require the use of potentials.
}, we often use this style (preferring the type $[I] \, \tau \loli \M \, 0 \, \sigma$ over the type $\tau \loli \M \, I \, \sigma$) even in cost analyses which are not amortized.

Another option is a type which attaches a single potential to each element of the input list, in a style indicative of the Banker's method:
$$
\forall n : \N .\, L^n\left([1] \, \texttt{nat}\right) \loli \M \, (n,\langle 0,0 \rangle) \, \left(L^n(\texttt{nat})\right)
$$
The terms corresponding to both of these types can be found in \autoref{appendix:a}. This cost analysis is tight and fairly uninteresting: it requires no ``real" amortized analysis. To illustrate how \dlambdaamor handles describing cost analyses for programs where amortization is required for tight bounds, we show how a few classic examples of amortized analysis can be written in \dlambdaamor.

\subsubsection{Insertion Sort}
Our second example will illustrate how the AARA-style costs of \dlambdaamor will allow us to verify quadratic-and-higher cost bounds, while only ever solving linear constraints. Insertion sort is a good example of this class of program, since its cost analysis is very understandable (nested loops, nothing fancy), while still being an interesting function.

For insertion sort, we will assume the traditional cost metric for sorting algorithms: all comparisons cost one unit. The insertion sort we will write will be monomorphic, and assume a comparison operator of type
$$
\texttt{leq} \, : \, \tau \otimes \tau \loli \M \, \langle 1 \rangle \, 2
$$
where $2$ is defined to be $1 \oplus 1$.

Since elements of the list will have to be compared multiple times, we will use the exponential modality ($!\tau$ in \autoref{ch:intro}) to have insertion sort operate over lists of infinite-use $\tau$s. With this in mind, we give the insertion function the following type:
$$
\texttt{insert} \, : \, \forall n : \N. \, !\tau \loli L^n\left(!\tau\right) \loli \M \, (n,\langle 0,1\rangle) \, \left(L^{n+1}\left(!\tau\right)\right)
$$
The type of this function should be intuitive: at worst, we scan the list once, incurring $n = \phi(n,\langle 0,1\rangle)$ cost. Folding this function over a list yields the insertion sort algorithm, which has the type shown below.
$$
\texttt{ins_sort} \, : \, \forall n : \N. \, L^n\left(!\tau\right) \loli \M \, (n,\langle 0,0,1\rangle) \, \left(L^n\left(!\tau\right)\right)
$$

We can compute that $\phi(n,\langle 0,0,1\rangle) = \binom{n}{2} = \frac{1}{2}n^2 - \frac{1}{2}n$, which gives \texttt{ins_sort} the requisite quadratic cost bound.
This cost analysis is fairly elementary, but it's important to note the design of \dlambdaamor (via AARA) is what makes this analysis possible. Because the constraints on polynomial functions are only ever coefficient-wise and linear, checking the corresponding terms (found in \autoref{appendix:a}) is easy for SMT solvers, despite the cost bound being quadratic. 

%Moreover, the terms themselves are reached by inserting the \texttt{ret}s, \texttt{bind}s, and \texttt{shift}s into the proper locations in the term one would already expect to have to write when writing this function.


\subsubsection{Functional Queue}
The first example of amortized analysis is the traditional functional queue \cite{okasaki:purely-functional-data-structures}. Here, a queue is represented as a pair of lists, $l_f$ and $l_r$, which we refer to as the front and rear lists, respectively. To enqueue an element, we cons it to the head of the front list, and to dequeue, an element is removed from the head of the rear list. If the rear list is empty when a dequeue operation is issued, the front list is reversed into the rear.

If we assume that cons operations are the only costly operation, and that they each incur one cost, this dequeue operation has worst-case complexity $O(n)$ where $n$ is the size of the queue (the sum of the sizes of $l_f$ and $l_r$). While ``most" calls to dequeue will be $O(1)$, the worst case is $O(n)$ since the function needs to reverse the entire front list whenever the rear list is emppty. However, by employing the banker's method, we may enforce the invariant that each element of the front list carries two credits to be used to pay for its eventual reversal. Under this scheme, both enqueue and dequeue are constant amortized time.

This entire informal analysis is captured formally by the types of the enqueue and dequeue operations in \dlambdaamor. To encode this analysis, we define a queue to be of type $ L^n([2] \, \tau) \otimes L^m \, \tau$: a pair of $\tau$-lists, where the front has $2$ potential on each of its $n$ elements.

The enqueue function has the following type.

$$
\texttt{enq} \; : \; \forall n,m : \N. \, [3] \, 1 \loli \tau \loli L^n([2] \, \tau) \otimes L^m \, \tau \loli \M \, \langle 0 \rangle \, \left(L^{n+1}([2] \, \tau) \otimes L^m \, \tau\right)
$$

From a queue and three extra potential, we may enqueue a single element, resulting in queue with one more element on its front list, for no cost. The term implementing \texttt{enc} can be found in \autoref{appendix:a}. The type of dequeue is somewhat more involved, since the sizes of the output lists are not a simple function of this inputs. In addition, the function has a precondition: the queue cannot be empty. These two numerical restrictions provide a nice illustration of \dlambdaamor's refinement types.

$$
\texttt{deq} \; : \; \forall m,n : \N. (m + n > 0) \implies L^n([2] \, \tau) \otimes L^m \, \tau \loli \M \langle 0 \rangle \left(\exists n',m' : \N. (n' + m' + 1 = n + m) \amp \left(L^{n'}([2] \, \tau) \otimes L^{m'} \, \tau\right)\right)
$$

\texttt{deq} takes a nonempty queue, and produces another queue which has one element removed. The implementation of \texttt{deq} relies on a function \texttt{move}, which reverses the rear list into the front. The terms for all functions involved can be found in \autoref{appendix:a}.


%\begin{figure}
%\label{fig:example-dlambdaamor-enc}
%\caption{\texttt{enc} function in \dlambdaamor}
%\end{figure}

\subsubsection{Cost-Parametric Map}
While many existing languages and type systems for (amortized) resource analysis also support higher-order functions, the allowable analyses with higher-order functions are limited. One such limitation is that function arguments to higher-order functions are usually assumed to be constant-cost: for instance, in the cost analysis of a map, each application of the mapping function is assumed to incur the same amount of cost.

To improve on this, we employ a cost family $C : \N \to \R^+$ to encode the costs of each application of the function: the $i$-th call to the function is thought to incur $C(i)$ cost. Then in total, the map function incurs $\sum_{0 \leq i < n} C(i)$ cost. This analysis is reified in the type of map:
$$
\begin{array}{l}
\texttt{map} \; : \; \forall \alpha,\beta : \star. \forall C : \N \to \R^+. \forall n : \N. \\
\hspace{3em} !\left(\forall i : \N. [C \, i] \, 1 \loli \texttt{Nat}(i) \loli \alpha \loli \M \, \langle 0 \rangle\,  \beta\right) \loli\\
\hspace{3em}  !\texttt{Nat}(n) \loli \\
\hspace{3em}  L^n \,\alpha \loli\\ 
\hspace{3em}  \M \, \langle \texttt{const}\left(\sum_{0 \leq i < n} C(i)\right) \rangle\, \left(L^n \, \beta\right)
\end{array}
$$

Most importantly, the mapping function has type $!\left(\forall i : \N. [C \, i] \, 1 \loli \texttt{Nat}(i) \loli \alpha \loli \M \, \langle 0 \rangle\,  \beta\right)$. Since it must be applied to each element of the list, its type is $!$-ed to ensure it may be duplicated. The function is parameterized by the index $i$ on which it operates. To ensure that the mapping function at $i$ is actually only ever used at index $i$, the mapping function takes an additional argument of type $\texttt{Nat}(i)$, which is the singleton type of natural numbers equal to $i$\footnote{
This is simply an alias for $L^i \, 1$.
}. Finally, the mapping function requires $C \, i$ potential to run, and incurs no amortized cost, which ensures that its actual cost is bounded by $C \, i$.

Given the mapping function, the function \texttt{map} then transforms an $L^n \, \alpha$ into a monadic computation of an $L^n \, \beta$, incurring $\sum_{0 \leq i < n} C(i)$ amortized cost. As usual, the term implementing map can be found in \autoref{appendix:a}


%\subsection{Church Numerals}
%A similar trick can be used to write other cost-parametric higher-order functions. One particularly interesting instance of this is the iteration function: from a  function $\texttt{f} : \tau \loli \tau$, this functional computes $\texttt{f}^n : \tau \loli \tau$: this function is perhaps better known as the church numeral $n$. If \texttt{f} has cost $c$, then $\texttt{f}^n$ clearly has cost $cn$. In \dlambdaamor, we can give this function a \textit{far} more precise type which encodes a very strong analysis of the cost behavior of church numerals.

%First, we generalize the monomorphic iteration to iteration over a sequence of types: the  church numeral $n$ accepts a sequence of maps $\alpha \, i \loli \alpha \, (i + 1)$ for any $\N$-indexed family of types $\alpha$, and produces a function $\alpha \, 0 \loli \alpha \, n$. Next, we allow for the transition maps to be \textit{costly}. Similarly to the map, we index by a cost family $C : \N \to \R^+$ to allow for the possibility that the functions each have different amortized costs.  The intuitive cost analysis is again similar to that of map. To add some nontrivial cost into the mix, we will require that all function applications incur one cost. The church numeral $n$ applies the sequence of maps in order, each incurring $C \, i$ cost for $0 \leq i < n$, so in total, the resulting function $\alpha \, 0 \loli \alpha \, n$ has cost $n + \sum_{i < n} C \, i$, which accounts for the costs to run the functions, plus the $1$ cost to apply each of them. In the full type of church numerals, these costs are represented as potentials in negative position. Finally, we define the type of church numerals $\texttt{Nat}(n)$ \red{(Notational overload here... do we stick with the original?)} as an indexed type of kind $\N \to \star$.

%$$
%\texttt{Nat} \, : \, \N \to \star \, = ??
%$$

%With this type in hand, we can begin to write down church numerals! The church numeral zero is trivial: it is essentially the identity. Moreover, it should be intuitively clear (by parametricity) that the term shown below is the \textit{only} inhabitant of its type.

%$$
%\texttt{zero} \, : \, \texttt{Nat} \, 0 = ??
%$$

%More interesting however, are the church numeral operations! Most basic among them is the successor function, of type $\forall n : \N. [2] \, 1 \loli \texttt{Nat} \, n \loli \M \, \langle 0 \rangle \, \left(\texttt{Nat} \, (n+1)\right)$. The basic idea is simple: given a church numeral $N$, we produce a new one by iterating from $0$
%to $n$ by $N$, and then applying one last transition function. The full term, however, is fairly involved, and it can be found (along with church numeral addition) in \autoref{appendix:a}


\section{Overview of \bilambdaamor}
\label{sec:bilambdaamor-overview}
For \dlambdaamor to be useful as a programming language, it must be implementable! While a declarative type system on paper is useful for modeling and proving purposes, it has limited utility from a language engineering standpoint. \dlambdaamor is far more implementation-ready than its predecessor \lambdaamor, but the rules of its type system do not provide us with an obvious implementation method. Traditionally, one hopes to implement a type system in a manner similar to implementing a definitional interpreter \cite{reynolds:acm72}. For each judgment of the type system, the programmer writes a function which essentially runs a non-backtracking proof search for that judgment, with the type position as an input or output depending on if the function is type checking or inference.

Unfortunately, many of the judgments of \dlambdaamor -- which we will see when the formalism is presented in \autoref{sec:dlambdaamor-syntax-and-types}-- do not have straightforward implementations. The difficulties of coming up with implementations stems from five critical challenges which must be overcome before we can implement \dlambdaamor. Our eventual solutions to these five challenges form the basis of \bilambdaamor, the \textit{algorithmic} version of \dlambdaamor which we will subsequently implement.

\begin{enumerate}
  \item The main type-assignment judgment of \dlambdaamor yields an ambiguous proof search method. It is not at all clear which rule to apply at any given step of building a derivation, since there are some rules (subtyping, weakening) that can always be tried at each stage. Indeed, one could always implement proof search for \dlambdaamor using backtracking, but it is preferable to avoid this if possible. Instead, we would like our implementation-ready calculus \bilambdaamor to be \textit{syntax-directed} in the sense that the outermost syntax of the current term informs us which typing rule must be applied next to build a successful derivation. 
  
  \item \dlambdaamor includes full System F impredicative polymorphism, but a well-known result of \citet{wells:pal91} states that type inference for System F is undecidable. Hence, we will not be able to design a type inference algorithm for \dlambdaamor. A natural second option is to shoot for implementing a type checker. Unfortunately, this too has its limitations. To implement proper type checking, the syntax of \dlambdaamor would have to be changed such that every variable binder includes a type annotation. This is a heavy burden on the programmer: annotating binders with types is tedious, error prone, and generally uninteresting\footnote{
As \citet{pierce:lics03} notes: ``The more interesting your types get, the less fun it is to write them down!"
%https://www.cis.upenn.edu/~bcpierce/papers/tng-lics2003-slides.pdf
  }. Instead, \bilambdaamor adopts \textit{bidirectional type checking}, a technique pioneered by \citet{pierce-and-turner:lti} which trades off some of the generality of full type inference for added ergonomics over standard type checking.
  
  \item \dlambdaamor's subtyping relation provides a challenge which should be familiar to the reader who is versed in the implementation of dependent type theories. The inclusion of indexed types means that the deciding the subtyping relation requires (essentially) deciding $\beta$ equality at the type level: for instance, establishing the subtyping relation $(\lambda i : \N. L^i\, \tau) \, 3 \subty L^3 \, \tau$ requires a step of $\beta$-reduction. Luckily, the equational theory of types is simpler than that of a simply-typed lambda calculus, since the type-level lambda in \dlambdaamor $\lambda i : S. \tau$ ranges over index terms, not types. This allows for a very simple single-pass normalization procedure which allows us to subsequently decide the subtyping relation.
  
  \item Many of the crucial rules of the \dlambdaamor subtyping relation include constraint validity premises: for instance, the rule for deriving subtyping of potential types $[I|\vec{p}] \, \tau \subty [I|\vec{q}] \, \sigma$ has $\vec{q} \leq \vec{p}$ as a premise. These premises will need to be discharged by an SMT solver. However, repeatedly pausing the subtyping algorithm to send constraints to a solver each time the premise of a rule requires one be verified is inefficient. Instead, we would prefer to do one pass of typechecking, followed by a single call to the solver. To achieve this, the judgments of \bilambdaamor ``output" constraints. The intended meaning of this is that when the constraints are valid, the declarative version of the same judgment is derivable.
  
  \item The final barrier to implementation comes not from the refinement type or cost analysis features of \dlambdaamor, but simply from the fact that it is an affine type system. Multi-premise rules require the context to the type-checker be split into disjoint parts which can be used by each premise. This choice is nondeterministic: there is no way to know a priori what allocation of resources to which premise until later. To solve this, we employ a classical technique for implementing substructural type systems, which we refer to as the IO method \cite{cervesato:tcs00}.

\end{enumerate}

In the rest of the section, we present the solutions to these five problems that we choose to adopt. All five solutions are well-known techniques, but to our knowledge \bilambdaamor is the language to show that they may all be simultaneously integrated into a single system. Since the five techniques are orthogonal, we present each feature of \bilambdaamor in isolation for a significantly simpler language. In \autoref{sec:bilambdaamor-syntax-and-types}, we will present the formalism for \bilambdaamor, which applies the below-presented techniques to the declarative calculus \dlambdaamor.

\subsubsection{Bidirectional Type Systems}
\label{sec:bilambdaamor-overview-bidir}
Bidirectional type inference, also known as ``local type inference" is a type system algorithmization technique pioneered by \citet{pierce-and-turner:lti}. The technique works by separating the typing judgment $\Gamma \vdash e : \tau$ of a declarative type system into two algorithmic judgments: $\Gamma \vdash e \checks \tau$ and $\Gamma \vdash e \infers  \tau$, which are read ``$e$ checks against $\tau$" and ``$e$ infers $\tau$" (sometimes ``synthesizes"), respectively. These two judgments are mutually-recursively defined in a specific manner. The process of turning a declarative type system into a bidirectional algorithmic one is straightforward to the point of mechanical: Dunfield and Pfenning \cite{dunfield:popl04} provide a simple-to-follow recipe for this conversion, which extends from the simple type system they consider all the way to \dlambdaamor. 

Syntax-directed algorithmic type systems presented in a bidirectional style are trivially implementable: the implementation strategy is built into the structure of the rules. To implement a bidirectional type system, one writes two mutually-recursive functions \texttt{check:ctx->tm->typ->unit} and \texttt{infer:ctx->tm->typ} by recursion on the term input: the recursive calls are guided by the premises of each rule. Note that the types of these functions indicate the intended \textit{modes} of the three positions of the judgment, in the sense of logic programming. In the checking judgment, all positions are imagined to be \textit{inputs}, while the inference judgment indicates that the type position is an \textit{output} of the judgment.

As alluded to earlier, the ``inference" of the judgment $\Gamma \vdash e \infers \tau$ is not full inference, but merely ``local" inference: this judgment is derivable when enough information is present in the form of $e$ to determine its type. This is in contrast to full type inference, where the type of a term may not be fully known until its typing constraints are considered in the context of those from the larger term in which it sits. For this reason, every syntactic form in the language has either an inference or checking rule: if requiring one of the premises to be inference gathers enough information to determine the type of the conclusion, then that conclusion will be an inference judgment. Otherwise, the judgment will be checking.

%\subsubsection{Subsumption and Annotation}

To mediate between the two judgments, bidirectional type systems include two special rules. First, is the rule which is traditionally referred to as ``subsumption": to show that $e \checks \tau$, it suffices to show that $e \infers \tau$. In other words, if $e$ can infer a type, then it checks against that type. This rule is usually strengthened by subtyping:
$$
\infer{\Gamma \vdash e \checks \tau}{\Gamma \vdash e \infers \tau' & \tau' \subty \tau}
$$ For $e$ to check against $\tau$, it suffices for $e$ to synthesize a more precise type $\tau'$.

Going in the other direction from a checking premise to an infering conclusion is somewhat more involved. In general, the desired converse rule is not true: there will always be terms such that $e \checks \tau$ but it is not the case that $e \infers \tau$. To remedy this, bidirectional type systems introduce a new piece of syntax to the declarative language on which they're based: annotations. When $e$ checks against $\tau$, the annotated term $(e : \tau)$ infers the type $\tau$:
$$
\infer{\Gamma \vdash (e : \tau) \infers \tau}{\Gamma \vdash e \checks \tau}
$$

These annotations must be manually added to terms by the programmer as they write the program. However, the only place where annotations are truly required are at the sites of \textit{bare $\beta$-redexes}. For example, to check the term $(\lambda x. e)\, e'$, it must be annotated as $(\lambda x.e : \tau \to \sigma) \, e'$. Since most programs only contain bare $\beta$-redexes in the form of let-bindings, this requirement is both predictable and fairly ergonomic. In fact, the only type that truly must be annotated is the so-called ``cut type", the $\tau$ in $(\lambda x.e : \tau \to \sigma) \, e'$. This is because when checking $(\lambda x. e)\, e'$ against $\sigma$, the type of $e$ \textit{must} be $\sigma$, and the only unknown type is type of $e'$ (and $x$). In the eventual implementation, we include an annotated let-binding construct \texttt{let x : t = e in e'} for exactly this reason.

It is important to remember that these annotations are \textit{not} present in a declarative syntax. It will eventually be useful (when discussing the relation between \bilambdaamor and \dlambdaamor) to have the ability to talk about the ``underlying" declarative term of an algorithmic term, which is achieved by simply removing all type annotations $(e : \tau)$ from a term. We usually denote this $|e|$, when $e$ is an algorithmic term, and sometimes refer to it as the \textit{erasure} of a term. The erasure can be trivially defined by recursion on raw terms, with the critical case being $|(e : \tau)| = |e|$.

% \subsubsection{Soundness and Completeness}
As of yet, the relationship between a declarative calculus and its bidirectional algorithmic counterpart has been left unstated. However, the point of the bidirectional calculus is to be able to algorithmically generate declarative derivations! To this end, one always requires that the bidirectional type system be \textit{sound} for the declarative one.
\begin{theorem}[Bidirectional Soundness]
If $\Gamma \vdash e \checks \tau$, then $\Gamma \vdash e : \tau$
\end{theorem}
In other words, successfully running $\texttt{check}(\Gamma,e,\tau)$ is sufficient to show that $e$ in fact has type $\tau$ in context $\Gamma$.

Conversely, completeness is also desirable, but not strictly necessary for bidirectional type systems.  The most obvious statement of completeness ($\Gamma \vdash e : \tau$ implies $\Gamma \vdash e \checks \tau$) does not, in fact, hold. If the term $e$ contains un-annotated explicit $\beta$-redexes, the algorithmic system will fail to infer function types in those positions. For this reason, the following slightly weaker theorem is used as the completeness result for bidirectional type systems.
\begin{theorem}[Bidirectional Completeness]
\label{thm:bidir-compl-example}
If $\Gamma \vdash e : \tau$, then there exists $e'$ such that $\Gamma \vdash e' \checks \tau$, and $|e'| = e$, where $|e'|$ is the annotation-erasure of $e'$.
\end{theorem}

When proven constructively, this completeness result encodes an algorithm which inserts annotations into the term $e$ so that the resulting term checks against $\tau$. When \autoref{thm:bidir-compl-example} is proven directly by induction, the algorithm it encodes introduces far more annotations than is often strictly necessary. We improve on this with our completeness proof of \bilambdaamor in \autoref{sec:metatheory} by proving an equivalent statement whose constructive proof inserts fewer annotations than the standard theorem.

\subsubsection{Algorithmic Subtyping and Normalization}
To implement the subsumption rule mentioned above, a decision procedure for the subtyping relation $\tau \subty \tau'$ is required. However, \dlambdaamor's subtyping is not immediately implementable for two important reasons.

Firstly, like \dlambdaamor's typing relation, it is not syntax directed: the relation includes two rules (reflexivity and transitivity) that can be used at any step of a derivation. To avoid a backtracking implementation, it will be necessary to design an algorithmic subtyping relation for \bilambdaamor which includes neither of these rules. Of course, the algorithmic subtyping will need to be sound and complete for the declarative one. This requirement means that the algorithmic subtyping relation will need to have reflexivity and transitivity as admissible rules: in effect, we will need to prove identity and cut elimination.

The second (and more pernicious) problem is the inclusion of indexed types. While many refinement type systems (including DML \cite{xi:jfp07}, on which \lambdaamor's refinement types are based) include indexed types \cite{zenger:tcs97}, they are usually implemented only as types of the form $\forall i : S. \tau$. While useful, these indexed types are limited: their abstraction and application is controlled by term-level introduction and elimination rules. Instead, \dlambdaamor includes indexed types of the form $\lambda i : S. \tau$, which operate entirely at the type level, without programmer input required. The inclusion of type-level abstractions and applications does require the subtyping relation to include $\beta$ equalities for these indexed type families: without them, the subtyping relation would not be able to judge relations like $(\lambda i : \N. L^i\, \tau) \, 3 \subty L^3 \, \tau$, where the subtying relation holds up to $\beta$ equality.

The inclusion of $\beta$-conversion makes a simple algorithmic subtyping relation unlikely, since any way of deciding declarative subtyping must also decide $\beta$-equality of this small $\lambda$-calculus at the type level. However, the situation is sufficiently simple that we can get away with a fairly low-powered solution. To this end, \bilambdaamor's subtyping relation is split into two phases. First, both types are evaluated (or \textit{normalized}) to normal forms, and then judged for subtyping by a relation which only contains the congruence rules. These normal forms are only normal insofar as they contain no type-level $\beta$-redexes. Since index terms only become important at constraint-solving time, we do not require that the index terms appearing in a normal form type be normal in any sense. This notion of normal form has a major benefit: since abstractions $\lambda i : S.\tau$ range over index terms and not types, a $\beta$ reduct has strictly fewer type connectives than its redex. For this reason, the normalization can be implemented in a single pass: substituting an index term for a free variable in a type in normal form yields another type in normal form. The two-phase algorithmic subtyping relation, as well as the normalization proof, are discussed in detail in \autoref{sec:dlambdaamor-normalization}

\subsubsection{Constraint Generation}
\label{sec:bilambdaamor-overview-constr}
As motivated in \autoref{sec:dlambdaamor-overview}, most of the changes to \lambdaamor that result in \dlambdaamor are there for the purpose of simplifying the constraint-solving process that arises as a part of subtyping. Efficiently handling these constraints is crucial to an efficient implementation. For this reason, it is useful to defer the discharging of inference rules' constraint validity premises until \textit{after} the typechecking pass has finished.

We operationalize this in \bilambdaamor by designing each judgment to ``output" a constraint: we replace declarative judgments $\mathcal{J}$ with algorithmic ones $\mathcal{J} \gens \Phi$, where $\Phi$ is a constraint, thought of as an output of the judgment. For instance, a constraint-emitting algorithmic version of a declarative type-assignment judgment $\Gamma \vdash e : \tau$ might look like $\Gamma \vdash e : \tau \gens \Phi$. The intended meaning of this (and the shape of the soundness theorem for an algorithmic judgment with a constraint output) is that if we can derive the algorithmic judgment $\mathcal{J} \gens \Phi$ and $\Phi$ holds, then the declarative judgment $\mathcal{J}$ is derivable.

This scheme is pervasive. Since every judgment in \dlambdaamor either has a rule with a constraint validity premise or depends on one that does, every judgment in \bilambdaamor must emit constraints. The pattern in transforming a declarative judgment to an algorithmic one which emits constraints is fairly uniform: the output constraint of a rule is essentially the conjunction of the constraints output by its premises. One must also ensure that implications and quantifiers are inserted for constraints and index variables bound in premises: the logical structure of the output constraint mirrors the structure of the premises.

The constraints $\Phi$ that each judgment emits are drawn from a syntax of logical formulae, which is outlined in \autoref{fig:dlambdaamor-syntax} in \autoref{sec:dlambdaamor-syntax-and-types}. This syntax is very general, allowing unrestricted universal and existential quantification, the usual truth-functional logical connectives, as well as equalities and inequalities between index terms. While this theory is (almost certainly) not decidable, SMT solvers handle it well enough for practical purposes, as we will see in \autoref{sec:lambdaamor-impl}.

%\red{Talk a bit here about where these arise in other places}

\subsubsection{I/O Method}
\label{sec:bilambdaamor-overview-io}
On top of the implementation challenges created by the fancier aspects of \dlambdaamor's type system, its affine-ness presents a well-understood barrier to implementation. To illustrate, consider writing the following case of the \texttt{check:ctx->tm->typ->unit} function from earlier.\footnote{
To simplify some of the presentation of this sub-section, we will specialize to the non-bidirectional setting, and work in a simply-typed language where binders are fully annotated.
}

$$
\texttt{check gamma Pair(e1,e2) Tensor(t1,t2) = } ??
$$

This case corresponds to the introduction rule for tensor,

%As outlined in Section~\textbf{??}, the implementation of bidirectional type systems usually takes the form of two mutually recursive functions \texttt{check:ctx->tm->typ->bool} and \texttt{infer:ctx->tm->typ}. These functions are implemented recursively on the second argument, and the recursive calls for a specific case are dictated by the premises of the corresponding typing rule. However, in the presence of an affine type system, this clean story is somewhat complicated. To illustrate, consider this simplified version of a first cut at the algorithmic tensor introduction rule.
$$
\infer{\Gamma_1,\Gamma_2 \vdash (e_1,e_2) : \tau_1 \otimes \tau_2}{\Gamma_1 \vdash e_1 : \tau_1 & \Gamma_2 \vdash e_2 : \tau_2}
$$


It is not at all clear how to proceed in this case. The tensor introduction rule prescribes that we make two recursive calls \texttt{check gamma1 e1 t1} and \texttt{check gamma2 e2 t2}, but provides no direction how to obtain \texttt{gamma1} and \texttt{gamma2} from \texttt{gamma}: the rule is presented in the standard way so that the two halves of the context are given at the outset.

This problem has two naive solutions. Firstly, one could analyze the structure of \texttt{e1} and \texttt{e2} to determine the variables they each use, and partition the context accordingly. This approach is very inefficient: even if done with a pre-processing step, it adds at least one pass through the term. Secondly, one could split the context \textit{symbolically}, and generate yet more constraints to unify at the end of the typechecking process.

Instead of either of these, we adopt a more principled approach based on the work of \citet{cervesato:tcs00}. In short, we extend the main typing judgment with yet another output-- this time a second context, which contains the variables which were unused in typing the term. A simplified version of the typing judgment takes the form $\Gamma \vdash e : \tau \gens \Gamma'$, where $\Gamma$ is the \textit{input context}, and $\Gamma'$ is the \textit{output context}. The key idea of this setup (known sometimes as the I/O method) is that we may thread the contexts through the premises of a rule as follows:

$$
\infer{\Gamma \vdash (e_1,e_2) : \tau_1 \otimes \tau_2 \gens \Gamma_2}{\Gamma \vdash e_1 : \tau_1 \gens \Gamma_1 & \Gamma_1 \vdash e_2 : \tau_2 \gens \Gamma_2}
$$

The first premise (the first component of the pair) has access to the entire input context, and it outputs $\Gamma_1$, the variables in $\Gamma$ which were unused in typing $e_1$. This context is then used as the \textit{input} context for checking $e_2$. Since affine variables may be used at most once, the only variables which $e_2$ may access are those unused by $e_1$. This property is enforced in declarative systems ``in parallel" by splitting the context up front, but it may similarly be enforced ``sequentially" by lazily deciding which premises may use which variables in this algorithmic style.

The key rule in designing an algorithmic type system which uses the I/O method is the affine variable rule. When a variable is used, it must be removed from the output context:

$$
\infer{\Gamma \vdash x : \tau \gens \Gamma \setminus \{x\}}{x : \tau \in \Gamma}
$$

Moreover, this I/O method will be trivial to implement. We simply change the type of \texttt{check} and \texttt{infer} to output a context as well. Since these functions both receive and output a context, one can think of typechecking with the I/O method as happening inside a state monad of contexts, as opposed to the usual reader monad. While this solution is clearly preferable to the naive ones efficiency-wise, it is not at all obvious that this way of algorithmizing an affine type system is sound, much less complete, for the standard presentation of the rules. 

Writing $\Gamma \vdash e : \tau$ (with no output context) as the declarative typing relation, a first cut at a soundness theorem for this calculus is the following:

\begin{theorem}[First Cut at Soundness of the I/O Method]
If $\Gamma \vdash e : \tau \gens \Gamma'$, then $\Gamma \vdash e : \tau$
\end{theorem}

While true, the statement of this theorem isn't strong enough to be proven by a direct induction. Intuitively, $\Gamma$ contains variables that are unused in the typing of $e$, namely those in $\Gamma'$. Thus, it makes sense to strengthen the conclusion to type $e$ in a context with only the free variables it mentions, namely $\Gamma \setminus \Gamma'$.

\begin{theorem}[Soundness of the I/O Method]
If $\Gamma \vdash e : \tau \gens \Gamma'$, then $\Gamma \setminus \Gamma' \vdash e : \tau$
\end{theorem}

Note that $\Gamma \setminus \Gamma'$ is well-defined because $\Gamma' \subseteq \Gamma$, a fact which must be proven by induction over the algorithmic rules. The completeness theorem is simpler to state, but harder to prove.

\begin{theorem}[Completeness of the I/O Method]
If $\Gamma \vdash e : \tau$, then there is some $\Gamma'$ such that $\Gamma \vdash e : \tau \gens \Gamma'$
\end{theorem}

The proof of this theorem relies on the fact that weakening is also admissible for an algorithmic type system using the I/O method. When new variables are added to the input context, they simply ``flow through" the judgment to the output context, and are left unused.

\begin{theorem}[Admissibility of Weakening for the I/O Method]
If $\Gamma \vdash e : \tau \gens \Gamma'$, then for all $\Gamma''$, we have that $\Gamma,\Gamma'' \vdash e : \tau \gens \Gamma',\Gamma''$
\end{theorem}

\section{Syntax and Type System of \dlambdaamor}
\label{sec:dlambdaamor-syntax-and-types}

In this section, we will finally rip off the band-aid and begin to discuss the formal system that makes up \dlambdaamor. All of the main motivating concepts for the design of this formalism have been discussed in the previous sections. When the cost monad and potential modalities with their AARA-style representations, refinements, and polymorphism are combined, we have a recipe for some potentially convoluted inference rules. The puzzled reader is encouraged to refer to \citet{xi:jfp07} for an account of similar material, without any of the added burden of resource-tracking features.

Since \dlambdaamor is only a minor revision of \lambdaamor, the structure of its formalism closely follows that of the original found in \citet{rajani-et-al:popl21}. 
We will begin by presenting its syntax, which differs from \lambdaamor only in its added term-level index and type variables, which are added for syntax-directedness purposes. Next, we discuss \dlambdaamor's type system, which mirrors that of \lambdaamor judgment-for-judgment. The only major difference between the systems is that we take some extra care to pin down the invariants and presuppositions of each judgment: this extra level of precision and formality is required to prove some of the proof-theoretic properties of \dlambdaamor with which we conclude the section.

\subsection{Syntax of \dlambdaamor}
\begin{figure}
\input{figs/dlambdaamor-syntax}
\caption{Syntax of \dlambdaamor}
\label{fig:dlambdaamor-syntax}
\end{figure}

In preparation to discuss \dlambdaamor's type system, we present its syntax in \autoref{fig:dlambdaamor-syntax}.

\subsubsection{Index Terms, Sorts, Kinds, and Constraints}
\dlambdaamor's refinement types are modeled in the style of DML, which takes the form of a two-level type system. As discussed in \autoref{sec:dlambdaamor-overview}, these refinements allow the user to assign potential to types which depends on the sizes of data structures, such as the lengths of lists. These numerical values are denoted by \textit{index terms} ($I,J$) which decorate some of the types and surface syntax of \dlambdaamor. Index terms may be of three possible numerical \textit{base sorts}: natural numbers $\N$, positive real numbers $\R^+$, and potential vectors of some fixed length $k$, $\vec{\R^+}$. Additionally, \dlambdaamor also includes first-order sort-level functions.

The syntax of index terms themselves is generated by the standard arithmetic operations, along with constants, variables, and application/abstraction forms for the sort-level functions. Of special note are the \texttt{const} and $\Sigma$ constructs. For an index term $I$ of sort $\R^+$, the term $\texttt{const}(I)$ is of potential vector sort, and may be thought of as the ``constant" potential vector $(I,0,\dots,0)$, such that for all $n \N$, $\phi(n,\texttt{const}(I)) = I$.
The $\Sigma$ construct is as expected, although the upper bound is non-inclusive: the sum $\sum_{i=I_0}^{I_1} J$ sums from $J[I_0/i]$ to $J[(I_1-1)/i]$, as long as the range is nonempty, when the sum is of course zero\footnote{
Including arbitrary sums in index terms has the consequence that the constraints from subtyping may be nonlinear. In practice this is rarely an issue--- sums are only needed in specific analyses such as cost-parametric map/fold and church numerals.
}.

\dlambdaamor also supports full System F-style impredicative polymorphism, as well as sort-indexed types. We denote the kind of types as $\star$. Note that sort-indexed types may have sort-level arrows in negative position, and so sort-function-indexed types are included also.

Finally, \dlambdaamor includes constraints over index terms, which can be combined with conjunction, disjunction, implication, and both kinds of quantification. We will not provide a proof system for these constraints. Instead, we will only ever interact with constraints via an abstract validity relation $Theta ; \Delta \vDash \Phi$, and all the proofs of soundness and completeness in \autoref{sec:metatheory} will be relative to a decision procedure/oracle for this relation $\vDash$. In the implementation of \lambdaamorimpl, we think of this relation parameter of the type system as being instantiated by the SMT solver backend, which does a good enough job of deciding the theory for practical purposes.

\subsubsection{Types}
\dlambdaamor's types include all of the standard connectives from affine logic, namely positive and negative products ($\otimes$ and $\amp$), sums ($\oplus$), affine functions ($\loli$), and the exponential modality $! \tau$. Of course, \dlambdaamor also supports a litany of more specialized types for amortized cost analysis.

Chief among these are the cost monad and potential types, A monadic type $M \, (I,\vec{p}) \, \tau$ classifies monadic computations of type $\tau$, which may incur up to $\phi(I,\vec{p})$ cost. The type formation rules ensure that $I$ is of sort $\N$, and $\vec{p}$ is of sort $\vec{\R^+}$. With the same restrictions on the sorts of its index terms, the potential type $[I|\vec{p}]\, \tau$ classifies values with at least $\phi(I,\vec{p})$ potential. In addition to the AARA-style potential, \dlambdaamor also has a ``constant potential" modality $[I] \, \tau$, whose values are those of type $\tau$, with $I = \phi(n,\texttt{const}(I))$ potential, for any $n$. While not strictly necessary for the theoretical development of \dlambdaamor, this modality is sometimes useful in practice.

Index variables may be quantified over in types with the $\forall i : S.\tau$ and $\exists i : S.\tau$ types, and polymorphic type variables are quantified over using the $\forall \alpha : K .\tau$ type constructor-- we do not support existential types, though there is no metatheoretical barrier to their inclusion.

As previously mentioned, the type of lists $L^I \, \tau$ is refined by length-- the values of this type all have length $I$. Next, \dlambdaamor also includes two ``constraint types", $\Phi \implies \tau$, and $\Phi \amp \tau$. Values of the first type are known to have type $\tau$ when $\Phi$ holds, and values of the second type are values of type $\tau$, along with an (irrelevant) proof of $\Phi$. As \lambdaamor has no error handling mechanism, this construct is helpful for statically preventing errors by encoding function pre and post-conditions in a type: for instance, the \texttt{head} function may be typed as $\forall n : \N. (n \geq 1) \implies (L^n \, \tau \loli \tau)$

Finally, \dlambdaamor's types include abstraction and application forms for indexed types. The abstraction form $\lambda i :S.\tau$ has kind $S \to K$ when $\tau$ has kind $K$, and so term variables will never have type $\lambda i : S.\tau$, as it is a higher-kinded type.

\subsubsection{Terms}
\label{}
While the original presentation of \lambdaamor takes great care to include only a barebones term syntax, \dlambdaamor will have to expand this syntax somewhat to ensure that the textual representation of a program is unambiguous for programming purposes. Practically, this means that every logical connective has explicit syntactic introduction and elimination forms, whereas this is handled silently in \lambdaamor.

The term syntax for all of the standard connectives should be familiar. The two products are distinguished by double angled brackets for positive pairs, and parentheses for negative pairs. All binders are un-annotated to reduce the burden on the programmer. Lists are constructed with nil and cons constructors, and the elimination form is a pattern match. The last standard inclusion is a fixpoint operator \texttt{fix}, which allows us to write generally-recursive functions. 

The refinement type syntax is similar to that of the logical connectives: universal quantifiers are introduced and eliminated with $\Lambda i. e$ abstraction and application $e \, [I]$, while existentials have the traditional \texttt{pack}/\texttt{unpack}. The two constraint types are less standard: constraint conjunction is introduced with a pair of angle brackets $<e>$ and eliminated with a \texttt{clet}, while constraint implication is introduced and eliminated with a ``silent" abstraction $\Lambda. e$, and eliminated with an application $e \, \{\}$.

The syntax associated to the amortized analysis constructs is likely less familiar. The monadic cost type $M \, (I,\vec{p}) \, \tau$ has three operations associated with it: $\texttt{ret}(e)$ and $\texttt{bind} \, x = e_1 \, \texttt{in}\, e_2$, the unit and bind of the monad, respectively, as well as $\texttt{tick}[I|\vec{p}]$, an atomic operation which incurs a cost of $\phi(I,\vec{p})$. The potential type has introduction form $\texttt{store}[I|\vec{p}](e)$ and elimination form $\texttt{release} \, x = e_1, \texttt{in} \, e_2$. Similarly, the \textit{constant} potential type has introduction form $\texttt{store}[I](e)$, and the same elimination syntax as the AARA-style potential type.

\subsection{Type System of \dlambdaamor}
\begin{figure}
\input{figs/dlambdaamor-typing-judgments}
\caption{Judgment Forms of the \dlambdaamor Type System}
\label{fig:dlambdaamor-typing-judgments}
\end{figure}

In \autoref{fig:dlambdaamor-typing-judgments}, we provide a listing of the judgments which make up \dlambdaamor's type system. Selected rules are presented in \autoref{fig:dlambdaamor-selected-typing-rules}, and a listing of all rules can be found in \autoref{appendix:a}.

\subsubsection{Contexts}
Judgments in \dlambdaamor have as many as five contexts.
 Contexts $\Psi$ map type variables to their kinds. $\Theta$ is an index variable context, which maps index variables to their sorts. $\Delta$ is a list of constraints, which are assumptions of the judgment-- constraints in $\Delta$ may mention variables in $\Theta$, and so there is a weak form of dependence between the two contexts. The final two contexts $\Omega$ and $\Gamma$ are term variable contexts, which map variables to their types. The context $\Omega$ is referred to as the exponential context, and it contains variables which may be used more than once: i.e. are not subject to the affine restriction
\footnote{
One may think of all types in $\Omega$ implicitly beginning with $!$, and imagine the variable rule for exponential variables to be silently inserting the counit $!\tau \loli \tau$. This dual-context construction is standard in the study of modal types. \cite{kavvos:lmcs}
}. Finally, the context $\Gamma$ lists the rest of the variables, which may be used at most once.

To avoid questions of exchange, we consider all of the contexts except for $\Delta$ up to permutations. Indeed, we will frequently treat contexts like sets, testing membership. Further, it will be useful later on to take intersections, unions, and differences of contexts: these operations will only be defined when both operations involved are subsets of a common superset.

\subsubsection{Index Terms and their Sorts}
\begin{figure}
\input{figs/dlambdaamor-selected-sort-kind-constr-rules}
\caption{Selected Sort, Kind, and Constraint Rules}
\label{fig:dlambdaamor-selected-sort-kind-constr-rules}
\end{figure}

The rules that make up the sort system for index terms (prefixed I-) are mostly self-explanatory: we ensure that arguments to arithmetic operators have the same sorts.
Since all three base sorts ($\N$, $\R^+$, $\vec{\R^+}$) are nonnegative, the rule for subtraction $I - J$ must ensure that $I \geq J$. As discussed in \autoref{sec:dlambdaamor-overview}, the rule I-ConstVec shows how \texttt{const} promotes index terms of sort $\R^+$ to sort $\vec{\R^+}$, while the rule I-Shift types the shift operation on potential vectors (\autoref{defn:shift}). Finally, the I-Lam and I-App rules give the introduction and elimination rules for the index-level functions.

\subsubsection{Types and their Kinds}
The type formation rules (prefixed K-) for \dlambdaamor are very straightforward. All types have kind $\star$, with the exception of the index type abstraction and elimination forms. The rule K-FamLam ensures that an indexed type $\lambda i : S. \tau$ has kind $S \to K$ when $\tau$ has kind $K$, and the indexed-type application $\tau \, I$ has kind $K$ when $\tau$ has kind $S \to K$ and $I$ is of sort $S$, as seen in K-FamApp.

\subsubsection{Subtyping}
\begin{figure}
\input{figs/dlambdaamor-selected-subty-rules}
\caption{Selected Subtyping rules}
\label{fig:dlambdaamor-selected-subty-rules}
\end{figure}
The majority of the rules for subtyping in \dlambdaamor (prefixed by S-) are the standard congruences for logical connectives. The rules for the types involved in cost analysis for refinements, however, warrant some discussion.

The rule S-Monad gives the subtyping relation for the cost monad: $\M \, (I,\vec{q}) \, \tau_1 \subty \M (J,\vec{p}) \,\tau_2$ when $\tau_1 \subty \tau_2$, $I = J$, and $\vec{q} \leq \vec{p}$ componentwise. The soundness of this rule relies on the fact that $\phi(n,\vec{q}) \leq \phi(n,\vec{p})$ when $\vec{q} \leq \vec{p}$, and the fact that the cost annotations represent \textit{upper bounds}-- it is safe to use a computation which incurs less cost in a context which expects one that incurs more. Dually, it is always safe to throw away potential in the subtyping rules for the two potential modalities, S-Pot and S-ConstPot.

In addition to the subtyping rules at base kind, the rules S-FamLam and S-FamApp govern the subtyping of indexed types. The rule S-FamLam states that subtyping at kind $S \to K$ is simply generated by pointwise subtyping in the codomain $K$, while S-FamApp is a standard congruence rule. Note that S-FamApp requires that the two arguments be equal: since we do not require that indexed types be monotone, this is the strongest possible form of the rule. Finally, the rules S-FamBeta-$1$ and S-FamBeta-$2$ serve to include $\beta$-equality of type families in the subtyping relation.

\subsubsection{Context Well-Formedness Judgments and Context Subsumption}
\begin{figure}
\input{figs/dlambdaamor-ctx-wf-rules}
\caption{Context Well-formedness rules and Context Subsumption}
\label{fig:dlambdaamor-ctx-wf-rules}
\end{figure}
The judgment $\Theta ; \Delta \vdash \Phi \; \texttt{wf}$ ensures that $\Phi$ is a well-formed context: all index terms mentioned in it are sort-correct, and relations are only judged between index terms of the same sort.

\dlambdaamor also requires two auxiliary context-well-formedness judgments: $\Theta \vdash \Delta \; \texttt{wf}$ and $\Psi ; \Theta ; \Delta \vdash \Gamma \; \texttt{wf}$. The former ensures that the constraints in the context $\Delta$ are well-typed with respect to the context $\Theta$, and the latter ensures that all of the types in $\Gamma$ have kind $\star$.

Finally, The judgment $\Psi ; \Theta ; \Delta \vdash \Gamma' \wknto \Gamma$ determines when we may relax a context $\Gamma'$ to a weaker one $\Gamma$ with the T-Weaken rule. Intuitively, this judgment encodes the permission to weaken a context as a kind of record subtyping.


\subsubsection{Terms and their Types}
\begin{figure}
\input{figs/dlambdaamor-selected-typing-rules}
\caption{Selected \dlambdaamor rules}
\label{fig:dlambdaamor-selected-typing-rules}
\end{figure}

The typing rules for all of the logical connectives have the standard caveats for an affine type system: affine arrow introduction T-ArrI binds variable $x : A$ in the affine context $\Gamma$. Multi-premise rules like tensor introduction (T-TensorI) and sum elimination (T-Case) require splitting the affine context to type the premises. As usual, ``parallel" premises such as the two arms of a case may share affine resources, as only one branch will be taken at run-time.

Of greater interest are the rules for the cost analysis and refinement type-related constructs. The return of the cost monad lifts a pure value $e : \tau$ to a monadic computation $\texttt{ret}(e)$ which incurs no cost. So, the rule T-Ret types $\texttt{ret}(e)$ at $\M(I,\vec{0}) \, \tau$ for any index term $I$ of sort $\N$, where $\vec{0}$ is the length $k$ vector of $0$s. Since $\phi(I,\vec{0}) = 0$ independent of the base $I$, this rule has the desired effect. Meanwhile, the bind of the cost monad sums the costs of the computation and the continuation. T-Bind operationalizes this by typing $\texttt{bind}\, x = e \, \texttt{in} \, e' \; : \M(I,\vec{p} + \vec{q}) \, \tau_2$ when $e : \M(I,\vec{p}) \, \tau_1$ and $x : \tau_1 \vdash e' : \M(I,\vec{q}) \, \tau_2$. The soundness of this rule is justified by the linearity of the $\phi$ function from \autoref{thm:phi-linear}.

The two operations for the potential modality are carefully constructed to work harmoniously with the cost monad. Firstly, given a term $e : \tau$, the rule T-Store allows us to store $\phi(I,\vec{p})$ potential on the term by incurring that amount of cost: this takes the form of assigning the type $\M(I,\vec{p}) \, \left([I|\vec{p}] \, \tau\right)$ to the term $\texttt{store}[I|\vec{p}](e)$. Note that to access the underlying potential, one must first $\texttt{bind}$ the computation, in effect incurring the requisite $\phi(I,\vec{p})$ cost to have access to the potential. Dually, the rule T-Release gives the typing for using potential. The potential on a term $e : [I|\vec{p}] \, \tau_1$ can be used to pay for a monadic continuation $x : \tau_1 \vdash e' : \M(I,\vec{q} + \vec{p}) \, \tau_2$ to get
$\texttt{release}\, x = e \, \texttt{in} \, e' : \M(I,\vec{q}) \, \tau_2$. The rules for constant potentials follow a similar pattern: the constant store expression $\texttt{store}[J](e)$ has type $\M(I,\texttt{const}(J)) \, \left([J] \, \tau\right)$ by T-StoreConst.
We note that the type system enforces a discipline that all potential-related activities happen inside the cost monad, which greatly simplifies the type soundness proof found in \citet{rajani-et-al:popl21}, when compared to resource-aware type systems with pervasive cost.

The list type ($L^I \, \tau$) is length-indexed, and so its typing rules are somewhat more involved than the standard ones. To enforce the length refinement, the rules T-Nil and T-Cons specify that the empty list $\texttt{[]}$ has type $L^0 \tau$, while a cons list $e :: e'$ has type $L^{I+1} \tau$ for $e : \tau$ and $e' :: L^I \tau$. The list elimination rule T-Match is more or less standard, but the two branches are typed under extra constraints in the constraint context $\Delta$. If the scrutinee has type $L^I \tau$, then the nil case of the match is typed under the assumption that $I = 0$. Meanwhile the cons case is given the assumption $I \geq 1$, and the tail of the list is bound as having type $L^{I-1} \tau$.

In addition to the length-refined lists, the refinement type portion of \dlambdaamor's type system also includes index term quantifiers in types ($\forall,\exists$), as well as the two constraint types ($\Phi \amp \cdot$, $\Phi \implies \cdot$).  The treatment of the quantifiers is standard: the rules T-ILam and T-ExistE bind index variables in the index context $\Theta$, while the rules T-IApp and T-ExistI substitute in index terms provided by the syntax. The rules for the constraint types operate in a similarly dual fashion.

Finally, \dlambdaamor explicitly includes two non-logical rules. The first is a subtyping rule T-Sub, which may be used to downcast the type of a term to a less precise one. This rule has no corresponding syntactic form, and thus may be inserted anywhere in a derivation. The second is T-Weaken, which allows for the weakening of the two term variable contexts, $\Omega$ and $\Gamma$. As previously mentioned, the weakening relation on which this rule depends includes subtyping, and so a weaker context may include less precise types, and not just fewer available variables.


\subsubsection{Presuppositions}
All of the judgments presented so far are ``raw" judgments-- one may mechanically derive a proof of one using the inference rules, without regard for whether or not the judgment makes any sense. Traditionally, the requisite assumptions for stating a judgment in a sensical manner are known as \textit{presuppositions}. For example, the sort-checking judgment $\Theta ; \Delta \vdash I : S$ requires that the constraint context $\Delta$ be well-formed with respect to $\Theta$. There are many ways of handling these, but in this work we choose to make them explicit. Each raw judgment form has an associated judgment form which packages together the requisite well-formedness presuppositions for that judgment. We denote this by a subscript $p$ on the turnstile.

\begin{definition}
We say that $\Theta ; \Delta \pvdash I : S$ when $\Theta \vdash \Delta \; \texttt{wf}$ and $\Theta ; \Delta \vdash I : S$.
\end{definition}

\begin{definition}
We write $\Psi ; \Theta ; \Delta \pvdash \tau : K$ to mean that $\Theta \vdash \Delta \; \texttt{wf}$ and $\Psi ; \Theta ; \Delta \vdash \tau : K$
\end{definition}

\begin{definition}
We write $\Psi ; \Theta ; \Delta \pvdash \tau \subty \tau' : K$ to mean that
\begin{enumerate}
  \item $\Theta \vdash \Delta \; \texttt{wf}$
  \item $\Psi ; \Theta ; \Delta \vdash \tau : K$
  \item $\Psi ; \Theta ; \Delta \vdash \tau' : K$
  \item $\Psi ; \Theta ; \Delta \vdash \tau \subty \tau' : K$
\end{enumerate}
\end{definition}

\begin{definition}
We say $\Psi ; \Theta ; \Delta \pvdash \Gamma \wknto \Gamma'$ to mean that
\begin{enumerate}
  \item $\Theta \vdash \Delta \; \texttt{wf}$
  \item $\Psi ; \Theta ; \Delta \vdash \Gamma \; \texttt{wf}$
  \item $\Psi ; \Theta ; \Delta \vdash \Gamma' \; \texttt{wf}$
  \item $\Psi ; \Theta ; \Delta \vdash \Gamma \wknto \Gamma'$
\end{enumerate}
\end{definition}

\begin{definition}
We say that $\Psi ; \Theta ; \Delta ; \Omega ; \Gamma \pvdash e : \tau$ when
\begin{enumerate}
  \item $\Theta \vdash \Delta \; \texttt{wf}$
  \item $\Psi ; \Theta ; \Delta \vdash \Omega \; \texttt{wf}$
  \item $\Psi ; \Theta ; \Delta \vdash \Gamma \; \texttt{wf}$
  \item $\Psi ; \Theta ; \Delta \vdash \tau : \star$
  \item $\Psi ; \Theta ; \Delta ; \Omega \vdash e : \tau$
\end{enumerate}
\end{definition}

\subsection{Proof Theory of \dlambdaamor}
Any type system as complex as \dlambdaamor's has heaps of syntactic structure to exploit. This structure comes, as is traditional for type theories and type systems, in the form of admissible rules. Each of \dlambdaamor's five principal judgments have their own set of admissible rules, corresponding to the structural rules for each context, as well as other judgmental structure. Because of this, the proof theory of \dlambdaamor is incredibly rich. We will make no attempt to cover it all here, and instead pick and choose the theorems which will be useful down the line when it comes time to prove the soundness and completeness of \bilambdaamor. Below, we present the admissible rules which are used repeatedly in further metatheoretic developments: proofs and intermediate theorems can be found in \autoref{appendix:a}.

First, we prove two statements about the context well-formedness judgments that should be intuitively clear: since term variable contexts are not dependent, subsets of well-formed contexts are themselves well-formed.

\begin{restatable}[]{theorem}{ctxwfstreng}
If $\Psi ; \Theta ; \Delta \vdash \Gamma \; \texttt{wf}$ and $\Gamma' \subseteq \Gamma$ then $\Psi ; \Theta ; \Delta \vdash \Gamma' \; \texttt{wf}$
\end{restatable}

\begin{restatable}[]{theorem}{conwfstreng}
If $\Theta \vdash \Delta, \Phi \; \texttt{wf}$ then $\Theta \vdash \Delta \; \texttt{wf}$
\end{restatable}

Since the type and index variable contexts are fully structural, every judgment that uses them admits weakening and contraction. In practice, we will only ever need to explicitly use the weakening theorem for context subsumption.

\begin{restatable}[]{theorem}{ctxsubwkn}
\label{thm:ctx-sub-wkn}
If $\Psi ; \Theta ; \Delta \pvdash \Gamma \wknto \Gamma'$ and $\Psi' \supseteq \Psi$, $\Theta' \supseteq \Theta$, and $\Delta' \supseteq \Delta$, then
$\Psi' ; \Theta' ; \Delta' \pvdash \Gamma \wknto \Gamma'$
\end{restatable}

Substitution is admissible for all contexts in all judgments-- this is essentially a requirement of a type system! However, we will primarily be concerned with substitution for types, as it will become critical once we discuss normalization for types in \autoref{sec:dlambdaamor-normalization}.

\begin{restatable}{theorem}{typeidxsubst}
\label{thm:type-idx-subst}
If $\Psi ; \Theta, i : S ; \Delta \pvdash \tau : K$ and $\Theta ; \Delta \pvdash I : S$ then $\Psi ; \Theta ; \Delta \pvdash \tau[I/i] : K$
\end{restatable}

The strengthening theorem below only becomes relevant when passing back and forth between \dlambdaamor and our eventual algorithmic system. Intuitively, the concept is clear: if a subtyping relation holds between two types which share a common set of free variables, the subtyping can be derived while mentioning only those variables.

\begin{restatable}{theorem}{subtystreng}
\label{thm:subty-streng}
Suppose $\Psi ; \Theta ; \Delta \vdash \tau : K$ and $\Psi ; \Theta ; \Delta \vdash \tau' : K$.
If $\Psi' ; \Theta' ; \Delta \pvdash \tau \subty \tau' : K$ with $\Theta' \supseteq \Theta$ and $\Psi' \supseteq \Psi$, then $\Psi ; \Theta ; \Delta \pvdash \tau \subty \tau' : K$
\end{restatable}

The next theorem is a convenient equivalent characterization of the context subsumption judgment, which formalizes our intuition of this judgment as being essentially record subtyping: the inclusion of $\Gamma'$ into $\Gamma$ acts as record width subtyping, while pointwise subtyping on the intersection of the two contexts allows for record depth subtyping.

\begin{restatable}{theorem}{ctxsubsubset}
\label{thm:ctx-sub-subset2}
$\Psi ; \Theta ; \Delta \vdash \Gamma \wknto \Gamma'$ if and only if for all $x : \tau' \in \Gamma'$, there is some $\tau$ so that $x : \tau \in \Gamma$ and $\Psi ; \Theta ; \Delta \vdash \tau \subty \tau' : \star$.
\end{restatable}

Finally, we prove a compatability lemma about context subsumption that is similar in flavor to \autoref{thm:subty-streng}, which allows us to strengthen the assumptions of the judgment in a specific (but common) situation.

\begin{restatable}{theorem}{ctxsubswap}
\label{thm:ctx-sub-swap}
Suppose that
\begin{enumerate}
  \item $\Psi ; \Theta ; \Delta \pvdash \Gamma_1 \wknto \Gamma_1'$
  \item $\Psi' ; \Theta' ;  \Delta' \pvdash \Gamma_2 \wknto \Gamma_2'$
  \item $\Gamma_1 \supseteq \Gamma_2$ and $\Gamma_1' \supseteq \Gamma_2'$
\end{enumerate}
Then, $\Psi ; \Theta ; \Delta \pvdash \Gamma_2 \wknto \Gamma_2'$.
\end{restatable}


\section{Semantics and Soundness of \dlambdaamor}
\label{sec:dlambdaamor-sound}
For \dlambdaamor to be useful, its type system must be \textit{sound}. In this context, soundness means that the static amortized execution costs from the types given to programs are in fact actual upper bounds on the programs' real dynamic execution cost. To prove that \dlambdaamor's type system is sound in this way, we will appeal to a version of the soundness proof of \lambdaamor. As discussed in \autoref{sec:dlambdaamor-overview}, \lambdaamor differs from \dlambdaamor mainly in its treatment of potentials and costs. In fact the two languages are sufficiently similar (by design, of course) that there is a straightforward embedding of \dlambdaamor into \lambdaamor. This embedding is cost-preserving, and so the soundness of \dlambdaamor follows immediately from the soundness of \lambdaamor. Formally, we do not present a true embedding into \lambdaamor, as it does not have a sort of potential vectors. However, potential vectors can be trivially added to \lambdaamor: the kripke logical relation which forms the basis for its soundness proof never inspects index terms, and conflates index terms with the semantic objects they denote. For this reason, we freely consider \lambdaamor as having a sort of potential vectors in the style of \dlambdaamor, as well as a sort-level uninterpreted function $\phi : \N \times \vec{\R^+} \to \R$ which is additive and monotonic in the second argument.

In \autoref{sec:dlambdaamor-semantics}, we present the operational semantics for \dlambdaamor upon which the soundness theorem is based. This semantics is a big-step cost-indexed operational semantics: the cost indices are the concrete notion of cost that will be bounded by the statically-predicted costs in the soundness theorem.

Then, in \autoref{sec:dlambdaamor-embedding}, we sketch the embedding of \dlambdaamor into \lambdaamor, and further sketch proofs that the cost semantics of \dlambdaamor coincides with that of \lambdaamor under the embedding, as well as the overall soundness theorem of \dlambdaamor. We will not present the full details of the translation here. However, the strategy is clear, and we see no barriers to its formalization.


\subsection{Operational Semantics of \dlambdaamor}
\label{sec:dlambdaamor-semantics}
To pin down the exact cost of programs written in \dlambdaamor, we provide a \textit{cost semantics} for the language: a big-step operational semantics which is indexed by the cost of evaluation.

Operationally, \dlambdaamor behaves like a call-by-name monadic version of PCF. The cost semantics, for which selected rules are presented in \autoref{fig:dlambdaamor-selected-sem-rules}, consists of two separate judgments. First is a \textit{pure} evaluation relation: $e \Downarrow v$, which evaluates an expression of type $\tau$ to a value of the same type. Evaluations in this relation are not thought to incur any cost. The set of values includes all of the monadic actions, which must be subsequently \textit{forced} to be evaluated. This is accomplished with the forcing evaluation relation $e \Downarrow^\kappa v'$, which relates monadic values of type $\M \, I \, \tau$ to values of type $\tau$. The pure evaluation judgment is an entirely standard big-step semantics, but selected rules from the forcing judgment can be found in \autoref{fig:dlambdaamor-selected-sem-rules}.

The rules for the pure evaluation relation are straightforward- as all monadic terms are values, the pure relation simply behaves like a big-step evaluation relation for by-name PCF. The rules for the refinement syntax at term level behave as if the syntax for refinements has been erased at runtime- they contribute nothing meaningful to the operational semantics.

The rules for the forcing relation warrant some discussion. Since all monadic computations are values, the forcing relation depends on the pure relation to evaluate sub-expressions. For instance, the forcing relation evaluates $\texttt{ret}(e)$ to $v$ in $0$ steps when $e \Downarrow v$. The pure relation will take some steps of computation by performing $\beta$-redexes, but we will not consider these to be \textit{costly}--- only \texttt{tick}s incur any cost. Pure evaluations steps thus do not need to be accounted for in the forcing relation.

Most importantly, the $\texttt{tick}[I|\vec{p}]$ term evaluates with cost $\phi(I,\vec{p})$ to the trivial value $()$. This rule encodes the heretofore intutitive cost behavior of the type $\M (I,\vec{p}) \, \tau$, by explicitly assigning the atomic costly operation the cost $\phi(I,\vec{p})$ in our cost semantics.  The final cost-monadic term, the \texttt{bind}, is assigned cost in a purely compositional way. The evaluation of \texttt{bind} proceeds like the evaluation of a let-binding, where the costs of forcing the argument and then the subsequent continuation are added, and given as the total cost.

Finally, the two potential-related operations incur no semantic cost. This may come as a surprise-- the statically predicted cost for the \texttt{store} operation (for example) is the amount of potential to be allocated. However, this cost is entirely for bookeeping purposes to ensure that potentially is used soundly: it is not truly incurred when the program runs. Similarly, the \texttt{release} operation runs identically to \texttt{bind}: it is simply a monadic sequencing. This ``ghost" nature of potential at runtime is congruent with the way we think about amortized analysis. Recalling the notation of \autoref{ch:intro}, the operational semantics give the costs $C(f)$, while the static types encode the amortized cost $A(f) + \Delta\Phi$.

\begin{figure}
\input{figs/selected-sem-rules}
\caption{Selected Rules of \dlambdaamor's Cost Semantics}
\label{fig:dlambdaamor-selected-sem-rules}
\end{figure}

\subsection{Embedding of \dlambdaamor in \lambdaamor}
\label{sec:dlambdaamor-embedding}
The translation of \dlambdaamor into \lambdaamor requires little insight: we simply compile the costs and potentials written abstractly as a base and potential vector to the $\phi$ function applied to a pair. Concretely, the meat of the translation on types consists of two rules: the \dlambdaamor cost type $\M(I,\vec{p}) \, \tau$ is translated to the \lambdaamor $\M \left(\phi(I,\vec{p})\right)\, \tau'$, and the potential type $[I|\vec{p}] \, \tau$ is translated to $\left[\phi(I,\vec{p})\right] \, \tau'$, where $\tau'$ is the translation of $\tau$. These translations respect rules of the two type systems: the monotonicity and additivity of the $\phi$ function from \autoref{thm:phi-linear} justify the translations of the \texttt{bind} and \texttt{release} operations, as well as the subtyping rule for costs and potentials. The rest of the translation is primarily an erasure. \lambdaamor's syntax includes no explicit index terms or types at the term level, and so these are all erased. Finally, the shift operation is erased, a move which is justified by \autoref{thm:raml-shift}.

For the remainder of the section, we will write the embedding on all syntactic forms as $(\cdot)^\circ$. Further, the typing judgments of \lambdaamor will be distinguished from those of \dlambdaamor by using a $\Vdash$ for their turnstile, while the evaluation relation of \lambdaamor will be written with a single arrow $\downarrow$.

\begin{theorem}
\label{thm:dla-trans-sound}
If $\Psi ; \Theta ; \Delta ; \Omega ; \Gamma \pvdash e : \tau$ then $\Psi^\circ ; \Theta^\circ ; \Delta^\circ ; \Omega^\circ ; \Gamma^\circ \Vdash e^\circ : \tau^\circ$
\end{theorem}


\subsubsection{Statement of Soundness of \dlambdaamor}
To prove the soundness of \dlambdaamor, we begin by noting that its operational semantics are preserved under the erasure to \lambdaamor. This is to be expected: \dlambdaamor's cost semantics is simply that of \lambdaamor, only written in terms of the abstract costs $\phi(I,\vec{p})$.

\begin{theorem}
\label{thm:dla-trans-sem-sound}
If $e \Downarrow^\kappa v$, then $e^\circ \downarrow^\kappa v^\circ$
\end{theorem}

From this, the soundness theorem for \dlambdaamor follows immediately: the actual cost of running a closed monadic computation is bounded above by its statically-predicted amortized cost.

\begin{theorem}
\label{thm:dlambdaamor-sound}
If $\cdot \pvdash e : \M \, (I,\vec{p}) \, \tau$ and $e \Downarrow^\kappa v$, then $\kappa \leq \phi(I,\vec{p})$.
\end{theorem}
\begin{proof}
By \autoref{thm:dla-trans-sound} and \autoref{thm:dla-trans-sem-sound}, we have that $\cdot \Vdash e^\circ  : \M \, \phi(I,\vec{p}) \, \tau^\circ$, and $e^\circ \downarrow^\kappa v^\circ$. Then, by Theorem 1 of \citet{rajani-et-al:popl21}, we have that $\kappa \leq \phi(I,\vec{p})$, as required.
\end{proof}



\section{Algorithmic Type System of \bilambdaamor}
\label{sec:bilambdaamor-syntax-and-types}

\begin{figure}
\input{figs/bilambdaamor-typing-judgments}
\caption{Judgment Forms of the \bilambdaamor Type System}
\label{fig:bilambdaamor-typing-judgments}
\end{figure}

With the formalism of \dlambdaamor under control, we move on to presenting the formalism for \bilambdaamor. In short, this entails applying all of the type system algorithmization techniques from \autoref{sec:bilambdaamor-overview} to \dlambdaamor at once. As one might expect, this yields a fair bit of complexity in the type system, and so we spend a good deal of space in this section exploring the rules of \bilambdaamor in depth, and commenting on some non-obvious design decisions along the way. Additionally, we will formalize the type normalization procedure that is built into \bilambdaamor's two-phase subtyping.

\subsubsection{Syntax}
The syntax of \bilambdaamor is nearly identical to that of \dlambdaamor: this is by design, as \bilambdaamor is intended to be an implementable version of \dlambdaamor. The only difference is the addition of the type annotation syntax $(e : \tau)$ described in \autoref{sec:bilambdaamor-overview-bidir}. The main change between the two type systems is in the forms of the judgments. Some judgments change in only minor ways: the sort-checking, kind-checking, and constraint well-formedness judgments are all the same as in \dlambdaamor, with the exception of the added constraint outputs as described in \autoref{sec:bilambdaamor-overview-constr}. The subtyping judgment also sports a constraint output, but is also is split into two, with first a ``normal form subtyping" relation which judges one type to be a subtype of another when both are in normal form, and then the general algorithmic subtyping relation which relates two types by normalizing them and then relating them via the normal form subtyping relation. Finally, the typing judgment changes the most: it splits into a checking ($\downarrow$) and inferring/synthesis ($\uparrow$) judgment to support bidirectional type inference, with added constraint outputs for solving and unused variable context output for the I/O method. These judgment forms are all shown in \autoref{fig:bilambdaamor-typing-judgments}.


\subsubsection{Sorts, Kinds, and Well-Formed Constraints}
\begin{figure}
\input{figs/bilambdaamor-selected-sort-kind-constr-rules}
\caption{Selected Algorithmic Sort, Kind, and Constraint Rules}
\label{fig:bilambdaamor-selected-sort-kind-constr-rules}
\end{figure}

As we will see is true for the majority of the judgments of \bilambdaamor, the majority of the rules from \dlambdaamor carry over with only minor modification. Although they do form the typing rules for a (small) language embedded in \bilambdaamor, the sort-assignment, kind-assignment and well-formedness judgments for index terms, types, and constraints respectively, do not require a bidirectional treatment. This is because all binders in these three syntactic categories are fully annotated, and so we can easily implement sort/kind inference and checking without any difficulty. Similarly, since the index and type variable contexts are fully structural, there is no need for the I/O method. Hence, the only modification to these three judgments is the addition of the constraint output.

Intuitively, the three judgments all have very simple meanings: for instance, $\Theta ; \Delta \vdash I : S \gens \Phi$ is intended to mean that when $\Phi$ is valid, $\Theta ; \Delta \vdash I$ holds declaratively, and similarly for the other two judgment forms. This intuition is made formal by the soundness proofs in \autoref{sec:metatheory}.

We present a few selected rules from these judgments in \autoref{fig:bilambdaamor-selected-sort-kind-constr-rules}. As mentioned earlier, the vast majority of rules are carried over from \dlambdaamor: two good examples are AI-Plus and AC-Conj, which follow an identical structure to their declarative counterparts, and simply conjoin the output contexts from the premises in the conclusion.

Some declarative rules have an instance of the constraint validity relation as a premise: for example, the rule I-Minus requires $\Theta ; \Delta \vDash I \geq J$ to judge $\Theta ; \Delta \vdash I - J : bS$. In the algorithmic judgments of \bilambdaamor, these constraints are conjoined onto the output constraint of the conclusion. The algorithmic rule corresponding to I-Minus, AI-Minus, exemplifies this pattern. It has two premises to check that the two subterms $I$ and $J$ are of the proper sort, which emit constraints $\Phi_1$ and $\Phi_2$, respectively. The output constraint is then $\Phi_1 \wedge \Phi_2 \wedge (I \geq J)$. This is constructed in such a way that our eventual soundness theorem will be simple, if the output constraint is valid, then so is $I \geq J$, and we can thus use $\Theta ; \Delta \vDash I \geq J$ to construct a declarative proof that $I - J$ is sort-correct.

In a similar manner, in rules where premises bind index variables or assume constraints, the bound variable or constraint must be introduced to the conclusion's output constraint to maintain the well-formedness of output constraints. As an example, consider the rules AK-Forall and AC-Forall. Their premises output constraints $\Phi$ which may (and usually do) mention the universal index variable $i : S$, which is bound in the context $\Theta$. Then in the conclusion, this variable is no longer present, and so $\Phi$ need not be well-formed. To fix this, we explicitly quantify over the index variable $i$ in the conclusion's output constraint. 

The assumption context $\Delta$ bears a similar requirement, as illustrated by the rule AI-Sum. When the constraint $I_0 \leq i \leq I_1$ is assumed in a premise which emits a constraint $\Phi_3$, the output constraint is transformed to $I_0 \leq i \leq I_1 \to \Phi_3$ to preserve the meaning of the judgment.

Finally, it's worth noting a potential confusion about the algorithmic constraint well-formedness judgment, $\Theta ; \Delta \vdash \Phi \; \texttt{wf} \gens \Phi'$.
The output constraint $\Phi'$ does not encode the truth of $\Phi$. The soundness proof will make this concrete, but knowing that $\Theta ; \Delta \vDash \Phi'$ only implies that $\Phi$ is well-formed, but it need not be valid.

\subsubsection{Algorithmic Context-Wellformedness}
\begin{figure}
\input{figs/bilambdaamor-ctx-wf-rules}
\caption{Algorithmic Context Wellformedness Rules}
\label{fig:bilambdaamor-ctx-wf-rules}
\end{figure}

While these judgments are not a part of the typechecking algorithm we will eventually implement, \bilambdaamor includes algorithmic versions of the two context-wellformedness judgments from \dlambdaamor to be used in the presuppositions for the rest of the algorithmic judgments. The two judgments, constraint context well-formedness ($\Theta \vdash \Delta \; \texttt{wf} \gens \Phi$) and term variable context well-formedness ($\Psi ; \Theta ; \Delta \vdash \Gamma \; \texttt{wf} \gens \Phi$) have the expected intended meaning: when the output constraint is valid, the declarative version is thought to hold.

These judgments will not need to be implemented since typechecking will begin from empty contexts, where both judgments hold vacuously. Finally, we note that \bilambdaamor does not have an algorithmic version of the declarative context subsumption judgment, since it only exists in the declarative theory to be used in the weakening rule, which \bilambdaamor does not have. Indeed, weakening of the term context in \bilambdaamor is admissible, a fact which will be proven in \autoref{sec:metatheory}.


\subsection{Normalization of Types}
\label{sec:dlambdaamor-normalization}
To circumvent the issue of deciding $\beta$-equality of types as a part of \bilambdaamor's subtyping routine, we employ a normalization (or evaluation) procedure to eliminate all $\beta$-redexes from a type. Once these $\beta$-redexes have been eliminated, subtyping only requires congruence rules. The normalization proof that we describe in this section is a normalization relative to the equational theory induced by \dlambdaamor's subtyping relation, that is to say: we will eventually prove that a type and its normal form are mutual subtypes of each other \textit{with the subtyping relation of \dlambdaamor}. 
%This may seem strange-- after all, the normalization is required for \bilambdaamor's subtyping relation. However, we will see in \autoref{sec:metatheory} that to prove the completeness of \bilambdaamor's algorithmic subtyping, a normalization proof for \dlambdaamor's subtyping is exactly what's required. Moreover, the eventual soundness and completeness theorems for algorithmic subtyping will allow us to transport this normalization result to \bilambdaamor's type system when required.

This normalization procedure computes \textit{normal forms} for types, which should be thought of as canonical representatives of their $\beta$-equivalence classes. These normal forms can characterized syntactically: we present a pair of relations $\tau \, \texttt{ne}$ and $\tau \, \texttt{nf}$, which judge a type to be neutral or normal, respectively. Neutral types are those which can be of arrow kind, but will not induce any $\beta$-redexes when applied to an argument. Normal types are types which include no $\beta$-redexes. The former are required to define the latter: the type $\tau \, I$ is only in normal form when $\tau$ is not of the form $\lambda i : S.\tau'$. The rules generating these two relations can be found in \autoref{appendix:a}.

Before we present the normalization function, let us a moment to consider why the solution we are about to present is so simple. Proofs of normalization for most calculi require fairly high-powered techniques such as logical relations, categorical semantics, or hereditary substitution. The inherent complexity of normalization proofs stems from the fact that straightforward induction on terms rarely works, since one would need to induct on substitution instances of lambda terms, which are not subterms of the original term. However, \dlambdaamor's type-level lambdas do not range over types, they range over index terms. Because of this, substituting an index term into a type also cannot introduce any new type-level $\beta$-redexes, and so any substitution instance of an open type in normal form is also normal.

%These observations are codified in the following theorems:

%\begin{restatable}{theorem}{evalsmaller}
%$\#\texttt{eval}(\tau) \leq \#\tau$, where $\#(\cdot)$ denotes the number of connectives in a type.
%\end{restatable}
%\begin{proof}
% Induction on $\tau$
%\end{proof}


\begin{restatable}{theorem}{idxsubstnf}
\label{thm:idx-subst-nf}
~\begin{enumerate}
  \item If $\tau \; \texttt{ne}$ then $\tau[I/i] \; \texttt{ne}$
  \item If $\tau \; \texttt{nf}$ then $\tau[I/i] \; \texttt{nf}$
\end{enumerate}
\end{restatable}
%\begin{proof}
%We prove the two claims simultaneously by induction on the derivations of $\tau \; \texttt{ne}$ and $\tau \; \texttt{nf}$.
%\end{proof}

Because of these simplifying factors, we can define an evaluation function \texttt{eval} defined inductively on the structure of types which computes normal forms.
The most important clauses of the definition can be found in \autoref{fig:selected-eval-rules}. For all of the logical connectives, the definition proceeds compositionally-- the remaining rules can be found in \autoref{appendix:a}

\begin{figure}
\input{figs/selected-eval-rules}
\caption{Selected Clauses of the \texttt{eval} Function}
\label{fig:selected-eval-rules}
\end{figure}

The most important (and only nontrivial) clause of the definition is the application case. To evaluate $\tau \; I$, we begin by evaluating $\tau$. If its normal form
is a lambda, we simply perform the $\beta$-reduction. Note that we do not need to evaluate this substitution instance, as it must already be in normal form by \autoref{thm:idx-subst-nf}, assuming the correctness of the \texttt{eval} function. Otherwise, we simply re-apply the index term $I$.

It is not immediately clear that this function in fact computes what we want! For \texttt{eval} function to be a normalization procedure, its image must consist only of types in normal form, and every type must be equivalent to its evaluation. Note that we do not prove the stronger property that equivalence is completely characterized by syntactic equality of normal forms (up to equality of index terms). While almost certainly true, this property requires a bit more work to prove and is not required for the discussion, and so we omit it. Finally, we must also prove that the \texttt{eval} function preserves kinds-- this proof follows the same inductive structure as the proof of normalization, and so we bundle them together. We present the case for evaluation below, and the remainder of the cases can be found in \autoref{appendix:a}. The Normalization Theorem does depend on a small canonical forms lemma: types of arrow kind in normal form must either be lambdas or neutral.

\begin{restatable}[Canonical Forms for $S \to K$]{theorem}{canonforms}
If $\Psi ; \Theta ; \Delta \vdash \tau : S \to K$ and $\tau \; \texttt{nf}$, then either:
\begin{enumerate}
  \item $\tau = \lambda i : S.\tau'$ with $\tau' \; \texttt{nf}$
  \item $\tau \; \texttt{ne}$
\end{enumerate}
\end{restatable}


\begin{restatable}[Normalization Theorem]{theorem}{normthm}
\label{thm:norm-thm}
If $\Psi ; \Theta ; \Delta \pvdash \tau : K$, then:
\begin{enumerate}
  \item $\Psi ; \Theta ; \Delta \pvdash \texttt{eval}(\tau) : K$
  \item $\Psi ; \Theta ; \Delta \pvdash \tau \equiv \texttt{eval}(\tau) : K$
  \item $\texttt{eval}(\tau) \; \texttt{nf}$
\end{enumerate}
\end{restatable}


\begin{restatable}{theorem}{idxsubsteval}
~$\texttt{eval}(\tau[J/i]) = \texttt{eval}(\tau)[J/i]$
\label{thm:idx-subst-eval}
\end{restatable}

\subsubsection{Algorithmic Subtyping}

\begin{figure}
\input{figs/bilambdaamor-selected-subty-rules}
\caption{Selected Algorithmic Subtyping Rules}
\label{fig:bilambdaamor-selected-subty-rules}
\end{figure}

In \bilambdaamor, there is not one subtyping judgment, but two. The first, which we will refer to as ``normal form" subtyping (denoted $\subtynf$),  judges one type to be a subtype of another when both are in normal form. This relation contains all of the congruence rules from \dlambdaamor's subtyping relation-- these are simply transcriptions of their declarative counterparts. Just like with sort/kind-checking, constraint-validity premises are shuffled to the constraint output of the conclusion, and variables bound in premises are quantified over. Deciding a subtyping relation which only includes congruences reduces to syntax-directed search and solving the emitted constraints. Hence, the subtyping relation, selected rules of which can be found in \autoref{fig:bilambdaamor-selected-subty-rules}, is certainly algorithmic.

The second judgment (denoted $\subty$) is generated by a single rule, AT-Normalize. AT-Normalize encodes the first step of our two-step subtyping algorithm.
To show that $\Psi ; \Theta ; \Delta \vdash \tau_1 \subty \tau_2 : K \gens \Phi$, it suffices (and indeed it is necessary) to first normalize $\tau_1$ and $\tau_2$, and then judge that their normal forms are related by the normal form subtyping judgment, $\Psi ; \Theta ; \Delta \vdash \texttt{eval}(\tau_1) \subtynf \texttt{eval}(\tau_2) : K \gens \Phi$.
 
%This strategy should be familiar to the reader familiar with implementing dependently-typed languages. When implementing a dependent type theory, it is necessary to check equality of types, which may include programs. To do so, one first normalizes the types, and then checks them for syntactic equality-- \bilambdaamor's algorithmic subtyping is simply a directed version of this.

Two distinct phases and normalization aside, the remaining way that \bilambdaamor's subtyping differs from \dlambdaamor's is in the removal of two rules. The rules S-Refl and S-Trans from \dlambdaamor are not included in our algorithmic subtyping relation, as they are not syntax-directed. In \autoref{sec:metatheory}, we show that reflexivity and transitivity are admissible for types in normal form, and that these results may be lifted to the full relation through evaluation.


\subsubsection{Bidirectional Typing Rules}

\begin{figure}
\input{figs/bilambdaamor-selected-typing-rules}
\caption{Selected Algorithmic Typing Rules}
\label{fig:bilambdaamor-selected-typing-rules}
\end{figure}

As expected, the typing judgments of \bilambdaamor change the most. For one, we pass to a bidirectional type system. As discussed in \autoref{sec:bilambdaamor-overview-bidir}, this process is fairly standardized, and so the reader who has seen bidirectional type systems in the past will find no surprises in \bilambdaamor. The typing judgment is split in two, yielding a mutually recursive pair of checking and inference judgments. Secondly, typing judgment sports a constraint output in a manner identical to all of the other algorithmic judgments discussed so far. Finally, to handle the affine context $\Gamma$ in an algorithmic way, we employ the I/O method from \autoref{sec:bilambdaamor-overview-io}, adding an output context of unused variables $\Gamma'$, which are threaded through rules in a state-passing manner.

While all of these algorithmization techniques were described in the abstract in \autoref{sec:bilambdaamor-overview}, understanding how they work in the context of a type system as feature-rich as \bilambdaamor is another matter entirely. To this end, we take some time to describe the selected rules presented in \autoref{fig:bilambdaamor-selected-typing-rules}.

Algorithmizing the declarative typing rules is mostly mechanical for the rules governing logical connectives, but requires some ingenuity for the non-logical ones. For each declarative typing rule, we bidirectionalize each premise along with the conclusion, convert to IO-style contexts, and output constraints. In most cases, this is sufficient to algorithmize a rule. However, for some of the cost-related rules like AT-bind or AT-Release, further ``optimization" are required. In these cases, we build certain subtyping relations into the algorithmic typing rules to overcome the fact that subtyping can only be applied at the boundary between checking and inference, and not at will, as in the declarative typing rules of \dlambdaamor.

We begin with AT-Var-1, which allows us to use variables from the affine context. When $x : \tau \in \Gamma$, the term $x$ infers the type $\tau$. Using this rule in a derivation counts as a use of $x$, and so $x$ must be removed from the output context, as it is no longer unused.

The pair of rules AT-Lam and AT-App exhibit a common pattern which is common to nearly all negative logical connectives in \bilambdaamor. For introduction form, both the conclusion and premise are checking. In the elimination form, the conclusion as well as the principal judgment (the judgment typing the term being eliminated) are inferring, while all other premises check
\footnote{
The connection between bidirectional type systems and polarization/focusing which makes this pattern so ubiquitous in the rules of \bilambdaamor is deep, beautiful, and not fully understood. A wonderful overview of work on the subject, as well as exposition about how to bidirectionalizing your own declarative type systems can be found in a paper by \citet{dunfield19:bidir-survey}.
}.
The AT-Lam rule also illustrates a small oddity of the I/O method when applied to affine types. Since variables \textit{can} be left unsued, it's possible for the $\lambda$-bound variable $x$ to end up in the output context $\Gamma'$ of the premise checking the body of the lambda. For this reason, we must explicitly remove $x$ from the context of unused variables as it falls out of scope, lest it be possible to typecheck terms like $\angles{\lambda x. (), x}$, where a bound variable escapes its scope. The rule AT-App also illustrates how the ``threading" aspect of the I/O method is easily combined with the two kinds of typing judgments. To check $e_1 \, e_2$ in context $\Gamma$, the type of $e_1$ is inferred, returning unused variables $\Gamma_1$. Then, the type of $e_2$ is checked \textit{in context $\Gamma_1$}. That judgment ``returns" $\Gamma_2$, which is then used as the output judgment for the checking conclusion.

Dually, the rules AT-TensorI and AT-TensorE are a simple instance of the bidirectional rules for a positive logical connective. The introduction form has checking premises and conclusions, just like the negatives. On the other hand, the elimination form has an inferring principle judgment, but checking conclusion-- this is because positive elims all take the form of a (many-armed) let-binding, and the type of the continuation cannot be inferred locally. Because of this let-binding style, most positive elims must remove bound variables from the output context to deal with the same scoping issue as AT-Lam.

The remaining rules for logical connectives following a similar pattern: their bidirectional behavior is predetermined by logical concerns discovered by prior work in the area. Algorithmizing the rules for nonlogical connectives, however, requires quite a bit more work and cleverness.

As a case study, consider the rule T-Bind from \dlambdaamor:

$$
\inferrule
{
\Psi ; \Theta ; \Delta ; \Omega ; \Gamma_1 \vdash e_1 : \M \, (I,\vec{p})\, \tau_1\\
\Psi ; \Theta; \Delta ; \Omega ; \Gamma_2, x:\tau_1 \vdash e_2 \checks \M \, (I,\vec{q})\, \tau_2\\
}{
\Psi ; \Theta ; \Delta ; \Omega ; \Gamma_1,\Gamma_2 \vdash \texttt{bind } x = e_1 \texttt{ in } e_2 \checks \M \, (I,\vec{p} + \vec{q})\, \tau_2
}
$$

This plainly follows the let-binding style of positive elimination forms (despite not being a logical connective), and so the same direction pattern seems like a good choice. This rule has no constraint solving premises, and so the output constraints can be conjoined together. Finally, this term has the form of a let-binding, and so we thread contexts through the premises, removing $x$ in the conclusion. These three choices lead to the following first cut at an algorithmic bind rule:

$$
\inferrule
{
\Psi ; \Theta ; \Delta ; \Omega ; \Gamma \vdash e_1 \infers \M \, (I,\vec{p})\, \tau_1 \gens \Phi_1,\Gamma_1\\
\Psi ; \Theta; \Delta ; \Omega ; \Gamma_1, x:\tau_1 \vdash e_2 : \M \, (I,\vec{q})\, \tau_2 \gens \Phi_2,\Gamma_2\\
}{
\Psi ; \Theta ; \Delta ; \Omega ; \Gamma \vdash \texttt{bind } x = e_1 \texttt{ in } e_2 : \M \, (I,\vec{p} + \vec{q})\, \tau_2 \gens \Phi_1 \wedge \Phi_2, \Gamma_2 \setminus \{x\}
}
$$

Unfortunately, this rule is insufficient for potentially subtle reasons. When we implement \bilambdaamor, the checking judgment is implemented as a function (essentially) of type \texttt{ctx -> tm -> ty -> unit}, which proceeds by a (very large) case analysis on the term and type arguments. The algorithmic bind rule corresponds to the case where the term is the constructor for bind, and the type is the cost monad. However, we hope to not match further into the type to match the index term $\vec{p} + \vec{q}$, because the second component of the cost need not be syntactically a sum of potential vectors! To fix this, we take a slightly different approach. Instead of typing the conclusion at type $\M \, (I,\vec{p} + \vec{q})\, \tau_2$, we will instead have it check against the type $\M \, (I, \vec{q})\, \tau_2$, so long as the continuation checks against $\M \, (I,\vec{q} - \vec{p}) \, \tau_2$ (when $\vec{q} \geq \vec{p}$). Intuitively, this new rule encodes the same logic: the total amortized cost of the composite is the sum of the costs of $e_1$ and $e_2$.

A similar situation plays out if we consider the first component of the cost pair. The rule above indicates that the first components in the three monadic types need to be \textit{identical}. Just like requiring that the second component be \textit{literally} a sum, this is far too strong a condition: we only need require that they are provably equal. This leads us to the completed AT-Bind rule, as shown in \autoref{fig:bilambdaamor-selected-typing-rules}. A nearly identical game is played with the rule for the potential elimination form, AT-Release: we generalize the potentials to have syntactically but provably equal bases.

The introduction rules for monads and potentials also require some tweaking. To illustrate, we consider the declarative store rule, T-Store.
$$
\inferrule
{
\Theta ; \Delta \vdash I : \mathbb{N}\\
\Theta ; \Delta \vdash \vec{p} : \vec{\mathbb{R}^+}\\
\Psi ; \Theta ; \Delta ; \Omega ; \Gamma \vdash e : \tau\\
}{
\Psi ; \Theta ; \Delta ; \Omega ; \Gamma \vdash \texttt{store}[I|\vec{p}](e) : \M \, (I,\vec{p}) \, ([I| \vec{p}] \, \tau)
}
$$
We bidirectionalize this in a straightforward manner, by making both the premise and the conclusion checking. The constraint and context outputs are similarly trivial: they are passed from the output of the premise directly to the conclusion. We are then faced with yet another matching problem: the $I$s and $\vec{p}$s in the term and type are required to be syntactically equal. It is clear how to generalize the bases: we allow all three to be different, but provably equal. The proper formulation for the coefficient components is less clear, however. Inspiration comes from considering the ranges of sound but imprecise typings for the positions. For 
$\texttt{store}[K|\vec{w}](e)$ to check against $\M \, (I,\vec{q}) \, \left([J|\vec{p}]\right)$ when $I = J = K$, it ought to be allowable for $\vec{w}$ to be smaller than $\vec{q}$, and for $\vec{p}$ to be smaller than $\vec{w}$. When we ask for $\phi(I,\vec{q})$ potential, it is sound to overpay, and underdeliver. The final rule, AT-Store, allows just this.

%This optimization is justified by the subtyping rules AS-Pot and AS-Monad: an alternate way of thinking of AT-Store is that it's the ``basic" AT-Store derived from the declarative version, with subtyping baked in.


The last two interesting rules to be discussed are AT-Sub and AT-Anno. These rules are not analogues of rules which were present in \dlambdaamor. Instead, they are the two bidirectional-specific rules discussed in \autoref{sec:bilambdaamor-overview-bidir} which allow us to mediate between the checking and inference judgments. When a synactic form whose corresponding rule has a checking conclusion (such as a lambda) is placed in a position where its expected to infer (such as the principal position of application), an annotation must be introduced. However, in the opposite situation, a term whose rule has an inferring conclusion may always be used in a checking position, so long as the type which is inferred is more specific than the one the term is being checked against.


\subsubsection{Well-formedness and Presuppositions}
The judgments of \bilambdaamor presented thusfar have all been \textit{raw} judgments, in the same sense that we have presented no well-formedness restrictions. Just like in \dlambdaamor, we restrict the positions of each relation by well-formedness presuppositions. Again, these are denoted with a subscript $p$ on the turnstile.
Unlike, \dlambdaamor, these presuppositions are algorithmic in the sense that they use the corresponding judgments from \bilambdaamor to impose restrictions. Recalling that the intended meaning of an algorithmic judgment with constraint output is that the declarative analogue holds when the constraint is valid, all of the conditions in algorithmic presuppositions require their constraints to be solved.  This is a natural restriction: once soundness and completeness have been established, these algorithmic presuppositions are equivalent to their declarative counterparts.

\begin{definition}
We write $\Theta ; \Delta \pvdash I : S \gens \Phi$ to mean
\begin{enumerate}
  \item $\Theta \vdash \Delta \; \texttt{wf} \gens \Phi_1$ and $\Theta ; \cdot \vDash \Phi_1$
  \item $\Theta ; \Delta \vdash I : S \gens \Phi$
\end{enumerate}
\end{definition}

\begin{definition}
We write $\Psi ; \Theta ; \Delta \pvdash \tau : K \gens \Phi$ to mean
\begin{enumerate}
  \item $\Theta \vdash \Delta \; \texttt{wf} \gens \Phi_1$ and $\Theta ; \cdot \vDash \Phi_1$
  \item $\Psi ; \Theta ; \Delta \vdash \tau : K \gens \Phi$
\end{enumerate}
\end{definition}

\begin{definition}
We write $\Psi ; \Theta ; \Delta \pvdash \tau \subty \tau' : K \gens \Phi$ to mean that
\begin{enumerate}
  \item $\Theta \vdash \Delta \; \texttt{wf} \gens \Phi_1$ and $\Theta ; \cdot \vDash \Phi_1$
  \item $\Psi ; \Theta ; \Delta \vdash \tau : K \gens \Phi_2$ and $\Theta ; \Delta \vDash \Phi_2$
  \item $\Psi ; \Theta ; \Delta \vdash \tau' : K \gens \Phi_3$ and $\Theta ; \Delta \vDash \Phi_2$
  \item $\Psi ; \Theta ; \Delta \vdash \tau \subty \tau' : K \gens \Phi$
\end{enumerate}
\end{definition}

\begin{definition}
We say that $\Psi ; \Theta ; \Delta ; \Omega ; \Gamma \pvdash e : \tau \gens \Phi,\Gamma'$ when
\begin{enumerate}
  \item $\Theta \vdash \Delta \; \texttt{wf} \gens \Phi_1$ and $\Theta ; \cdot \vDash \Phi_1$
  \item $\Psi ; \Theta ; \Delta \vdash \Omega \; \texttt{wf} \gens \Phi_2$ and $\Theta ; \Delta \vDash \Phi_2$
  \item $\Psi ; \Theta ; \Delta \vdash \Gamma \; \texttt{wf} \gens \Phi_3$ and $\Theta ; \Delta \vDash \Phi_3$
  \item $\Psi ; \Theta ; \Delta \vdash \tau : \star \gens \Phi_4$ and $\Theta ; \Delta \vDash \Phi_4$.
  \item $\Psi ; \Theta ; \Delta ; \Omega \vdash e : \tau \gens \Phi,\Gamma'$
\end{enumerate}
\end{definition}

%Note the lack of structural rules

\section{Soundness and Completeness of \bilambdaamor with respect to \dlambdaamor}
\label{sec:metatheory}
With the algorithmic system of \bilambdaamor in place, the time has come to prove theorems about it. Ideally, we would like to prove that it behaves exactly the same as \dlambdaamor. That way, when we build \lambdaamorimpl in \autoref{sec:lambdaamor-impl}, we will know that (a) every program typechecked by our implementation declarative has the proper type in \dlambdaamor, and that (b) every well-typed program in \dlambdaamor \textit{can be} checked by our implementation. In this context, these two properties are known as soundness and completeness\footnote{
This may seem backwards to the reader already familiar with the terms-- we think of the declarative system as giving a ``ground truth" semantics of which terms have which types, and the algorithmic system as a proof system in which one may manually derive proofs of well-typedness. From this perspective, soundness and completeness are as described above.
}, respectively.

As is usually the case with such things, the soundness proofs are very straightforward. This is because \bilambdaamor is far more strict and structured than \dlambdaamor, so it is always fairly easy to lift a \bilambdaamor proof to a proof in \dlambdaamor. This mismatch simultaneously makes completeness quite difficult to prove: compiling a proof of one of the judgments of \dlambdaamor down to a structured one in \bilambdaamor requires some work in general. For this reason, we will begin with proving the soundness theorems, and subsequently move to proving completeness.

The general shape of the soundness theorems are all the same: for every algorithmic judgment $\mathcal{J} \gens \Phi$ (and corresponding declarative judgment $\mathcal{J}$) we prove that if there is a derivation of $\mathcal{J} \gens \Phi$ and $\Phi$ is valid, then there is a derivation of $\mathcal{J}$. Individual theorems may vary-- the inclusion of bidirectionality and the I/O method complicates the statement of soundness for typing-- but this is the main flavor. This pattern justifies the intended use of the algorithmic system: we derive algorithmic judgments using the implementation, which outputs constraints. If the constraints are solvable by a solver, the corresponding declarative judgment holds.

Dually, the completeness theorems have the ``opposite" shape: if $\mathcal{J}$ holds, then there is some \textit{valid} $\Phi$ such that the corresponding algorithmic judgment $\mathcal{J} \gens \Phi$ is derivable. Modulo handling the bells and whistles of an individual judgment (such as IO contexts or bidirectionality), all of our completeness statements will have this form. To simplify the wording of our theorems somewhat, we adopt the ``Twelf convention": theorem meta-variables which appear only in the conclusion of a theorem are implicitly existentially quantified. For instance, we will write ``If $\mathcal{J}$ then $\mathcal{J} \gens \Phi$ and $\vDash \Phi$" to mean ``If $\mathcal{J}$ then there is some $\Phi$ such that $\mathcal{J} \gens \Phi$ and $\vDash \Phi$".

Intuitively, this theorem structure justifies our intended usage of \lambdaamor, as an implementation of the algorithmic judgments must cover all possible uses of the declarative system. When combined with soundness (to translate algorithmic judgments back into declarative ones), our algorithm always succeeds to derive a proof of a declarative judgment, if one exists.

\subsubsection{Soundness of Index Terms, Constraints, Contexts, and Types}
The four most basic algorithmic judgments of \bilambdaamor mirror their declarative counterparts rule-for-rule: the only ``real" modification is the addition of constraint output. This uniformity means that the soundness proofs are fairly trivial single-pass inductions on derivations. Each of these proofs comes in two parts. First, we prove that the soundness holds as a a statement about ``raw" judgments by omitting the presuppositions. These theorems are garbage-in, garbage-out: malformed judgments in \bilambdaamor are sent to malformed judgments in \dlambdaamor. Afterwards, we prove that the presuppositions are preserved, and so well-formed judgments in \bilambdaamor are sent to well-formed judgments in \dlambdaamor. This two-step process is only necessary because the presuppositions have a mutually inductive structure: to untangle the knot, we must first prove the raw statements, and then repackage them with the required presuppositions afterwards.


Below, we will only present the versions of the theorems with presuppositions included: the gory details can be found in \autoref{appendix:a}. All of the proofs proceed by elementary inductions on derivations, occasionally using easy properties about constraint validity.

\begin{restatable}[Soundness of Index Context Well-Formedness]{theorem}{idxctxwfsound}
If $\Theta \vdash \Delta \; \texttt{wf} \gens \Phi$ and $\Theta ; \cdot \vDash \Phi$, then $\Theta \vdash \Delta \; \texttt{wf}$
\label{thm:idx-ctx-wf-sound}
\end{restatable}

\begin{restatable}[Soundness of Sort Checking]{theorem}{sortsound}
If $\Theta;\Delta \pvdash I : S \gens \Phi$ and $\Theta;\Delta \vDash \Phi$, then $\Theta;\Delta \pvdash I : S$ 
\label{thm:sort-sound}
\end{restatable}

\begin{restatable}[Soundness of Constraint Well-Formedness]{theorem}{constrsound}
If $\Theta ; \Delta \pvdash \Phi \texttt{ wf} \gens \Phi'$ and $\Theta ; \Delta \vDash \Phi'$ then $\Theta ; \Delta \pvdash \Phi \texttt{ wf}$
\label{thm:constr-sound}
\end{restatable}

\begin{restatable}[Soundness of Kind Checking]{theorem}{kindsound}
If $\Psi ; \Theta ; \Delta \pvdash \tau : K \gens \Phi$ and $\Theta ; \Delta \vDash \Phi$ then $\Psi ; \Theta ; \Delta \pvdash \tau : K$.
\label{thm:kind-sound}
\end{restatable}

\subsubsection{Soundness of Subtyping}
Subtyping provides a significantly more interesting soundness proof than the prior cases: we must justify that \bilambdaamor's two-step normalize-then-compare strategy is in fact sound for the declarative type system. The proof proceeds in two parts corresponding to the two judgments. We first prove soundness for the normal form subtyping, and then lift it, using the results about normalization from \autoref{sec:dlambdaamor-normalization}, to the full algorithmic subtyping relation.

\begin{restatable}[Soundness of Subtyping for Normal Forms]{theorem}{subtynfsound}
If $\Psi ; \Theta ; \Delta \pvdash \tau_1 \subtynf \tau_2 : K \gens \Phi$ and $\Theta ; \Delta \vDash \Phi$ then $\Psi ; \Theta ; \Delta \pvdash \tau_1 \subty \tau_2 : K$
\label{thm:subtynf-sound}
\end{restatable}

\begin{restatable}[Soundness of Subtyping]{theorem}{subtysound}
If $\Psi ; \Theta ; \Delta \pvdash \tau_1 \subty\tau_2 : K \gens \Phi$ and $\Theta ; \Delta \vDash \Phi$, then $\Psi ; \Theta ; \Delta \pvdash \tau_1 \subty \tau_2 : K$
\label{thm:subty-sound}
\end{restatable}
\begin{proof}
There is only one case: $\Psi ; \Theta ; \Delta \pvdash \tau_1 \subty\tau_2 : K \gens \Phi$ by way of $\Psi ; \Theta ; \Delta \vdash \texttt{eval}(\tau_1) \subtynf \texttt{eval}(\tau_2) : K \gens \Phi$ with $\Theta ; \Delta \vDash \Phi$. By \autoref{thm:subtynf-sound}, $\Psi ; \Theta ; \Delta \vdash \texttt{eval}(\tau_1) \subty \texttt{eval}(\tau_2) : K$. By \autoref{thm:norm-thm} and two uses of S-Trans, $\Psi ; \Theta ; \Delta \pvdash \tau_1 \subty \tau_2 : K$, as required.
\end{proof}

\subsubsection{Soundness of Typechecking}

As is to be expected, the soundness of the bidirectional type-checking judgment is the most involved. Before we can prove it, a few small lemmata are required.
First, we prove (as was noted before) that the output contexts of the I/O method in both typing judgments have a strong regularity condition: the output context is always a subset\footnote{
This containment is almost always strict: in fact, a corollary of \autoref{thm:tycheck-sound} is that the containment is strict unless the term is closed.
}
of the input context.

\begin{restatable}{theorem}{lsc}
~\begin{itemize}
  \item If $\Psi ; \Theta ; \Delta ; \Omega ; \Gamma \vdash e \checks \tau \gens \Phi, \Gamma'$ then $\Gamma' \subseteq \Gamma$
  \item If $\Psi ; \Theta ; \Delta ; \Omega ; \Gamma \vdash e \infers \tau \gens \Phi, \Gamma'$ then $\Gamma' \subseteq \Gamma$
\end{itemize}
\label{thm:lsc}
\end{restatable}

The next lemma is required for cases of \autoref{thm:tycheck-sound} where variables are bound in premises and subsequently removed in the conclusion. In essence, it proves compatibility between the set difference operator which removes variables from the output, and context extension.

\begin{restatable}{theorem}{tychecksoundlemma}
\label{thm:tycheck-sound-lemma}
If $\Gamma' \subseteq \Gamma$, then
$(\Gamma, x : \tau) \setminus \Gamma' \subseteq \Gamma \setminus (\Gamma' \setminus \{x : \tau\}), x : \tau$. Moreover, if:
~\begin{itemize}
  \item $\Theta \vdash \Delta \; \texttt{wf}$
  \item $\Psi ; \Theta ; \Delta \vdash \Gamma \; \texttt{wf}$
  \item $\Psi ; \Theta ; \Delta \vdash \tau : \star$
\end{itemize}
Then $\Psi ; \Theta ; \Delta \pvdash \Gamma \setminus (\Gamma' \setminus \{x : \tau\}), x : \tau \wknto (\Gamma, x : \tau) \setminus \Gamma'$.
\end{restatable}
\begin{proof}
The first part follows by an elementary set-theoretic containment proof, and the second is immediate by applying the presuppositions.
\end{proof}

Because the two judgments (checking and inference) are mutually inductively defined, we must prove each judgment's corresponding soundness theorem simultaneously. The theorem must also handle two as-of-yet untreated differences of \bilambdaamor and \dlambdaamor. First, the syntax of algorithmic terms is different from the declarative ones. To translate an algorithmic term to a declarative one, we rely on the erasure transformation from \autoref{sec:bilambdaamor-overview-bidir} to remove all type annotations from a term. Second, the use of the I/O method means we must incorporate the ``context-strengthening"-style completeness theorem from \autoref{sec:bilambdaamor-overview-io}. All together, we arrive at the following theorem.

\begin{restatable}[Soundness of Type Checking/Inference]{theorem}{tychecksound}
\label{thm:tycheck-sound}
~\begin{enumerate}
 \item If $\Psi;\Theta;\Delta;\Omega;\Gamma \pvdash e \checks \tau \gens \Phi, \Gamma'$ and $\Theta;\Delta \vDash \Phi$ then $\Psi;\Theta;\Delta;\Omega;\Gamma \setminus \Gamma' \pvdash |e| : \tau$
 \item If $\Psi;\Theta;\Delta;\Omega;\Gamma \pvdash e \infers \tau \gens \Phi, \Gamma'$ and $\Theta;\Delta \vDash \Phi$ then $\Psi;\Theta;\Delta;\Omega;\Gamma \setminus \Gamma' \pvdash |e| : \tau$
\end{enumerate}
\end{restatable}

The proof of \autoref{thm:tycheck-sound} is not particularly enlightening: we prove both claims simultaneously by induction on the judgment premises, liberally applying \autoref{thm:tycheck-sound-lemma} when binders are used.


\subsubsection{Completeness of Sorts, Constraints, Contexts, and Kinds}
Perhaps expectedly, the four basic judgments admit very simple completeness proofs. Similarly to their soundness proofs, these are all proved by single-pass inductions on derivations. Again, these proofs are split into two parts to untie the knot: we first prove completeness of ``raw" judgments, and then repackage the theorems with presuppositions after all of the raw theorems have been proven.


\begin{restatable}{theorem}{sortcompl}
If $\Theta;\Delta \pvdash I : S$, then $\Theta;\Delta \pvdash I : S \gens \Phi$ and $\Theta;\Delta \vDash \Phi$.
\label{thm:sort-compl}
\end{restatable}

\begin{restatable}{theorem}{idxctxwfcompl}
If $\Theta \vdash \Delta \; \texttt{wf}$ then $\Theta \vdash \Delta \; \texttt{wf} \gens \Phi$ with $\Theta ; \cdot \vDash \Phi$
\label{thm:idx-ctx-wf-compl}
\end{restatable}

\begin{restatable}{theorem}{constrcompl}
If $\Theta ; \Delta \pvdash \Phi \; \texttt{wf}$, then $\Theta ; \Delta \pvdash \Phi \; \texttt{wf} \gens \Phi'$ with $\Theta ; \Delta \vDash \Phi'$
\label{thm:constr-compl}
\end{restatable}

\begin{restatable}{theorem}{kindcompl}
If $\Psi;\Theta;\Delta \pvdash \tau : K$, then $\Psi;\Theta;\Delta \pvdash \tau : K \gens \Phi$ with $\Theta ; \Delta \vDash \Phi$
\label{thm:kind-compl}
\end{restatable}

\subsubsection{Completeness of Subtyping}
The proof that \bilambdaamor's subtyping is complete is perhaps the most exciting proof we will see. As has been discussed numerous times, \dlambdaamor's inclusion of index term-indexed types means that proving the algorithmic subtyping complete is tantamount to deciding $\beta$-equivalence of a limited lambda calculus. It may be tempting\footnote{
I certainly was tempted.
} to attempt to split the proof of completeness of subtyping into two statements: one could attempt to first prove that the algorithmic normal form subtyping relation is complete for types in normal form: i.e. that if $\tau_1 \subty \tau_2$ in \dlambdaamor with $\tau_1,\tau_2 \, \texttt{nf}$, then there is some solvable $\Phi$ such that $\tau_1 \subtynf \tau_2 \gens \Phi$. Unfortunately, this is not easily provable: if the premise is a use of transitivity, the cut type may not be in normal form, and thus the inductive hypothesis cannot be applied.

The actual proof of completeness of algorithmic subtyping relies on two key admissibility theorems, namely of reflexivity and transitivity. Since \dlambdaamor's subtyping includes the rules S-Refl and S-Trans but \bilambdaamor's doesn't include analogues of these (for the purposes of syntax-directedness), the algorithmic subtyping must be able to emulate these rules whenever they occur in a declarative derivation. In both cases, the proof proceeds by proving the statement for normal forms, and then lifting the result to the full algorithmic relation through normalization.

\begin{restatable}[Reflexivity of Algorithmic Subtyping for Neutral Forms]{theorem}{subtynerefl}
If $\Psi ; \Theta ;  \Delta \pvdash \tau : K$ and $\tau \;\texttt{ne}$, then $\Psi ; \Theta ;  \Delta \pvdash \tau\subtynf \tau : K \gens \Phi$ with $\Theta ; \Delta \vDash \Phi$
\label{thm:subtyne-refl}
\end{restatable}

\begin{restatable}[Reflexivity of Algorithmic Subtyping for Normal Forms]{theorem}{subtynfrefl}
If $\Psi ; \Theta ;  \Delta \pvdash \tau : K$ and $\tau \;\texttt{nf}$, then $\Psi ; \Theta ;  \Delta \pvdash \tau\subtynf \tau : K \gens \Phi$ with $\Theta ; \Delta \vDash \Phi$
\label{thm:subtynf-refl}
\end{restatable}

\begin{restatable}[Reflexivity of Algorithmic Subtyping]{theorem}{subtyrefl}
If $\Psi ; \Theta ;  \Delta \pvdash \tau : K$ then $\Psi ; \Theta ;  \Delta \pvdash \tau\subty \tau : K \gens \Phi$ with $\Theta ; \Delta \vDash \Phi$
\label{thm:subty-refl}
\end{restatable}
\begin{proof}
By AS-Normalize, it suffices to show that $\Psi ; \Theta ;  \Delta \vdash \texttt{eval}(\tau) \subtynf \texttt{eval}(\tau) : K \gens \Phi$. By \autoref{thm:norm-thm}, we have that $\texttt{eval}(\tau) \; \texttt{nf}$, and $\Psi ; \Theta ; \Delta \pvdash \texttt{eval}(\tau) : K$, By \autoref{thm:subtynf-refl}, we have $\Psi ; \Theta ;  \Delta \vdash \texttt{eval}(\tau) \subtynf \texttt{eval}(\tau) : K \gens \Phi$ and $\Theta ; \Delta \vDash \Phi$, as required. 
\end{proof}

\begin{restatable}[Transitivity of Algorithmic Subtyping for Normal Forms]{theorem}{subtynftrans}
If $\Psi ; \Theta ; \Delta \pvdash \tau_1 \subtynf \tau_2 : K \gens \Phi_1$ and $\Psi ; \Theta ; \Delta \pvdash \tau_2 \subtynf \tau_3 : K \gens \Phi_2$ with $\Theta ; \Delta \vDash \Phi_1 \wedge \Phi_2$, then $\Psi ; \Theta ; \Delta \vdash \tau_1 \subtynf \tau_3 : K \gens \Phi$ such that $\Theta ; \Delta \vDash \Phi$.
\label{thm:subtynf-trans}
\end{restatable}

\begin{restatable}[Transitivity of Algorithmic Subtyping]{theorem}{subtytrans}
\label{thm:subty-trans}
If $\Psi ; \Theta ; \Delta \pvdash \tau_1 \subty \tau_2 : K \gens \Phi_1$ and $\Psi ; \Theta ; \Delta \pvdash \tau_2 \subty \tau_3 : K \gens \Phi_2$ with $\Theta ; \Delta \vDash \Phi_1 \wedge \Phi_2$, then $\Psi ; \Theta ; \Delta \pvdash \tau_1 \subty \tau_3 : K \gens \Phi$ and $\Theta ; \Delta \vDash \Phi$
\end{restatable}
%\textbf{Rewrite in new style}
\begin{proof}
By inversion, $\Psi ; \Theta ; \Delta \vdash \texttt{eval}(\tau_1) \subtynf \texttt{eval}(\tau_2) : K \gens \Phi_1$ and $\Psi ; \Theta ; \Delta \vdash \texttt{eval}(\tau_2) \subtynf \texttt{eval}(\tau_3) : K \gens \Phi_1$ By \autoref{thm:norm-thm} and \autoref{thm:subtynf-trans}, 
$\Psi ; \Theta ; \Delta \vdash \texttt{eval}(\tau_1) \subtynf \texttt{eval}(\tau_3) : K \gens \Phi$ with $\Theta ; \Delta \vDash \Phi$. Then, by AS-Normalize,
$\Psi ; \Theta ; \Delta \vdash \tau_1 \subty \tau_3 : K \gens \Phi$, as required.
\end{proof}

The following theorem is essentially a subtyping version of \autoref{thm:idx-subst-nf}: not only does index term substitution preserve the property that types are in normal form, it also preserves all subtyping relations. This theorem depends on a series of theorems that index-term substitution is admissible for sort assignment, kind assignment, and constraint well-formedness. These are all proved (in \autoref{appendix:a}) by appealing to the corresponding substitution theorem in \dlambdaamor, and taking a round trip through soundness and completeness for the judgment in question.

\begin{restatable}[Admissibility of Normal Form Subtyping Substitution]{theorem}{subtynfidxsubst}
\label{thm:subtynf-idx-subst}
~Suppose the following:
 \begin{itemize}
   \item $\Psi ; \Theta, i : S ; \Delta \pvdash \tau_1 \subtynf \tau_2 : K \gens \Phi$ with $\Theta ; \Delta \vDash \Phi$ and $\Theta \vdash \Delta \; \texttt{wf}$.
   \item $\Theta ; \Delta \pvdash I : S \gens \Phi_1$ with $\Theta ; \Delta \vDash \Phi_1$
   \item $\Theta ; \Delta \pvdash J : S \gens \Phi_2$ with $\Theta ; \Delta \vDash \Phi_2$ 
   \item $\Theta ; \Delta \vDash I = J$
 \end{itemize}
 Then, $\Psi ; \Theta ; \Delta \pvdash \tau_1[I/i] \subtynf \tau_2[J/i] : K \gens \Phi'$ for some $\Phi'$ with $\Theta ; \Delta \vDash \Phi'$.
\end{restatable}

A corollary for the above admissibility theorem is that type evaluation essentially commutes with type family application. This theorem is pivotal for proving the AS-FamApp case of \autoref{thm:subty-compl} below.

\begin{restatable}[Type Family Application Commutes with Evaluation]{theorem}{evalapplemma}
If $\Psi ; \Theta ; \Delta \pvdash \texttt{eval}(\tau_1) \subtynf \texttt{eval}(\tau_2) : S \to K \gens \Phi$ and $\Theta ; \Delta \vDash \Phi \wedge I = J$ with $\Theta ; \Delta \pvdash I : S$ and $\Theta ; \Delta \pvdash J : S$ then 
$\Psi ; \Theta ; \Delta \pvdash \texttt{eval}(\tau_1 \; I) \subtynf \texttt{eval}(\tau_2 \; J) : K \gens \Phi'$ for some $\Theta ; \Delta \vDash \Phi'$.
\label{thm:eval-app-lemma}
\end{restatable}
\begin{proof}
By inversion on $\Psi ; \Theta ; \Delta \vdash \texttt{eval}(\tau_1) \subtynf \texttt{eval}(\tau_2) : S \to K \gens \Phi$.
\begin{itemize}
  \item For the first case, suppose the derivation was $\Psi ; \Theta ; \Delta \vdash \lambda i : S. \tau_1' \subtynf \tau_2' : S \to K \gens \Phi$
  from $\Psi ; \Theta, i : S ; \Delta \vdash \tau_1' \subtynf \tau_2' : K \gens \Phi'$. By \autoref{thm:subtynf-idx-subst},
  $\Psi ; \Theta ; \Delta \pvdash \tau_1'[I/i] \subtynf \tau_2'[J/i] : K \gens \Phi'$, for some $\Theta ; \Delta \vDash \Phi'$. But $\texttt{eval}(\tau_1 \; I) = \tau_1'[I/i]$ and $\texttt{eval}(\tau_2 \; J) = \tau_2'[J/i]$.
  \item Now, suppose the derivation was $\Psi ; \Theta ; \Delta \vdash \tau_1' \; L_1 \subtynf \tau_2' \; L_2 : S \to K \gens \Phi \wedge (L_1 = L_2)$, where $\texttt{eval}(\tau_1) = \tau_1' \; L_1$ and $\texttt{eval}(\tau_2) = \tau_2 \; L_2$. These must both be $\texttt{ne}$, since they are both applications, and therefore
  $\texttt{eval}(\tau_1) \; I = \texttt{eval}(\tau_1 \; I)$ and $\texttt{eval}(\tau_2) \; J = \texttt{eval}(\tau_2 \; J)$, as required.
\end{itemize}
\end{proof}

Finally, we prove the full completeness of algorithmic subtyping. The proof proceeds by a single induction on the hypothesis. The reflexivity and transitivity cases are handled by Theorems \ref{thm:subty-refl} and \ref{thm:subty-trans}. %We present the most interesting case (for S-Fam-Beta1) below, and leave the remaining cases for \autoref{appendix:a}.

\begin{restatable}[Completeness of Algorithmic Subtyping]{theorem}{subtycompl}
If $\Psi ; \Theta ; \Delta \pvdash \tau_1 \subty \tau_2 : K$ then  $\Psi ; \Theta ; \Delta \pvdash \tau_1 \subty \tau_2 : K \gens \Phi$ and $\Theta ; \Delta \vDash \Phi$.
\label{thm:subty-compl}
\end{restatable}
%\begin{proof}
%~\begin{itemize}
%   \item[(S-Fam-Beta1)] Suppose $\Psi ; \Theta ; \Delta \vdash (\lambda i : S. \tau) \; J \subty \tau[J/i] : K$. By AS-Normalize, it suffices to show that
%   $\Psi ; \Theta ; \Delta \vdash \texttt{eval}((\lambda i : S. \tau) \; J) \subtynf \texttt{eval}(\tau[J/i]) : K \gens \Phi$ and $\Theta ; \Delta \vDash \Phi$.
%   But, $\texttt{eval}((\lambda i : S. \tau) \; J) = \texttt{eval}(\tau)[J/i]$ by definition, and $\texttt{eval}(\tau[J/i]) = \texttt{eval}(\tau)[J/i]$ by \autoref{thm:idx-subst-eval}. By \autoref{thm:norm-thm},  $\Psi ; \Theta ; \Delta \vdash \texttt{eval}(\tau)[J/i] : K$, and so by \autoref{thm:subtynf-refl}, we have that $\Psi ; \Theta ; \Delta \vdash \texttt{eval}(\tau)[J/i] \subtynf \texttt{eval}(\tau)[J/i] : K \gens \Phi$ with $\Theta ; \Delta \vDash \Phi$ as required.
% \end{itemize}
%\end{proof}

\subsubsection{Completeness of Typechecking}

Finally, we arrive at the completeness of \bilambdaamor's typechecking algorithm. To begin, we must prove the admissibility of \dlambdaamor's weakening rule (T-Weaken) in \bilambdaamor. This requires a fairly sizable and involved simultaneous induction on the checking and inference judgments, which must account for all of the bells and whistles of the bidirectional typechecking with constraints and I/O contexts. The theorem is best understood as a \bilambdaamor -specific version of the mock I/O weakening theorem from \autoref{sec:bilambdaamor-overview-io}. When we weaken the affine context, the added variables flow through, and remain unused.

%\red{(How the hell do I make this readable??)}

\begin{restatable}[Admissibility of Algorithmic Weakening]{theorem}{admitsweaken}
\label{thm:admits-weaken}
~\begin{enumerate}
  \item If $\Psi ; \Theta ; \Delta ; \Omega ; \Gamma \pvdash e \checks \tau \gens \Phi,\Gamma''$ with $\Theta ; \Delta \vDash \Phi$, then whenever $\Psi ; \Theta ; \Delta \pvdash \Gamma' \wknto \Gamma$ and $\Psi ; \Theta ; \Delta \pvdash \Omega' \wknto \Omega$, there are $\Phi_1$, $e_1$, $\Gamma_1$ so that $|e_1| = |e|$, $\Theta ; \Delta \vDash \Phi_1$, $\Psi ; \Theta ; \Delta \pvdash \Gamma_1 \wknto \Gamma' \setminus \Gamma$, $\Psi ; \Theta ; \Delta \pvdash \Gamma_1 \wknto \Gamma''$, and $\Psi ; \Theta ; \Delta ; \Omega' ; \Gamma' \pvdash e_1 \checks \tau \gens \Phi_1,\Gamma_1$.
  \item If $\Psi ; \Theta ; \Delta ; \Omega ; \Gamma \pvdash e \infers \tau \gens \Phi,\Gamma''$ with $\Theta ; \Delta \vDash \Phi$, then whenever $\Psi ; \Theta ; \Delta \pvdash \Gamma' \wknto \Gamma$ and $\Psi ; \Theta ; \Delta \pvdash \Omega' \wknto \Omega$, there are $\Phi_2$, $e_2$, $\Gamma_2$ so that $|e_2| = |e|$, $\Theta ; \Delta \vDash \Phi_2$, $\Psi ; \Theta ; \Delta \pvdash \Gamma_2 \wknto \Gamma' \setminus \Gamma$, $\Psi ; \Theta ; \Delta \pvdash \Gamma_2 \wknto \Gamma''$ and $\Psi ; \Theta ; \Delta ; \Omega' ; \Gamma' \pvdash e_2 \infers \tau \gens \Phi_2,\Gamma_2$.
\end{enumerate}
\end{restatable}

The actual statement of completeness is easily understandable. For any declarative type assignment, we can always annotate the term with types so that it can either check or infer, while outputting a valid constraint. We note that it is not strictly necessary to prove the theorem in this form-- the careful reader may have noticed that (2) is implied by (1) and a single use of AT-Anno. This method is in fact the traditional way of proving bidirectional completeness. However, the algorithm its proof encodes inserts many unnecessary annotations: any term in inference position will be annotated, even if the term is already a syntactic form whose rule has inferring conclusion. However, by providing an inductive hypothesis for an inference judgment at every stage, we give terms which may infer their own types the opportunity to do so. For this reason, the algorithm which the proof of completeness encodes inserts far fewer annotations than the traditional one.

\begin{restatable}[Completeness of Type Checking/Inference]{theorem}{tycheckcompl}
\label{thm:tycheck-compl}
If $\Psi;\Theta;\Delta;\Omega;\Gamma \pvdash e : \tau$, then:
\begin{enumerate}
  \item There are $e'$, $\Phi'$, $\Gamma'$ such that $|e'| = e$, $\Theta ; \Delta \vDash \Phi'$, and $\Psi;\Theta;\Delta;\Omega;\Gamma \pvdash e' \checks \tau \gens \Phi', \Gamma'$.
  \item There are $e''$, $\Phi''$, $\Gamma''$ such that $|e''| = e$, $\Theta ; \Delta \vDash \Phi''$, and $\Psi;\Theta;\Delta;\Omega;\Gamma \pvdash e'' \infers \tau \gens \Phi'',\Gamma''$
\end{enumerate}
\end{restatable}


\section{Implementation of \lambdaamor}
\label{sec:lambdaamor-impl}
To exhibit the practical use of \dlambdaamor, we present an implementation of \bilambdaamor, which we will simply refer to as \lambdaamorimpl. The implementation consists of approximately 16,000 lines of OCaml, and is freely available at the URL below, under a BSD license.
\begin{center}
\url{https://gitlab.mpi-sws.org/dg/amortized-analysis-types-impl}
\end{center}
Functionally, the implementation sports a typechecker and interpreter for \dlambdaamor, as well as a command-line interface for interactive use.

We begin the section by discussing the format of programs in \lambdaamorimpl. To use \lambdaamorimpl as a programming language, we require some language features other than the type-checking and evaluation of single expressions, as modeled by \bilambdaamor. To this end, we introduce four top-level declaration forms which can be composed to form programs in \lambdaamorimpl. Next, we discuss the artifact itself in some depth: giving an overview of the project's structure, elaborating on design decisions, and remarking on a few places where the implementation departs from the theory. Finally, we present an experimental evaluation of the tool, and compare it with existing resource-aware languages.
 
 %Next, we show \lambdaamorimpl in action, by typechecking and evaluating the examples from Section~\textbf{??}, as well as exhibiting some of the different modes of use and interaction we envision for languages like this one. We also exhibit some of the ergonomic features available in \lambdaamorimpl which are not present in the core theory.
 

\subsection{Declarations and Structure of \lambdaamorimpl Programs}
\begin{figure}
\input{figs/lambdaamorimmpl-decls}
\caption{Declarations in \lambdaamorimpl}
\label{fig:lambdaamorimmpl-decls}
\end{figure}

Programs in \lambdaamorimpl are lists of \textit{declarations}, which can be any of four forms: let-bound definitions, type and index term aliases, and \texttt{do}-declarations. The syntax for each can be seen in \autoref{fig:lambdaamorimmpl-decls}. These four declaration types are allow programmers to ergonomically write interesting programs by composing them from smaller ones, and building up abstractions. All four declarations should be understood as simply exposing existing judgmental structure from \dlambdaamor to the programmer, and as such they do not add or subtract from the expressive power of the language.

Index term and type aliases are somewhat self-explanatory: a programmer may give names to types and index terms they wish to use later. Since types in \lambdaamorimpl can get quite complex, liberal use of type aliases is often very helpful. Importantly, both index term and type aliases may be of higher sort and kind, respectively, and so a user can give names to type families and index functions, too.

\texttt{do}-declarations are reminiscent of top-level interaction in Haskell's GHCI interpreter. In GHCI, computations in the IO monad entered at top level are not only evaluated, but forced for effect. The \texttt{do} of \lambdaamorimpl serves a similar role by allowing monadic computations which have previously been built up to be run, and have their \textit{actual} run-time costs computed. The reader may find it helpful to think of \texttt{do}-declarations as being a \texttt{bind} into an ambient cost-monadic context, again in a manner similar to GHCI. In order to preserve soundness of costs, the only computations that may be run at the top level are those that return \textit{observable} types-- i.e. those that are built up from base types, lists, sums, products, and exponentials.

Let-bindings in \lambdaamorimpl behave just like top-level bindings in any other functional language. The only twist is that all top-level let-bound variables are required to be exponential variables, and hence cannot use any affine variables bound by \texttt{do}-declarations.


\subsection{Overview of Phases}
\begin{figure}
\input{figs/lambdaamorimpl-file-structure}
\caption{File Structure of \lambdaamorimpl}
\label{fig:lambdaamorimpl-file-structure}
\end{figure}
A program to be run by \lambdaamorimpl follows a straightforward path. It is first lexed and parsed from its textual form into an abstract syntax representation. This abstract syntax is then passed to the typechecker, which closely follows the algorithmic approach prescribed by \bilambdaamor. This typechecking emits constraints, which are then passed to an SMT solver using the Why3 prover frontend \cite{boogie11why3}. If the constraints come back valid, the program can then be passed to the built-in definitional interpreter \cite{reynolds:acm72}, which runs the program according to the cost semantics of \citet{rajani-et-al:popl21}.

For the remainder of this section, we will take a tour through the implementation and design choices of each of the phases of the language's execution. Each of these roughly corresponds to a single module in the \lambdaamorimpl source, so a table of files along with their descriptions can be found in \autoref{fig:lambdaamorimpl-file-structure}. Finally, some of the structure of the implementation is borrowed and inspired from previous developments in resource-aware and bidirectional type systems, and so we are careful to flag our predecessors for each pass.


\subsubsection{Lexing and Parsing}
\lambdaamorimpl uses off-the-shelf OCaml lexer and parser generators, \texttt{ocamllex} and \texttt{ocamlyacc}. While not the most performant options, these do fine for our purposes. The syntax of \lambdaamorimpl was carefully chosen to resemble the syntax of \bilambdaamor as much as possible while retaining an unambiguous grammar, and while ensuring that users need not type unicode symbols.

While the language syntax is closely modeled on that of \bilambdaamor, it must be extended to support the top-level features introduced for ease of use in \lambdaamorimpl. The only change to the term syntax is the introduction of a syntax (wildcards/underscores) for typed holes in \lambdaamorimpl, in the style of OLEG \cite{mcbride:thesis}, Agda\cite{norell:afp08}, or Haskell, which allow a programmer to typecheck partial programs, and be informed about what types the checker expects to fill the holes.

The lexer and parser are specified in \texttt{src/lexer.mll}, respectively \texttt{src/parser.mly}. The parser emits an abstract syntax representation of a program-- the type of these syntax trees, as well as all of the abstract syntax of the language, is found in \texttt{src/syntax.ml}.


\subsubsection{AST Freshening Pass}
Since this development includes no mechanized metatheory, all variables in \lambdaamorimpl are represented as strings for simplicity. This poses complications for the substitution operation, as one would need to $\alpha$-convert variables at each substitution instance. To resolve this, the AST of a program is fed to a ``freshener" immediately after parsing, which $\alpha$-converts all terms so that every bound variable is globally unique\footnote{This pass is modeled off of a similar one from the Granule language \cite{orchard-et-al:popl19}.}. A single freshening pass is sufficient to eliminate the possibility of variable capture in closed terms. When handling open terms, such as those that include index and type aliases, we always re-freshen index terms and types before they are substituted in to prevent capture.

\subsubsection{Normalization}
Thanks to the theoretical simplicity of \lambdaamor's type normalization algorithm, \lambdaamorimpl's implementation of it is similarly simple: the code (in \texttt{src/normalize.ml}) is only $\approx 150$ lines. The procedure works in two passes, first by evaluating object-language types into a meta-language type of types \textit{in normal form}, and then quoting back. In practice, most types in \lambdaamor programs are in normal form. To avoid unnecessarily normalizing types, we tag types with a status bit which is set when the type is in normal form.

\subsubsection{Bidirectional Type Checking}
The core typechecking algorithm of \lambdaamorimpl is very faithful to the core algorithmic calculus presented in \autoref{sec:bilambdaamor-syntax-and-types}. Search functions for each of the four user-facing judgments (sort-checking, kind-checking, subtyping, typechecking) are implemented in the file \texttt{src/tycheck.ml}. The sort-checking and kind-checking judgments both operate on fully annotated terms. For this reason, we implement full inference and checking for both: with sort/kind as output and input, respectively. Subtyping is implemented as expected: both types are normalized, and then passed to a helper function which decides the normal form subtyping relation of \bilambdaamor. Finally, the main pair of type checking and inference judgments are implemented in the usual bidirectional style as a pair of mutually recursive functions. All of these functions, in addition to their usual return types (unit for checking functions, sort/kind/type for inference functions) also return the constraints output by their corresponding judgments, to be passed to the solver.

To simplify the lives of programmers, we do deviate slightly from the core calculus in a few places. First, the type checking and inference judgments include a few ``parallel rules": instances where the bidirectional rule has a checking or inference conclusion, but we also include a case for the other mode. While not strictly required for completeness, these extra rules can make programming more ergonomic. Next, we always normalize the output of the type inference function: this is helpful in cases where the type of a term inferred in an elimination position has a $\beta$-redex as its head, and not the expected connective. This is also clearly still sound, as it can be emulated by adding an annotation in the requisite elimination position. Finally, we note the behavior of typed holes in \lambdaamorimpl. This feature is a practical necessity in languages with type systems as complex as ours. Fortunately, the bidirectional framework makes them simple to implement: when the type checking or inference judgments hit the hole, checking is halted, and the expected type of the hole (in the case of a checking judgment) as well as the current context is printed for the user.

To simplify some of the boilerplate involved in implementing the functions corresponding to each algorithmic judgment, we introduce a monadic discipline inspired by the implementation of BiRelCost \cite{cicek-et-al:pldi19}. We use a combined state/error monad called \texttt{'a checker} to simultaneously handle the four fully structural contexts and the substructural one via the I/O method (hence state, not reader), as well as managing type errors. OCaml's \texttt{let*} syntax allows us to cleanly write the typechecker in a manner similar to \texttt{do}-notation in Haskell, while a preponderance of useful monadic combinators lifted from BiRelCost make for very readable code.

Of course, the typechecker must not only handle the core term calculus, but also the top-level declaration features. Because of the inclusion of type and index term aliases, the state part of the \texttt{checker} monad must also include a type and index term environment which binds aliases to their values, on top of the existing type contexts. The top-level declarations require more choices to be made. 

Top level term declarations $\texttt{let x : t = e}$ are implicitly typed as exponential terms-- we erase the affine context before checking them, and the variable \texttt{x} given type \texttt{t} in the exponential context $\Omega$. This allows functions declared at the top level to be used many times, instead of just once, which is the intended pattern of use for a top-level definition.

The only terms which are bound in the affine context at top level are variables resulting from the \texttt{do} declarations, which are checked to have monadic type. Since the result of a monadic computation can store potential, the result of a \texttt{do} declaration must not be duplicated.

\subsubsection{Constraint Elaboration}
The language of \dlambdaamor's constraints includes equations and inequalities over potential vectors (index terms of sort $\potvec$). While many solvers allow us to define our own theory to handle this nonstandard type, it is instead  preferable to instead appeal to built-in (and highly optimized) real arithmetic theories. For this purpose, all constraints output from the typechecker are elaborated to transform equalities over potential vectors to componentwise equalities over reals.

This works in three phases. First, the length of the potential vectors ($k \geq 2$) is determined. This can be passed as a command-line parameter of \lambdaamorimpl, or computed as the largest potential vector appearing in the program. Then, all potential vector constants are padded with zeroes up to length $k$, while quantifiers over potential vector index variables $i : \vec{\R^+}$ are replaced by variables $i_n : \R$, for $0 \leq n < k$. Finally, index terms and (in)equalities over potential vectors are ``flattened" to operate componentwise,

This elaboration only works because the language of index terms is sufficiently first-order-- if one added truly higher-order index terms, the elaboration would be significantly more complex. The implementation of this transformation is quite simple, and can be found in \texttt{src/constr_elab.ml}.

\subsubsection{Constraint Solving}
The actual constraint solving of \lambdaamorimpl is handled by the Why3 platform \cite{boogie11why3}. Why3 is a unified frontend for a number of SMT solvers, which allows the user to switch between the proovers of their choosing. After the constraint elaboration phase, the constraints are translated into a format understandable by Why3 using its OCaml API. We then interface with the prover by building a Why3 proof goal for each constraint emitted by the typechecker. This set of proof goals is then checked in sequence by the prover, and the results are reported to the user.

\subsubsection{Interpreter}
Finally, a type-correct program can be interpreted. The interpreter included with \lambdaamorimpl is a straightforward definitional implementaion of the big-step cost-indexed operational semantics of \dlambdaamor: nothing too fancy. When the interpreter is invoked, all of the \texttt{do} declarations are run. The cost semantics tallies up the total actual cost of running a single declaration, and presents it, along with the statically predictied cost to the user. By the soundness theorem of \dlambdaamor, the predicted cost will always be an upper bound on the actual cost.

\subsection{Examples and Experimental Evaluation}
We now return to the examples presented in \autoref{sec:dlambdaamor-examples}. All of these programs (and more) have been implemented in \lambdaamorimpl: the source code for each can be found in the \texttt{.la} files in the \texttt{examples/} directory of the artifact repository. Below, we discuss each of the examples presented previously, as well as some new ones included with \lambdaamorimpl. The examples were chosen to test a variety of \lambdaamorimpl's features, and show a breadth of the kinds of analyses that can be performed using \lambdaamorimpl.

\begin{itemize}
  \item \texttt{examples/addone.la}: As presented in \autoref{sec:dlambdaamor-examples}. This program is quite simple, only making use of the RAML-style potentials and a single shift. We include this in the test suite purely as a coherence check.
  \item \texttt{examples/ins_sort.la}: As presented in \autoref{sec:dlambdaamor-examples}. Insertion sort has quadratic complexity, so the inclusion of this example demonstrates how \lambdaamorimpl can pass linear constraints to a solver when analyzing functions with degree-2 or higher cost behavior. The file also includes an example of a \texttt{do}-declaration, where the \texttt{ins_sort} function is run to sort a list.
  \item \texttt{examples/queue.la}: As presented in \autoref{sec:dlambdaamor-examples}. The functional queue is the first example of amortized analysis, which makes nontrivial use of potentials combined with costs.
  \item \texttt{examples/binary.la}: The analysis of a binary counter is a classic example of an amortized analysis, and the running example of \autoref{ch:rec-extr}. This uses similar techniques to the functional queue (RAML-style potentials, refinements)
  \item \texttt{examples/map.la}: As presented in \autoref{sec:dlambdaamor-examples}. The cost-polymorphic map is an example of a function which cannot be typed in RAML \cite{hoffmann-et-al:cav12}, the best-established resource-aware language, due to its complex use of higher order quantifiers over cost families. These quantifiers are reflected in the constraints which are emitted, which provide the first nontrivial goal for the solver backend.
  \item \texttt{examples/foldr.la}: The analysis and type of \texttt{foldr} can be expressed in a similar manner to that of \texttt{map}. We include this example primarily because it is discussed in depth in the original \lambdaamor paper \cite{rajani-et-al:popl21}
  \item \texttt{examples/church.la}: Our final example is the assignment of a \textit{very} complex type to church numerals. In short, we generalize the type of the ``iterate" function $!(\tau \loli \tau) \loli \N \loli \tau \loli \tau$ to operate over a type family $\alpha : \N \to \star$ and a cost family $C : \N \to \R^+$ to get the much more precise type:
  $$
   \forall \alpha : \N \ \to \star. \forall C : \N \to \R^+. \left(\forall i. \alpha \, i \loli \M \, \langle C \, i \rangle \, \left(\alpha \, (i+1)\right)\right) \loli \texttt{Nat}(n) \loli \alpha \, 0 \loli \M \, \langle \sum C\, i\rangle \, \left(\alpha n\right)
  $$
  Further details can be found in \citet{rajani-et-al:popl21}. The file includes two operations defined on these church numerals, namely successor and addition.
\end{itemize}

The type-checking times for all of these examples can be found in \autoref{fig:lambdaamorimpl-eval}. Total time denotes the end-to-end running time, from parsing the input file all the way through running the program (when applicable). Solving time refers to the amount of time spent by the prover in solving the constraints, while checking time refers to the time taken by the type checker itself, as well as the constraint elaboration phase. Each statistic is an average over ten runs. In all cases, the execution time not spent constraint solving is negligible-- the SMT solver is by far the largest bottleneck. However, the total time remains low, even for programs like \texttt{church} which emit very large high-order constraints. Finally, the total time not accounted for by checking and solving can be atributed to Why3 initialization and file IO. 

The evaluation was performed on a 2018 MacBook Pro running OSX 10.13.6 with a 2.3GHz Intel Core i5 processor and 8GB of memory. The artifact itself is written in OCaml, and was compiled using OCaml v4.09.1, along with its included versions of OCamllex and OCamlyacc. Library dependencies can be found in the \texttt{dune} file. Constraints emitted from the typechecker were solved by Why3 v1.3.1 \cite{boogie11why3}, using Z3 v4.8.5 \cite{de2008z3} as a prover.

\begin{figure}
\input{figs/lambdaamorimpl-eval}
\caption{Benchmarks of \lambdaamorimpl}
\label{fig:lambdaamorimpl-eval}
\end{figure}

\section{Related Work}
\label{sec:lambdaamor-related-work}
While still greatly under-researched, the prospect formally verifying cost bounds of programs is sufficiently intriguing that researchers have been chipping away at the problem for decades. While \lambdaamorimpl takes a specific intrinsic approach to verifying amortized cost, many other approaches exist.

\subsubsection{Program Logics and Static Analyses}
As program logics are commonly used to reason about and verify program correctness, it is natural to consider their utility in verifying costs of programs, as well! Some recent work in this line includes that of \citet{carbonneaux-et-al:pldi15}, who develop a Hoare logic with quantative reasoning capabilities. Moreover, modern logics like separation logic can used to emulate some kinds of amortized complexity analysis \cite{chargueraud-et-al:jar19, mevel-et-al:esop19}. \citet{cicek-et-al:pldi19} use a \textit{relational} program logic to derive bounds on the cost difference between two programs of the same type. Finally, \citet{li2021reasoning} develop a dual pair of program logics \cite{ohearn:popl20} for reasoning about the cost of lazy functional programs. A related (usually more automated) technique for verifying cost properties of programs is static analysis. The COSTA project
\citep{albert-et-al:jar11,albert-et-al:tcs12:cost-analysis,albert-et-al:tocl13:inference} has used this to great effect in their work on automatically verifying cost usage of Java bytecode programs. While all of these papers do a good job of achieving their goals, the goals are very different from ours. In particular, they all take an extrinsic point of view on cost verification: a program and the certificate of its cost are separate.

\subsubsection{Proof Assistant Libraries}
Some work has studied ways of extending existing proof assistants to reasoning about cost, or building libraries which emulate such features. \citet{danielsson:popl08} presents an Agda library for semiformal cost analysis of purely functional data structures. This work is based on an indexed cost monad similar to that of \lambdaamorimpl, but does not include types for handling potential. Interestingly, the framework handles cost analysis of laziness, which we do not. In the Coq side of the world, \citet{mccarthy-et-al:flops16} have developed a similar library based on a similar monadic discipline, but this time for by-value evaluation. Both of these works are similar in spirit to \lambdaamorimpl, as they provide intrinsic cost reasoning abilities in a richly-typed framework. However, both build this ability on top of an existing proof assistant, whose logic does not have a first-class notion of cost or potential. %This limits the amount of automation and user support that the libraries can provide, since they are not operating at the level of the proof assistant itself. 
% - Danielsson
%  - Another library
%Theorem proving cost analysis:
% - mccarthy-et-al:flops16
%  - Coq monad for cost analysis (not first class)
% That one thesis about cost analysis in coq?% - LYA
%   - Kinda like LRT

\subsubsection{Languages and Proof Assistants with Resource-Aware Type Systems}
The works which bear the most resemblance to \lambdaamorimpl are those which develop a full language with a resource-aware type systems from scratch. These projects all fall along a spectrum, from languages with fully automated cost bound inference which on the surface behave much like languages with built-in static analyses, to resource-aware richly-typed proof languages in the style of\lambdaamorimpl.

A classic example of the first type is Resource Aware ML \cite{hoffmann-et-al:cav12}, an implementation of Automated Amortized Resource Analysis \cite{hofmannjost03aara}, from which \lambdaamor borrows its univariate polynomial potentials. While RAML provides highly-automated cost-bound inference, it struggles to handle some moderately complex language features like variable-cost higher-order functions. \lambdaamorimpl is at least as expressive as AARA, since AARA embeds into \lambdaamor \cite{rajani-et-al:popl21}. Moreover, \lambdaamorimpl can verify cost-polymorphic functions such as \texttt{map} (\autoref{sec:dlambdaamor-examples}).
Further along the spectrum is a language like TiML \cite{wang-et-al:oopsla17:timl}, in which the programmer annotates regular ML functions with statically-checked cost bounds. The cost verification techniques involved are somewhat brittle. For instance,  TiML includes a set of pattern-based heuristics for solving recurrences, and generates nonlinear constraints when handling polynomial potentials. TiML is also not sound for amortized analysis, as its type system is not affine. Both RAML and TiML can only type terminating functions, while \lambdaamorimpl includes general (polymorphic) recursion. The closest-in-spirit language to \lambdaamorimpl is Liquid Resource Types (LRT) \cite{koth-et-al:icfp20}, an ambitious project which builds on Liquid Haskell \cite{10.1145/2628136.2628161}, and previous work on resource-guided synthesis \cite{knoth+19resourceguided} to provide a language for formally verifying amortized cost properties of functional programs. LRT has one notable feature that outpaces all other developments in the space (including \lambdaamorimpl): it allows for value-dependent cost analyses. In principle, \lambdaamorimpl could be extended to allow for such analyses by enhancing its refinement types, although we have yet to consider this. The largest practical benefit of \lambdaamorimpl over LRT is the ability to associate any amount of poential to any value. LRT shares RAML's restriction that potentials may only be associated to the constructors of inductive types, and the amount of potential is dictated by the shape of the constructors. Because of this, to assign different amounts of potential to a datatype, one must carefully re-define the datatype so that its constructors hold the correct amount of potential. In contrast, \lambdaamorimpl allows one to separate these concerns, and assign different amounts of potential to the same data type.

Recent work has also attempted to develop type theories with resource analysis capabilities. Quantitative Type Theory (QTT) \cite{atkey:lics18} continues in with philosophy of \citet{girard-et-al:tcs92:} in using a generalized form of n-linear types to track resources in a dependent type theory. Similar work by the Granule project \cite{choudhury-et-al:popl21} \cite{orchard-et-al:popl19} seeks similar, albeit more practical, ends: the integration of resource reasoning capabilities into general dependently typed languages. However, none of these developments are specific to the resource of cost. To the author's knowledge, the only type theory with built-in notions of program cost is Cost-Aware Type Theory (CATT) \cite{niu-harper:catt}.