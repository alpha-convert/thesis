\red{(This intro is very tentative, and I'm holding off on fixing it until I know exactly what the rest of thesis looks like)}
As the importance of ubiquity of modern software increases, so too does its complexity. The burden of this complexity blowup lands squarely on the shoulders of software developers, who are asked to create increasingly intricate systems, with little extra help. In response to this, many developers have turned to tooling and languages to help ease the burden: this is exemplified by the rise of Rust and safe systems languages \citehere which provide safety guarantees, along with the integration of static analysis tools into the standard development practices of many large software companies. While these practices go a long way to improve developer experience, they are limited in the domains of understanding that they improve. Notably, there are very few existing tools which help developers reason about the algorithmic complexity and performance of their software. The days when the performance and resource usage could be easily discerned from source code by eye are long gone, and yet very few techniques have stepped in to fill the void.

%The long-term goal of the field of language-based resource analysis is to create languages and tools which fill this gap by providing methods for statically determining a program's resource usage, and presenting this information to developers. Works (\textbf{??}) in this field usually come in one of two forms: techniques which which give resource guarantees to large classes of programs at once, and those which allow one to analyze a single program a time. The first category is typified by the creation of languages with resource-aware type systems which allow programmers to reason about their programs' resource usage as they write them. Some examples of projects of this type include Resource Aware ML \citehere, Granule \citehere, and TiML \citehere. The second category is exemplified by verification techniques such as program logics and static analyses-- given a program, these tools can be used to semi-automatically derive or prove resource usage properties. Some great examples of this class include RelCost \citehere, Infer \citehere, and work in improvement theory \citehere.

%In this thesis, we will explore and extend one development in each of these two categories. More specifically, we will specialize our notion of resource to only consider algorithmic complexity or time cost, and examine two techniques which analyze it using amortized analysis.

\subsection{Contributions}

We begin in Chapter~\ref{chap:lambda-amor} by exploring a project of the first category: a resource-aware programming language. In particular, we consider \lambdaamor, a language for amortized cost analysis with an immensely expressive type system which subsumes multiple competing resource-aware type systems \cite{rajan-et-al:popl21}. The original creators of \lambdaamor considered it primarily for this purpose: as a unifying calculus for multiple resource-aware type systems. In this chapter, we consider it instead from a language designer's perspective and investigate what it takes to \textit{implement} the language. As a programming language, \lambdaamor fills an analogous point in the design space of resource aware languages to that of dependently-typed languages like Agda \citehere in the space of verification techniques. The highly expressive type system that \lambdaamor sports allows for intrinsic verification \citehere of program costs, similarly to how dependent or refinement types allow for intrinsic verification of functional correctness. However, the same expressiveness that allows interesting amortized bounds to be statically determined in \lambdaamor makes this implementation task no easy feat, and so we draw on a plethora of type system algorithmization techniques to eventually arrive at a calculus which is amenable to implementation. Finally, we present and evaluate an implementation of \lambdaamor in OCaml.

Then, in Chapter~\ref{chap:rec-extr}, we move to considering a development in category two: a formal method for manually analyzing the amortized cost of functional programs. The method we present is based on \textit{recurrence extraction}, and closely mirrors the traditional algorithm analysis technique of the same name. In fact, the technique we present serves to formally justify the informal on-paper calculations already performed by practitioners of functional languages. The formal version of recurrence extraction was pioneered by \citet{danner-et-al:plpv13}, and has since been extended to increasingly complex functional languages. In this chapter, we extend it to handle amortized cost analysis.


% Expressive
% - Subsumes AARA
% - Can encode complex examples not statically verifiable by other existing languages
% Expressivity comes at the cost of ease of implementation
% We consider 

\red{Do I have to do attribution sentences here? This work was performed at ... with... funded by... presented at....}


\section{Amortized Analysis Primer}
\label{sec:amortized-primer}
The intuition behind both developments in this thesis is rooted in amortized analysis, the classical algorithm analysis technique first presented by \citet{tarjan:amortized-complexity}. As such, we will provide a brief introduction to the technique here, in addition to presenting a few examples of its utility, one of which we will use as a running example.

Amortized analysis was initially conceived of as a technique for analyzing the worst-case cost of a sequence of operations on a data structure. Without amortization, such cost analyses can be very imprecise Naively, the worst-case cost of a sequence of operations is bounded by the sum of the worst-case cost for each operation. However, this usually fails to take into account internal data structure invariants which make it impossible (or not always true) that each operation in the sequence executes with its worst-case complexity. For a concrete example of this phenomenon, consider the (contrived, yet useful) example of a binary counter shown in Figure~\ref{fig:bin-counter}.

\begin{figure}
  %bit, counter, inc,set
  \caption{Binary Counter Data Structure}
  \label{fig:bin-counter}
\end{figure}

The type \texttt{counter} is a list of bits, with the least significant bit at the head. The \texttt{inc} operation increments the binary counter by one. To illustrate where a standard analysis goes wrong, we will analyze the cost of a sequence of $n$ increment operations, starting from the empty list-- this behavior is encapsulated by the \texttt{set}, which iterates \texttt{set} $n$ times. For simplicity, the only costly operations are cons (\texttt{::}) operations, which cost one unit of time each.

Given a counter of length $k$, \texttt{inc} performs at most $k + 1$ cons operations-- at worst, the counter is all ones, and \texttt{inc} must walk down the entire list flipping ones to zeroes, finishing by cons-ing a one onto the end. It's easy to see that after $i$ calls to \texttt{inc}, the counter has length bounded by $\left \lceil{\log_2 i}\right \rceil$, the number of needed for the binary representation of $i$. Thus, the total cost of \texttt{set n} is
\begin{align*}
  \sum_{i=1}^n \left\lceil{\log_2 i}\right \rceil + 1
  &\leq \sum_{i=1}^n \log_2 i + 2\\
  &\leq 2n + \sum_{i=1}^n \log_2 i\\
  &\leq 2n + \log_2(n!)\\
  &\leq 2n + n\log_2 n
\end{align*}

While it may be useful for some applications, this bound is not tight. To see why, consider the case where the counter is set to the value of $15_{10}$, or $1111_2$. A  call to increment on this counter costs the full $5$ cons operations, leaving the counter at $10000_2$. However, a subsequent call to \texttt{inc} only costs $1$ to flip the first bit. Indeed, very few calls to \texttt{inc} traverse the whole list-- the vast majority only flip one or two bits. This example illustrates the tension of doing these naive analysis, and provides the primary insight for amortized analysis: while one data structure operation may be expensive, it may also restructure the data structure in such a way which makes \textit{subsequent} operations cheaper than the worst case\footnote{
This is the genesis of the name \textit{amortized} analysis: expensive function operations effectively pay for subsequent ones to be cheaper, evoking \textit{amortization} from accounting.
}.

\subsection{Physicist's Method}
The most common method for operationalizing this insight is by the physicist's method of amortized analysis. This method proceeds by associating a data structure with a real-valued ``potential function" $\Phi : S \to \mathbb{R}$ on its states. The only restriction on potential functions is that they be nonnegative everywhere.
Then, for any sequence of data structure operations $f_i$ with costs $c_i$ and intermediate states $s_i$ (pictured in Figure~\ref{fig:amortized-situation}, with $s_0$ initial and $s_i = f_i(s_{i-1})$), we may define the \textit{amortized cost} of each operation as:
$$
a_i = c_i + \Phi(s_i) - \Phi(s_{i-1})
$$

\begin{figure}
  \caption{The Generic Amortized Analysis Setup}
  \label{fig:amortized-situation}
\end{figure}

The amortized cost of an operation is the actual cost, plus the change in potential across it. When we sum the amortized cost of all the operations across the sequence, the sum telescopes:
\begin{align*}
  \sum_{i=1}^n a_i &= \sum_{i=1}^n c_i + \Phi(s_i) - \Phi(s_{i-1})\\
                   &= \Phi(s_n) - \Phi(s_0) + \sum_{i=1}^n c_i
\end{align*}
Then, if $\Phi(s_0) = 0$, we get that the total amortized cost is an upper bound on the total actual cost.
$$
\sum_{i=1}^n a_i \geq \sum_{i=1}^n c_i
$$
A good intuition for potential functions $\Phi$ is that $\Phi(s)$ represents the amount of work subsequent operations have to do in order to modify the data structure.
In practice, one usually picks potential functions such that when an expensive operation $f_i$ runs, $\Phi(s_{i-1})$ is very large, and $\Phi(s_i)$ is very small such that subsequent operations are cheap.

Returning to the binary counter example, the traditional choice of potential function is to take $\Phi(\texttt{xs})$ to be the number of 1-bits in \texttt{xs}.
For simplicity, we will denote the actual cost of a call to $\texttt{inc xs}$ by $C(\texttt{xs})$, and its amortized cost by $A(\texttt{xs}) = C(\texttt{xs}) + \Phi(\texttt{inc xs}) - \Phi(\texttt{xs})$.

\begin{theorem}
For all \texttt{xs:counter}, $A(\texttt{xs}) = 2$
\end{theorem}
\label{thm:bc-phys}
\begin{proof}
By a straightforward structural induction on \texttt{xs}.
\begin{itemize}
  \item ($\texttt{xs} = \texttt{[]}$): \texttt{inc []} does $1$ cons operation, $\Phi(\texttt{[]}) = 0$ and $\Phi(\texttt{[1]}) = 1$, so the amortized cost is $2$.
  \item ($\texttt{xs} = \texttt{y::ys}$): If $\texttt{y} = \texttt{0}$, then the \texttt{inc xs} does $1$ cons operation. Since $\Phi(\texttt{1::ys}) - \Phi(\texttt{y::ys}) = (1 + \Phi(\texttt{ys})) - \Phi(\texttt{ys}) = 1$, we again have that the amortized cost of \texttt{inc xs} is $2$. Now, suppose $\texttt{y} = \texttt{1}$. Then \texttt{inc xs} incurs $1$ cost from the cons operation, plus the cost of \texttt{inc ys}. So, we may compute:
  \begin{align*}
    A(\texttt{y::ys}) &= C(\texttt{y::ys}) + \Phi(\texttt{inc (y::ys)}) - \Phi(\texttt{y::ys})\\
    &= (1 + C(\texttt{ys}) + \Phi(\texttt{inc ys}) - (1 + \Phi(\texttt{ys}))\\
    &= C(\texttt{ys}) + \Phi(\texttt{inc ys}) - \Phi(\texttt{ys})\\
    &= A(\texttt{ys)}
  \end{align*}
  But by the inductive hypothesis, $A(\texttt{ys}) = 2$, which completes the proof.
\end{itemize}
\end{proof}

An immediate corollary of this fact is that the amortized cost of \texttt{set n} is bounded by $2n$, by the same telescoping argument as before. Most importantly, since the binary counter \texttt{[0]} has potential $0$, the amortized cost of \texttt{set n} is an upper bound on the \textit{actual} cost of \texttt{set n}. This last step is crucial. A priori, amortized costs give no information about the actual costs of program execution, and are only a bound on the actual cost if the final potential is greater than the initial.

\subsubsection{Single Function and Gas Tank Analyses}
Often, amortized analysis is applied to individual functions, rather than a sequence. This may be thought of as the length-one case of amortized analysis.
Given a function \texttt{f:a->b} and potential functions $\Phi_\texttt{a} : \texttt{a} \to \R$ and $\Phi_\texttt{b} : \texttt{b} \to \R$, we may define
the amortized cost of \textbf{f} as $A_\texttt{f}(\texttt{x}) = C_\texttt{f}(\texttt{x}) + \Phi_\texttt{b}(\texttt{f x}) - \Phi_\texttt{a}(\texttt{x})$,
where $C_\texttt{f}(\texttt{x})$ is the cost of \texttt{f}. As long as $\Phi_\texttt{b}(\texttt{f x}) \geq \Phi_\texttt{a}(\texttt{x})$, the amortized cost is an upper bound on the actual cost.

A common variation on this concept is to pick the potential functions such that the amortized cost is \textit{zero}. Then, the actual cost $C_\texttt{f}(\texttt{x})$ is exactly $\Phi_\texttt{a}(\texttt{x}) - \Phi_\texttt{b}(\texttt{f x})$. The usual intuition here is to think of the available potential as a sort of ``gas tank" which the function must siphon from to do work. Then, the total gas used, $\Phi_\texttt{a}(\texttt{x}) - \Phi_\texttt{b}(\texttt{f x})$, is an upper bound on the amount of work done by the function.


\subsection{Banker's Method}
While the physicist's method is powerful, some situations call for a more fine-grained analysis. In this case, we employ the so-called banker's method of amortized analysis. The banker's method works by introducing imaginary ``credits" to a data structure, which may be ``attached" to the values in a program. These credits are thought of to interact with the cost model of the language in a special way: credits can always be created from thin air at a cost of one unit of time, and then they can subsequently be discarded or ``spent" to decrease execution cost by one unit. In the setting of the banker's method, this is what we mean when we refer to ``amortized cost"-- the real cost of an operation, plus the cost of creating and spending credits along the way. Crucially, if an operation begins with no credits available, then the amortized cost must be an upper bound on the actual cost since credits must be created (incurring a cost of $1$) before they can be spent (decreasing the cost by $1$). All of this is best illustrated by returning to the binary counter example.

We begin by enforcing a credit invariant on values of type \texttt{counter}: every \texttt{1} bit must have a credit attached. It's worth confirming that the increment function is able to maintain this invariant: if the counter is empty, we spawn a credit, attach it to a \texttt{1} bit, and cons it to the front of the list.
If the least significant bit is \texttt{0}, we again spawn a credit, flip the bit to \texttt{1}, and attach the credit. Finally, if the least significant bit is \texttt{1}, then we detatch its credit, spend it, recurse down the tail, and finish by cons-ing a \texttt{0} onto the front. Of course, none of this is manifest in the code
\footnote{
Readers familiar with concurrent separation logic might find this idea familiar: credits are a form of ghost state.
}.
Just like with potential in the physicist's method, these proofs must happen off to the side on paper.

Finally, we may perform the analysis itself.

\begin{theorem}
For all \texttt{xs:counter} satisfying the credit invariant, the amortized cost of \texttt{inc xs} is $2$.
\end{theorem}
\label{thm:bc-bank}
\begin{proof}
We proceed by structural induction on \texttt{xs}.
\begin{itemize}
  \item ($\texttt{xs} = \texttt{[]}$): \texttt{inc []} does $1$ cons operation and spawns one credit, for an amortized cost of $2$.
  \item ($\texttt{xs} = \texttt{y::ys}$): If $\texttt{y} = \texttt{0}$, then \texttt{inc xs} does one cons operation and spawns a single credit, for an amortized cost of $2$. Finally, if $\texttt{y} = \texttt{1}$, then \texttt{inc xs} makes a single recursive call \texttt{inc ys}, which has amortized cost $2$, by inductive hypothesis. But then, the function spends the credit attached to \texttt{y}, which cancels out the cost $1$ incurred by cons-ing a \texttt{0} onto the result of the recursive call. In total, this case has $2$ amortized cost, as required.
\end{itemize}
\end{proof}

\subsection{Comparisons}

The reader may note that the proof of Theorem~\ref{thm:bc-bank} was remarkably similar to the proof of Theorem~\ref{thm:bc-phys}. This is, unsurprisingly, not by coincidence. The reason for this similarity is that the banker's method can be thought of as a concretization of the physicist's method. Rather than ``globally" assigning potential to the states of a data structure, the banker's method ``localizes" the potential, thought of as discrete credits, on  specific values in the state. Because of this, analyses with the banker's method have an ``operational" feel to them, while analyses using the physicist's method have a more ``calculational" flavor.

On the whole, the two methods of amortized analysis are essentially equivalent in power. Given a banker's method analysis, we may turn it into a physicist's method analysis by taking the potential function to be the total number of credits. Conversely, physicist's method analyses can be converted to use the banker's method by maintaining the invariant that $ \left\lceil{\Phi(\texttt{s})}\right \rceil$ credits be kept on the value \texttt{s}.

Proofs using the banker's method are often more tedious and traditionally less formal. In Chapter~\ref{chap:rec-extr}, we present a formalization of the banker's method by way of recurrence extraction. Chapter~\ref{chap:lambda-amor} presents \lambdaamor, which is based primarily on the physicist's method.

\section{Substructural and Modal Types Primer}
\label{sec:modal-and-substructural}
While Chapter~\ref{chap:rec-extr} is primarily concerned with approaching cost analysis from a recurrence extraction angle, it will, like Chapter~\ref{chap:lambda-amor}, involve the creation of a type system. The two type systems under consideration, those of \lambdaA and \lambdaamor, share a major feature which is common to most developments in resource analysis: they are both \textit{affine substructural} type systems. Most type systems have three ``non-logical" rules governing the behavior of their typing context. The rule that allows contexts to be freely considered up to permutations is called \textit{exchange}, the rule allowing variables to be dropped from the context is called \textit{weakening}, and the rule which allows variables to be duplicated is referred to as \textit{contraction}. The three rules are sometimes explicitly included in the list of rules generating a typing judgment, but more often than not they are omitted and derived as admissible rules.

Substructural type systems are ones in which some or all of the traditional structural rules governing typing contexts are disallowed. Disallowing all of them yields ordered types \citehere and allowing only exchange yields linear types \citehere. Most importantly for this thesis however, is the combination of exchange and weakening (but not contraction), which yields an \textit{affine} type system. In an affine type system, contexts are sets, where each variable from the context may be used at most once.

The rules of an affine type system are set up in such a way that the contraction rule is inadmissible. In particular, every multi-premise rule ``splits" the context so that a variable cannot be used by more than one sub-term. For instance, consider the usual product introduction rule of a fully-structural type system:
$$
\inferrule{\Gamma \vdash e_1 : A \\ \Gamma \vdash e_2 : B}{\Gamma \vdash (e_1,e_2) : A \times B}
$$
is transformed into the following rule to support affine types:
$$
\inferrule{\Gamma_1 \vdash e_1 : A \\ \Gamma_2 \vdash e_2 : B}{\Gamma_1,\Gamma_2 \vdash (e_1,e_2) : A \otimes B}
$$
The context must be divided into two disjoint parts $\Gamma_1$ and $\Gamma_2$ to be given to the two premises\footnote{
The name of the connective changes also-- in a fully structural type system, positive and negative products are isomorphic, and thus written $\times$. Passing to a substructural type system disentangles the two notions.
}.

Affine types are a natural object of study in the resource-usage literature, since they naturally enforce a resource-usage restriction: using a variable ``consumes" it, which corresponds to the consumption of the resources the variable holds. In the type systems we will study for the remainder of this thesis, the notion of resource which our affine type systems will track will be the credits and potentials of amortized analysis. Indeed, non-duplication of variables in an affine type system can be leveraged to enforce the non-duplication requirement of credits and potential in the bankers and physicists method of amortized analysis.

Of course, this restriction on the usage of variables is very strong from a programming perspective. Even incredibly simple programs often require the re-use of a single variable. The traditional solution to this is the introduction of a new type $!A$, which represents values of type $A$ that can be used arbitrarily many times. This $!$, usually referred to as the \textit{exponential modality} is our first example of a modality, or a unary operator on types. To ``implement" this modality in the type system, one usually adds a second context to the typing judgment of so-called exponential variables, which can be used multiple times \citehere. This context is fully structural-- it is not split in multi-premise rules. For instance, the product introduction rule becomes something like:
$$
\inferrule{\Omega;\Gamma_1 \vdash e_1 : A \\ \Omega; \Gamma_2 \vdash e_2 : B}{\Omega; \Gamma_1,\Gamma_2 \vdash (e_1,e_2) : A \otimes B}
$$
and we also add an exponential variable rule to use variables from the $\Omega$ context.
$$
\inferrule{ }{\Omega, x : A; \Gamma \vdash x : A}
$$
The modality itself is governed by a pair of simple introduction and elimination rules. A term can be made exponential with the introduction rule, so long as it does not depend on any affine resources:
$$
\inferrule{\Omega ; \cdot \vdash e : A}{\Omega ; \Gamma \vdash e : \, !A}
$$
The elimination rule allows for a term of type $!A$ to be bound as one of type $A$ in the exponential context, and thus be used many times:
$$
\inferrule{\Omega ; \Gamma_1 \vdash e : \,!A \\ \Omega, x :: A ; \Gamma_2 \vdash e' : B}{\Omega ; \Gamma_1,\Gamma_2 \vdash \texttt{let} \, !x \, = \, e \,\texttt{in}\, e' : B}
$$

Both \lambdaA and \lambdaamor make use of versions of this modality, but \lambdaA generalizes it to also allow $n$-affine types, whose values may be used at most $n$ times. In addition, each type system uses a \textit{graded} modality to quantify the usage of potential and credits. While the presentation (and terminology) is slightly different, the $!_r A$ modality of \lambdaA and the $[I] \, A$ modality of \lambdaamor can be thought of as essentially one and the same. 